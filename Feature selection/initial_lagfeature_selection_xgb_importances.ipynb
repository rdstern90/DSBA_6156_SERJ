{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "rand_state = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipelines and functions \n",
    "1. Preprocessing 2. Sampling 3. Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines: Defining the categorical imputation and one-hot encoder for categorical variables.\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "        # (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)), #Commented out because the categorical variables won't play nice with dummies between test/train. Retry when we do a full train model. Can impute values on test_data.csv if necessary.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining the numerical imputation and standard scaler for numerical variables.\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    "           #(\"scale\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# def_prep_df: Preparing the TRAINING data for creating and testing the model.\n",
    "def prep_df(df, target, target_to_drop):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "    # save statement_age column\n",
    "    statement_age_s = df['statement_age']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=[\"s_2\", 'statement_age', target_to_drop])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    over_threshold\n",
    "    \n",
    "\n",
    "    df.drop(over_threshold.index, \n",
    "            axis=1, \n",
    "            inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols_all = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_66', 'd_68', 'b_31', 'd_87']\n",
    "    cat_cols = [col for col in X.columns.str.lower() if col in cat_cols_all]\n",
    "    num_cols = [col for col in X.columns.str.lower() if col not in cat_cols]\n",
    "    \n",
    "    # get dummies for categorical variables\n",
    "    Xcat = pd.get_dummies(X[cat_cols], columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    X = pd.concat([X[num_cols],Xcat], axis=1)\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_age_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "            y.values.reshape(-1, 1)\n",
    "            )\n",
    "    y_processed = pd.DataFrame(y_processed, index=df_index)\n",
    "\n",
    "    \n",
    "    return X_processed, y_processed, cols_list\n",
    "\n",
    "\n",
    "def prep_lag_df(df, target):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "\n",
    "    # save statement_delta column\n",
    "    statement_delta_s = df['statement_delta']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=['statement_delta'])\n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    # cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, cols_list)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Apply preprocessing (scale)\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_delta_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    # y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "    #         y.values.reshape(-1, 1)\n",
    "    #         )\n",
    "    y_processed = pd.DataFrame(y, index=df_index)\n",
    "    print(y_processed.shape)\n",
    " \n",
    "    \n",
    "    return X_processed, y_processed, cols_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction):\n",
    "    n = 100\n",
    "    ids = np.array(df_train_y.index)\n",
    "    target = np.array(df_train_y['target'])\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=rand_state)\n",
    "    skf.get_n_splits(ids, target)\n",
    "\n",
    "    i = 0\n",
    "    id_subsets = [None]*n\n",
    "    for _, subset in skf.split(ids, target):\n",
    "        id_subsets[i] =list(ids[subset])\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    list1 = list(np.arange(0, int(usefraction[0]*100), 1))\n",
    "    list2 = list(np.arange(int(usefraction[0]*100), int(usefraction[0]*100)+int(usefraction[1]*100), 1))\n",
    "\n",
    "\n",
    "    train_ids = []\n",
    "    for i in list1:\n",
    "        train_ids.extend(id_subsets[i])\n",
    "    test_ids = []\n",
    "    for i in list2:\n",
    "        test_ids.extend(id_subsets[i])\n",
    "\n",
    "\n",
    "    X_train = X_processed[df_train.index.isin(train_ids)]\n",
    "    y_train = y_processed[df_train.index.isin(train_ids)]\n",
    "    X_test = X_processed[df_train.index.isin(test_ids)]\n",
    "    y_test = y_processed[df_train.index.isin(test_ids)]\n",
    "\n",
    "\n",
    "    print(f'Train data obs.: {len(X_train)}')\n",
    "    print(f'Test data obs: {len(X_test)}')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create initial df to be further processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_x = pd.read_parquet('./../ignore/train.parquet')\n",
    "# df_train_x.columns = df_train_x.columns.str.lower()\n",
    "# df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "# df_train_x = df_train_x.set_index('customer_id')\n",
    "\n",
    "df_train_x2 = pd.read_parquet('./../ignore/train_dfl.parquet')\n",
    "df_train_x2.columns = df_train_x2.columns.str.lower()\n",
    "\n",
    "df_train_y = pd.read_csv('./../ignore/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "\n",
    "\n",
    "# df_train = pd.merge(df_train_x, df_train_y, left_index=True, right_on='customer_id', how='left')\n",
    "\n",
    "# df_train['last_statement_flag'] = (df_train.groupby(df_train.index)['s_2']\n",
    "#                       .rank(method='dense', ascending=False)\n",
    "#                       .astype(int)\n",
    "#                    )\n",
    "\n",
    "# df_train['last_statement_target'] = df_train['target']*df_train['last_statement_flag'].apply(lambda x: 1 if x==1 else 0)\n",
    "# df_train = df_train.rename(columns={'last_statement_flag':'statement_age'})\n",
    "\n",
    "\n",
    "df_train = pd.merge(df_train_x2, df_train_y, left_index=True, right_on='customer_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which statements to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_statements = [0]\n",
    "# df_train = df_train[df_train['statement_age'].isin(use_statements)]\n",
    "df_train = df_train[df_train['statement_delta'].isin(use_statements)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the data after selecting statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453793, 158)\n",
      "(453793, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prep the dataframe\n",
    "# Note that the last column 'statement_age' is left in the dataframes for scoring, not for predicting!\n",
    "X_processed, y_processed, cols_list = prep_lag_df(df_train, target='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get samples for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data obs.: 317676\n",
      "Test data obs: 136117\n"
     ]
    }
   ],
   "source": [
    "# First vale of \"usefraction\" specifies the train size and the second, the test size (fraction of total train data available)\n",
    "X_train, X_test, y_train, y_test = get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction = [0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB\n",
    "feature selection loop using feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, Duration: 57.728 s, Score: 0.7559, Number of features: 157\n",
      "Iter 2, Duration: 51.195 s, Score: 0.75574, Number of features: 142\n",
      "Iter 3, Duration: 47.799 s, Score: 0.75635, Number of features: 127\n",
      "Iter 4, Duration: 42.2 s, Score: 0.75617, Number of features: 113\n",
      "Iter 5, Duration: 38.512 s, Score: 0.75683, Number of features: 101\n",
      "Iter 6, Duration: 33.932 s, Score: 0.7559, Number of features: 90\n",
      "Iter 7, First fit yielded too low score, trying again (number of features attempted: 80\n",
      "Iter 7, Duration: 64.497 s, Score: 0.75413 - Iteration failed, too large accuracy loss ([-0.00270615])\n",
      "Current feature reduction rate: 0.05\n",
      "Iter 8, Duration: 34.846 s, Score: 0.7559, Number of features: 90\n",
      "Iter 9, First fit yielded too low score, trying again (number of features attempted: 86\n",
      "Iter 9, Duration: 67.728 s, Score: 0.75498 - Iteration failed, too large accuracy loss ([-0.00185529])\n",
      "Current feature reduction rate: 0.025\n",
      "Iter 10, Duration: 34.007 s, Score: 0.7559, Number of features: 90\n",
      "Iter 11, Duration: 34.01 s, Score: 0.75641, Number of features: 88\n",
      "Iter 12, First fit yielded too low score, trying again (number of features attempted: 86\n",
      "Iter 12, Duration: 65.755 s, Score: 0.75546 - Iteration failed, too large accuracy loss ([-0.00137147])\n",
      "Current feature reduction rate: 0.0125\n",
      "Iter 13, Duration: 33.451 s, Score: 0.75641, Number of features: 88\n",
      "Iter 14, First fit yielded too low score, trying again (number of features attempted: 87\n",
      "Iter 14, Duration: 67.036 s, Score: 0.75431 - Iteration failed, too large accuracy loss ([-0.00252805])\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "feature_reduction_rate = 0.1 # Attempt to remove 10% of remaining features in each loop iteration\n",
    "accuracy_loss = 0.001 # accepted accuracy loss c.f. max accuracy\n",
    "scoring_method = 'delta0'\n",
    "\n",
    "\n",
    "scores = []\n",
    "feature_ind = [list(range(0,(X_train.shape[1]-1)))]\n",
    "feature_names = [cols_list]\n",
    "remove_n_features = [int(len(feature_ind[0])*feature_reduction_rate)]\n",
    "i = 0\n",
    "\n",
    "\n",
    "def get_reduced_features(xgbc, feature_names, remove_n_features):\n",
    "    xgb_feature_imp = pd.DataFrame({'name':feature_names,\n",
    "                                    'importances':[val[0] for val in xgbc.feature_importances_.reshape(-1,1)]})\n",
    "\n",
    "    xgb_feature_imp = xgb_feature_imp.sort_values(by='importances', ascending=False)\n",
    "    \n",
    "    df_features = xgb_feature_imp.iloc[:-remove_n_features,:]\n",
    "    feature_ind = df_features.index\n",
    "    feature_names = df_features['name']\n",
    "\n",
    "\n",
    "    return feature_ind, feature_names\n",
    "\n",
    "\n",
    "\n",
    "def fit_predict(feature_ind, feature_names, remove_n_features, X_train, y_train, X_test, y_test):\n",
    "    xgbc = xgb.XGBClassifier(use_label_encoder=False).fit(X_train[feature_ind], y_train, verbose=0, eval_metric='logloss')\n",
    "    \n",
    "    y_pred_a_xgb = pd.DataFrame({'customer_id':X_test.index.values,\n",
    "                            'scoring_var':X_test.iloc[:,-1].values,\n",
    "                            'prediction':[val[1] for val in xgbc.predict_proba(X_test[feature_ind])]})\n",
    "    \n",
    "    if scoring_method == 'last_statement':\n",
    "        proba_xgb = y_pred_a_xgb[y_pred_a_xgb['scoring_var']==1].set_index('customer_id')\n",
    "\n",
    "    elif scoring_method == 'delta0':\n",
    "        proba_xgb = y_pred_a_xgb[y_pred_a_xgb['scoring_var']==0].set_index('customer_id')\n",
    "\n",
    "    elif scoring_method == 'delta1':\n",
    "        proba_xgb = y_pred_a_xgb[y_pred_a_xgb['scoring_var']==1].set_index('customer_id')\n",
    "\n",
    "    elif scoring_method == 'average-deltas':\n",
    "        proba_xgb = y_pred_a_xgb.groupby('customer_id')['prediction'].mean()\n",
    "    \n",
    "    score = [amex_metric(y_test.groupby(y_test.index).max().rename(columns={0:'target'}), proba_xgb)]\n",
    "    feature_ind, feature_names = get_reduced_features(xgbc, feature_names, remove_n_features)\n",
    "\n",
    "    return score, feature_ind, feature_names, xgbc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    cscore, cfeature_ind, cfeature_names, cxgb = fit_predict(feature_ind[i], feature_names[i], remove_n_features[i], X_train, y_train, X_test, y_test) # \"c\" denotes current\n",
    "    scores += cscore # add score to list\n",
    "\n",
    "    if i > 0:\n",
    "        if (max(scores) - scores[i]) >= accuracy_loss: # Maximum residual between max and current score\n",
    "            \n",
    "            print(f'Iter {i+1}, First fit yielded too low score, trying again (number of features attempted: {len(feature_ind[i])}')\n",
    "            cscore, cfeature_ind, cfeature_names, cxgb = fit_predict(feature_ind[i], feature_names[i], remove_n_features[i], X_train, y_train, X_test, y_test) # \"c\" denotes current\n",
    "\n",
    "            if (max(scores) - cscore) >= accuracy_loss: # use same criterion again\n",
    "                \n",
    "                # print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Iteration failed, too large accuracy loss ({cscore-max(scores)}), removing fewer features')\n",
    "                print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Score: {round(scores[i],5)} - Iteration failed, too large accuracy loss ({cscore-max(scores)})')\n",
    "                feature_ind += [feature_ind[i-1]] # add list of feature indices to list\n",
    "                feature_names += [feature_names[i-1]] # add list of feature names to list\n",
    "\n",
    "                feature_reduction_rate = feature_reduction_rate*0.5 # decrease feature reduction rate if iteration failed\n",
    "\n",
    "                if int(len(feature_ind[i])*feature_reduction_rate) >= 1: # Check that at least 1 feature can be removed\n",
    "                    remove_n_features += [int(len(feature_ind[i])*feature_reduction_rate)]\n",
    "                    print(f'Current feature reduction rate: {feature_reduction_rate}')\n",
    "                    i += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print('Completed')\n",
    "                    break\n",
    "\n",
    "\n",
    "            else:\n",
    "                scores[i] = cscore # Overwrite first fit score with 2nd fit score\n",
    "                print(f'Iter {i+1},  Retry successful')\n",
    "\n",
    "\n",
    "\n",
    "    feature_ind += [cfeature_ind] # add list of feature indices to list\n",
    "    feature_names += [cfeature_names] # add list of feature names to list\n",
    "    remove_n_features += [int(len(feature_ind[i])*feature_reduction_rate)]\n",
    "    \n",
    "    print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Score: {round(scores[i],5)}, Number of features: {len(feature_ind[i])}')\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "savelist = list(feature_names[len(feature_names)-2])\n",
    "\n",
    "f = open('lag_features2.pickle', 'wb')\n",
    "pickle.dump(savelist, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('lag_features.pickle', 'rb')\n",
    "loadlist = pickle.load(f)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca7b95862a80fe182a4f5fb98c5ac1654efde8b126ad722bfd53b4f5adb0e8de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
