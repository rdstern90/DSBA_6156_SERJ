{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import xgboost as xgb\n",
    "\n",
    "rand_state = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipelines and functions \n",
    "1. Preprocessing 2. Sampling 3. Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines: Defining the categorical imputation and one-hot encoder for categorical variables.\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "        # (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)), #Commented out because the categorical variables won't play nice with dummies between test/train. Retry when we do a full train model. Can impute values on test_data.csv if necessary.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining the numerical imputation and standard scaler for numerical variables.\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    "           #(\"scale\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# def_prep_df: Preparing the TRAINING data for creating and testing the model.\n",
    "def prep_df(df, target, target_to_drop):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "    # save statement_age column\n",
    "    statement_age_s = df['statement_age']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=[\"s_2\", 'statement_age', target_to_drop])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    over_threshold\n",
    "    \n",
    "\n",
    "    df.drop(over_threshold.index, \n",
    "            axis=1, \n",
    "            inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols_all = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_66', 'd_68', 'b_31', 'd_87']\n",
    "    cat_cols = [col for col in X.columns.str.lower() if col in cat_cols_all]\n",
    "    num_cols = [col for col in X.columns.str.lower() if col not in cat_cols]\n",
    "    \n",
    "    # get dummies for categorical variables\n",
    "    Xcat = pd.get_dummies(X[cat_cols], columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    X = pd.concat([X[num_cols],Xcat], axis=1)\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_age_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "            y.values.reshape(-1, 1)\n",
    "            )\n",
    "    y_processed = pd.DataFrame(y_processed, index=df_index)\n",
    "\n",
    "    \n",
    "    return X_processed, y_processed, cols_list\n",
    "\n",
    "\n",
    "def prep_lag_df(df, target):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "\n",
    "    # save statement_delta column\n",
    "    statement_delta_s = df['statement_delta']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=['statement_delta'])\n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    # cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, cols_list)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    # Apply preprocessing (scale)\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_delta_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    # y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "    #         y.values.reshape(-1, 1)\n",
    "    #         )\n",
    "    y_processed = pd.DataFrame(y, index=df_index)\n",
    "    print(y_processed.shape)\n",
    " \n",
    "    \n",
    "    return X_processed, y_processed, cols_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction):\n",
    "    n = 100\n",
    "    ids = np.array(df_train_y.index)\n",
    "    target = np.array(df_train_y['target'])\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=rand_state)\n",
    "    skf.get_n_splits(ids, target)\n",
    "\n",
    "    i = 0\n",
    "    id_subsets = [None]*n\n",
    "    for _, subset in skf.split(ids, target):\n",
    "        id_subsets[i] =list(ids[subset])\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    list1 = list(np.arange(0, int(usefraction[0]*100), 1))\n",
    "    list2 = list(np.arange(int(usefraction[0]*100), int(usefraction[0]*100)+int(usefraction[1]*100), 1))\n",
    "\n",
    "\n",
    "    train_ids = []\n",
    "    for i in list1:\n",
    "        train_ids.extend(id_subsets[i])\n",
    "    test_ids = []\n",
    "    for i in list2:\n",
    "        test_ids.extend(id_subsets[i])\n",
    "\n",
    "\n",
    "    X_train = X_processed[df_train.index.isin(train_ids)]\n",
    "    y_train = y_processed[df_train.index.isin(train_ids)]\n",
    "    X_test = X_processed[df_train.index.isin(test_ids)]\n",
    "    y_test = y_processed[df_train.index.isin(test_ids)]\n",
    "\n",
    "    indices_train = df_train.index.isin(train_ids)\n",
    "    indices_test = df_train.index.isin(test_ids)\n",
    "\n",
    "\n",
    "    print(f'Train data obs.: {len(X_train)}')\n",
    "    print(f'Test data obs: {len(X_test)}')\n",
    "\n",
    "    # also extract the statement dates for combining the predictions later on\n",
    "    # train_statement_age = X_train['statement_age']\n",
    "    # test_statement_age = X_test['statement_age']\n",
    "    # X_train = X_train.drop(columns='statement_age')\n",
    "    # X_test = X_test.drop(columns='statement_age')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create initial df to be further processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_x = pd.read_parquet('./../ignore/train.parquet')\n",
    "# df_train_x.columns = df_train_x.columns.str.lower()\n",
    "# df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "# df_train_x = df_train_x.set_index('customer_id')\n",
    "\n",
    "df_train_x2 = pd.read_parquet('./../ignore/train_dfl.parquet')\n",
    "df_train_x2.columns = df_train_x2.columns.str.lower()\n",
    "\n",
    "df_train_y = pd.read_csv('./../ignore/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "\n",
    "\n",
    "# df_train = pd.merge(df_train_x, df_train_y, left_index=True, right_on='customer_id', how='left')\n",
    "\n",
    "# df_train['last_statement_flag'] = (df_train.groupby(df_train.index)['s_2']\n",
    "#                       .rank(method='dense', ascending=False)\n",
    "#                       .astype(int)\n",
    "#                    )\n",
    "\n",
    "# df_train['last_statement_target'] = df_train['target']*df_train['last_statement_flag'].apply(lambda x: 1 if x==1 else 0)\n",
    "# df_train = df_train.rename(columns={'last_statement_flag':'statement_age'})\n",
    "\n",
    "\n",
    "df_train = pd.merge(df_train_x2, df_train_y, left_index=True, right_on='customer_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which statements to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_statements = [0]\n",
    "# df_train = df_train[df_train['statement_age'].isin(use_statements)]\n",
    "df_train = df_train[df_train['statement_delta'].isin(use_statements)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the data after selecting statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453793, 158)\n",
      "(453793, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prep the dataframe\n",
    "# Note that the last column 'statement_age' is left in the dataframes for scoring, not for predicting!\n",
    "X_processed, y_processed, cols_list = prep_lag_df(df_train, target='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>statement_delta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a</th>\n",
       "      <td>-0.000218</td>\n",
       "      <td>-0.041117</td>\n",
       "      <td>-0.162153</td>\n",
       "      <td>0.005331</td>\n",
       "      <td>-0.017623</td>\n",
       "      <td>0.537648</td>\n",
       "      <td>-0.138857</td>\n",
       "      <td>-0.042601</td>\n",
       "      <td>-0.011163</td>\n",
       "      <td>-0.039037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.02264</td>\n",
       "      <td>0.01872</td>\n",
       "      <td>-0.059542</td>\n",
       "      <td>-0.001816</td>\n",
       "      <td>-0.003697</td>\n",
       "      <td>-0.059517</td>\n",
       "      <td>-0.079378</td>\n",
       "      <td>-0.026433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5</th>\n",
       "      <td>0.077427</td>\n",
       "      <td>-1.092973</td>\n",
       "      <td>-0.013429</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>-0.045351</td>\n",
       "      <td>0.120570</td>\n",
       "      <td>-0.138857</td>\n",
       "      <td>-0.093651</td>\n",
       "      <td>-0.022296</td>\n",
       "      <td>-0.039037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.02264</td>\n",
       "      <td>0.01872</td>\n",
       "      <td>-0.059542</td>\n",
       "      <td>-0.001816</td>\n",
       "      <td>-0.003697</td>\n",
       "      <td>-0.059517</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>-0.026433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00001b22f846c82c51f6e3958ccd81970162bae8b007e80662ef27519fcc18c1</th>\n",
       "      <td>0.311585</td>\n",
       "      <td>-0.041117</td>\n",
       "      <td>-0.071904</td>\n",
       "      <td>-0.022120</td>\n",
       "      <td>-0.045206</td>\n",
       "      <td>0.023992</td>\n",
       "      <td>-0.138857</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>-0.011163</td>\n",
       "      <td>-0.039037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.02264</td>\n",
       "      <td>0.01872</td>\n",
       "      <td>-0.059542</td>\n",
       "      <td>-0.001816</td>\n",
       "      <td>-0.003697</td>\n",
       "      <td>-0.059517</td>\n",
       "      <td>-0.152618</td>\n",
       "      <td>-0.026433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000041bdba6ecadd89a52d11886e8eaaec9325906c9723355abb5ca523658edc</th>\n",
       "      <td>0.017773</td>\n",
       "      <td>-0.041117</td>\n",
       "      <td>-0.043658</td>\n",
       "      <td>-0.010021</td>\n",
       "      <td>-0.024450</td>\n",
       "      <td>-0.420031</td>\n",
       "      <td>-0.138857</td>\n",
       "      <td>0.039050</td>\n",
       "      <td>-0.076013</td>\n",
       "      <td>-0.039037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.02264</td>\n",
       "      <td>0.01872</td>\n",
       "      <td>-0.059542</td>\n",
       "      <td>-0.001816</td>\n",
       "      <td>-0.003697</td>\n",
       "      <td>-0.059517</td>\n",
       "      <td>0.017433</td>\n",
       "      <td>-0.026433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad51ca8b8c4a24cefed</th>\n",
       "      <td>-0.086347</td>\n",
       "      <td>-0.041117</td>\n",
       "      <td>-0.027047</td>\n",
       "      <td>0.020530</td>\n",
       "      <td>-0.063886</td>\n",
       "      <td>0.023992</td>\n",
       "      <td>-0.138857</td>\n",
       "      <td>-0.014738</td>\n",
       "      <td>0.050612</td>\n",
       "      <td>-0.039037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.02264</td>\n",
       "      <td>0.01872</td>\n",
       "      <td>-0.059542</td>\n",
       "      <td>-0.001816</td>\n",
       "      <td>-0.003697</td>\n",
       "      <td>-0.059517</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>-0.026433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0         1  \\\n",
       "customer_id                                                              \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb... -0.000218 -0.041117   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.077427 -1.092973   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.311585 -0.041117   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.017773 -0.041117   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad... -0.086347 -0.041117   \n",
       "\n",
       "                                                           2         3  \\\n",
       "customer_id                                                              \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb... -0.162153  0.005331   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26... -0.013429  0.001772   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80... -0.071904 -0.022120   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233... -0.043658 -0.010021   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad... -0.027047  0.020530   \n",
       "\n",
       "                                                           4         5  \\\n",
       "customer_id                                                              \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb... -0.017623  0.537648   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26... -0.045351  0.120570   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80... -0.045206  0.023992   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233... -0.024450 -0.420031   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad... -0.063886  0.023992   \n",
       "\n",
       "                                                           6         7  \\\n",
       "customer_id                                                              \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb... -0.138857 -0.042601   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26... -0.138857 -0.093651   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80... -0.138857  0.004202   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233... -0.138857  0.039050   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad... -0.138857 -0.014738   \n",
       "\n",
       "                                                           8         9  ...  \\\n",
       "customer_id                                                             ...   \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb... -0.011163 -0.039037  ...   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26... -0.022296 -0.039037  ...   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80... -0.011163 -0.039037  ...   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233... -0.076013 -0.039037  ...   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.050612 -0.039037  ...   \n",
       "\n",
       "                                                         148      149  \\\n",
       "customer_id                                                             \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.027992  0.02264   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.027992  0.02264   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.027992  0.02264   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.027992  0.02264   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.027992  0.02264   \n",
       "\n",
       "                                                        150       151  \\\n",
       "customer_id                                                             \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.01872 -0.059542   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.01872 -0.059542   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.01872 -0.059542   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.01872 -0.059542   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.01872 -0.059542   \n",
       "\n",
       "                                                         152       153  \\\n",
       "customer_id                                                              \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb... -0.001816 -0.003697   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26... -0.001816 -0.003697   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80... -0.001816 -0.003697   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233... -0.001816 -0.003697   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad... -0.001816 -0.003697   \n",
       "\n",
       "                                                         154       155  \\\n",
       "customer_id                                                              \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb... -0.059517 -0.079378   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26... -0.059517  0.015071   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80... -0.059517 -0.152618   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233... -0.059517  0.017433   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad... -0.059517  0.014545   \n",
       "\n",
       "                                                         156  statement_delta  \n",
       "customer_id                                                                    \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb... -0.026433                1  \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26... -0.026433                1  \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80... -0.026433                1  \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233... -0.026433                1  \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad... -0.026433                1  \n",
       "\n",
       "[5 rows x 158 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get samples for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data obs.: 226902\n",
      "Test data obs: 226891\n"
     ]
    }
   ],
   "source": [
    "# First vale of \"usefraction\" specifies the train size and the second, the test size (fraction of total train data available)\n",
    "X_train, X_test, y_train, y_test = get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction = [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB\n",
    "feature selection loop using feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, Duration: 42.435 s, Score: 0.75135, Number of features: 157\n",
      "Iter 2, Duration: 31.302 s, Score: 0.75066, Number of features: 110\n",
      "Iter 3, Duration: 20.132 s, Score: 0.74851, Number of features: 63\n",
      "Iter 4, First fit yielded too low score, trying again (number of features attempted: 30\n",
      "Iter 4, Duration: 22.949 s, Score: 0.73028 - Iteration failed, too large accuracy loss ([-0.02107121])\n",
      "Current feature reduction rate: 0.15\n",
      "Iter 5, Duration: 19.796 s, Score: 0.74851, Number of features: 63\n",
      "Iter 6, Duration: 18.417 s, Score: 0.74886, Number of features: 59\n",
      "Iter 7, Duration: 16.907 s, Score: 0.74652, Number of features: 50\n",
      "Iter 8, Duration: 14.881 s, Score: 0.74465, Number of features: 42\n",
      "Iter 9, First fit yielded too low score, trying again (number of features attempted: 35\n",
      "Iter 9, Duration: 24.422 s, Score: 0.73964 - Iteration failed, too large accuracy loss ([-0.01171986])\n",
      "Current feature reduction rate: 0.075\n",
      "Iter 10, Duration: 14.815 s, Score: 0.74465, Number of features: 42\n",
      "Iter 11, Duration: 14.001 s, Score: 0.74376, Number of features: 40\n",
      "Iter 12, First fit yielded too low score, trying again (number of features attempted: 37\n",
      "Iter 12, Duration: 26.07 s, Score: 0.74077 - Iteration failed, too large accuracy loss ([-0.01058889])\n",
      "Current feature reduction rate: 0.0375\n",
      "Iter 13, Duration: 13.918 s, Score: 0.74376, Number of features: 40\n",
      "Iter 14, Duration: 14.686 s, Score: 0.74156, Number of features: 39\n",
      "Iter 15, Duration: 13.576 s, Score: 0.7416, Number of features: 38\n",
      "Iter 16, First fit yielded too low score, trying again (number of features attempted: 37\n",
      "Iter 16, Duration: 26.553 s, Score: 0.74132 - Iteration failed, too large accuracy loss ([-0.01003272])\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "feature_reduction_rate = 0.3 # Attempt to remove 10% of remaining features in each loop iteration\n",
    "accuracy_loss = 0.01 # accepted accuracy loss c.f. max accuracy\n",
    "scoring_method = 'delta0'\n",
    "\n",
    "\n",
    "scores = []\n",
    "feature_ind = [list(range(0,(X_train.shape[1]-1)))]\n",
    "feature_names = [cols_list]\n",
    "remove_n_features = [int(len(feature_ind[0])*feature_reduction_rate)]\n",
    "i = 0\n",
    "\n",
    "\n",
    "def get_reduced_features(xgbc, feature_names, remove_n_features):\n",
    "    xgb_feature_imp = pd.DataFrame({'name':feature_names,\n",
    "                                    'importances':[val[0] for val in xgbc.feature_importances_.reshape(-1,1)]})\n",
    "\n",
    "    xgb_feature_imp = xgb_feature_imp.sort_values(by='importances', ascending=False)\n",
    "    \n",
    "    df_features = xgb_feature_imp.iloc[:-remove_n_features,:]\n",
    "    feature_ind = df_features.index\n",
    "    feature_names = df_features['name']\n",
    "\n",
    "\n",
    "    return feature_ind, feature_names\n",
    "\n",
    "\n",
    "\n",
    "def fit_predict(feature_ind, feature_names, remove_n_features, X_train, y_train, X_test, y_test):\n",
    "    xgbc = xgb.XGBClassifier(use_label_encoder=False).fit(X_train[feature_ind], y_train, verbose=0, eval_metric='logloss')\n",
    "    \n",
    "    y_pred_a_xgb = pd.DataFrame({'customer_id':X_test.index.values,\n",
    "                            'scoring_var':X_test.iloc[:,-1].values,\n",
    "                            'prediction':[val[1] for val in xgbc.predict_proba(X_test[feature_ind])]})\n",
    "    \n",
    "    if scoring_method == 'last_statement':\n",
    "        proba_xgb = y_pred_a_xgb[y_pred_a_xgb['scoring_var']==1].set_index('customer_id')\n",
    "\n",
    "    elif scoring_method == 'delta0':\n",
    "        proba_xgb = y_pred_a_xgb[y_pred_a_xgb['scoring_var']==0].set_index('customer_id')\n",
    "\n",
    "    elif scoring_method == 'delta1':\n",
    "        proba_xgb = y_pred_a_xgb[y_pred_a_xgb['scoring_var']==1].set_index('customer_id')\n",
    "\n",
    "    elif scoring_method == 'average-deltas':\n",
    "        proba_xgb = y_pred_a_xgb.groupby('customer_id')['prediction'].mean()\n",
    "    \n",
    "    score = [amex_metric(y_test.groupby(y_test.index).max().rename(columns={0:'target'}), proba_xgb)]\n",
    "    feature_ind, feature_names = get_reduced_features(xgbc, feature_names, remove_n_features)\n",
    "\n",
    "    return score, feature_ind, feature_names, xgbc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    cscore, cfeature_ind, cfeature_names, cxgb = fit_predict(feature_ind[i], feature_names[i], remove_n_features[i], X_train, y_train, X_test, y_test) # \"c\" denotes current\n",
    "    scores += cscore # add score to list\n",
    "\n",
    "    if i > 0:\n",
    "        if (max(scores) - scores[i]) >= accuracy_loss: # Maximum residual between max and current score\n",
    "            \n",
    "            print(f'Iter {i+1}, First fit yielded too low score, trying again (number of features attempted: {len(feature_ind[i])}')\n",
    "            cscore, cfeature_ind, cfeature_names, cxgb = fit_predict(feature_ind[i], feature_names[i], remove_n_features[i], X_train, y_train, X_test, y_test) # \"c\" denotes current\n",
    "\n",
    "            if (max(scores) - cscore) >= accuracy_loss: # use same criterion again\n",
    "                \n",
    "                # print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Iteration failed, too large accuracy loss ({cscore-max(scores)}), removing fewer features')\n",
    "                print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Score: {round(scores[i],5)} - Iteration failed, too large accuracy loss ({cscore-max(scores)})')\n",
    "                feature_ind += [feature_ind[i-1]] # add list of feature indices to list\n",
    "                feature_names += [feature_names[i-1]] # add list of feature names to list\n",
    "\n",
    "                feature_reduction_rate = feature_reduction_rate*0.5 # decrease feature reduction rate if iteration failed\n",
    "\n",
    "                if int(len(feature_ind[i])*feature_reduction_rate) >= 1: # Check that at least 1 feature can be removed\n",
    "                    remove_n_features += [int(len(feature_ind[i])*feature_reduction_rate)]\n",
    "                    print(f'Current feature reduction rate: {feature_reduction_rate}')\n",
    "                    i += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print('Completed')\n",
    "                    break\n",
    "\n",
    "\n",
    "            else:\n",
    "                scores[i] = cscore # Overwrite first fit score with 2nd fit score\n",
    "                print(f'Iter {i+1},  Retry successful')\n",
    "\n",
    "\n",
    "\n",
    "    feature_ind += [cfeature_ind] # add list of feature indices to list\n",
    "    feature_names += [cfeature_names] # add list of feature names to list\n",
    "    remove_n_features += [int(len(feature_ind[i])*feature_reduction_rate)]\n",
    "    \n",
    "    print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Score: {round(scores[i],5)}, Number of features: {len(feature_ind[i])}')\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "savelist = list(feature_names[len(feature_names)-2])\n",
    "\n",
    "f = open('lag_features.pickle', 'wb')\n",
    "pickle.dump(savelist, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('lag_features.pickle', 'rb')\n",
    "loadlist = pickle.load(f)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca7b95862a80fe182a4f5fb98c5ac1654efde8b126ad722bfd53b4f5adb0e8de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
