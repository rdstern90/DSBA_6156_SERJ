{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "rand_state = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipelines and functions \n",
    "1. Preprocessing 2. Sampling 3. Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines: Defining the categorical imputation and one-hot encoder for categorical variables.\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "        # (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)), #Commented out because the categorical variables won't play nice with dummies between test/train. Retry when we do a full train model. Can impute values on test_data.csv if necessary.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining the numerical imputation and standard scaler for numerical variables.\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    "           #(\"scale\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# def_prep_df: Preparing the TRAINING data for creating and testing the model.\n",
    "def prep_df(df, target, target_to_drop):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "    # save statement_age column\n",
    "    statement_age_s = df['statement_age']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=[\"s_2\", 'statement_age', target_to_drop])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    over_threshold\n",
    "    \n",
    "\n",
    "    df.drop(over_threshold.index, \n",
    "            axis=1, \n",
    "            inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols_all = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_66', 'd_68', 'b_31', 'd_87']\n",
    "    cat_cols = [col for col in X.columns.str.lower() if col in cat_cols_all]\n",
    "    num_cols = [col for col in X.columns.str.lower() if col not in cat_cols]\n",
    "    \n",
    "    # get dummies for categorical variables\n",
    "    Xcat = pd.get_dummies(X[cat_cols], columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    X = pd.concat([X[num_cols],Xcat], axis=1)\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_age_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "            y.values.reshape(-1, 1)\n",
    "            )\n",
    "    y_processed = pd.DataFrame(y_processed, index=df_index)\n",
    "\n",
    "    \n",
    "    return X_processed, y_processed, cols_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction):\n",
    "    n = 100\n",
    "    ids = np.array(df_train_y.index)\n",
    "    target = np.array(df_train_y['target'])\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=rand_state)\n",
    "    skf.get_n_splits(ids, target)\n",
    "\n",
    "    i = 0\n",
    "    id_subsets = [None]*n\n",
    "    for _, subset in skf.split(ids, target):\n",
    "        id_subsets[i] =list(ids[subset])\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    list1 = list(np.arange(0, int(usefraction[0]*100), 1))\n",
    "    list2 = list(np.arange(int(usefraction[0]*100), int(usefraction[0]*100)+int(usefraction[1]*100), 1))\n",
    "\n",
    "\n",
    "    train_ids = []\n",
    "    for i in list1:\n",
    "        train_ids.extend(id_subsets[i])\n",
    "    test_ids = []\n",
    "    for i in list2:\n",
    "        test_ids.extend(id_subsets[i])\n",
    "\n",
    "\n",
    "    X_train = X_processed[df_train.index.isin(train_ids)]\n",
    "    y_train = y_processed[df_train.index.isin(train_ids)]\n",
    "    X_test = X_processed[df_train.index.isin(test_ids)]\n",
    "    y_test = y_processed[df_train.index.isin(test_ids)]\n",
    "\n",
    "    indices_train = df_train.index.isin(train_ids)\n",
    "    indices_test = df_train.index.isin(test_ids)\n",
    "\n",
    "\n",
    "    print(f'Train data obs.: {len(X_train)}')\n",
    "    print(f'Test data obs: {len(X_test)}')\n",
    "\n",
    "    # also extract the statement dates for combining the predictions later on\n",
    "    # train_statement_age = X_train['statement_age']\n",
    "    # test_statement_age = X_test['statement_age']\n",
    "    # X_train = X_train.drop(columns='statement_age')\n",
    "    # X_test = X_test.drop(columns='statement_age')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create initial df to be further processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = pd.read_parquet('C:/Users/joebu/programming_directory/DSBA_6156_SERJ/ignore/train.parquet')\n",
    "df_train_x.columns = df_train_x.columns.str.lower()\n",
    "df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "df_train_x = df_train_x.set_index('customer_id')\n",
    "\n",
    "df_train_y = pd.read_csv('C:/Users/joebu/programming_directory/DSBA_6156_SERJ/ignore/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.merge(df_train_x, df_train_y, left_index=True, right_on='customer_id', how='left')\n",
    "\n",
    "df_train['last_statement_flag'] = (df_train.groupby(df_train.index)['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(int)\n",
    "                   )\n",
    "\n",
    "df_train['last_statement_target'] = df_train['target']*df_train['last_statement_flag'].apply(lambda x: 1 if x==1 else 0)\n",
    "df_train = df_train.rename(columns={'last_statement_flag':'statement_age'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which statements to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_statements = [1,2,3]\n",
    "df_train = df_train[df_train['statement_age'].isin(use_statements)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the data after selecting statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360401, 204)\n"
     ]
    }
   ],
   "source": [
    "# Prep the dataframe\n",
    "# Note that the last column 'statement_age' is left in the dataframes for scoring, not for predicting!\n",
    "X_processed, y_processed, cols_list = prep_df(df_train, target='target', target_to_drop='last_statement_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get samples for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data obs.: 680246\n",
      "Test data obs: 680155\n"
     ]
    }
   ],
   "source": [
    "# First vale of \"usefraction\" specifies the train size and the second, the test size (fraction of total train data available)\n",
    "X_train, X_test, y_train, y_test = get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction = [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB\n",
    "feature selection loop using feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, Duration: 203.252 s, Score: 0.77687, Number of features: 203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2, Duration: 190.664 s, Score: 0.77536, Number of features: 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3, Duration: 137.927 s, Score: 0.77652, Number of features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4, First fit yielded too low score, trying again (number of features attempted: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4, Duration: 175.402 s, Score: 0.77235 - Iteration failed, too large accuracy loss ([-0.00451821])\n",
      "Current feature reduction rate: 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5, Duration: 130.052 s, Score: 0.77652, Number of features: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6, Duration: 123.325 s, Score: 0.77565, Number of features: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 7, Duration: 109.375 s, Score: 0.77533, Number of features: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 8, Duration: 94.095 s, Score: 0.77523, Number of features: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9, First fit yielded too low score, trying again (number of features attempted: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 9, Duration: 171.374 s, Score: 0.77251 - Iteration failed, too large accuracy loss ([-0.00435888])\n",
      "Current feature reduction rate: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10, Duration: 105.82 s, Score: 0.77523, Number of features: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11, First fit yielded too low score, trying again (number of features attempted: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11, Duration: 186.755 s, Score: 0.77376 - Iteration failed, too large accuracy loss ([-0.00311112])\n",
      "Current feature reduction rate: 0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 12, Duration: 90.767 s, Score: 0.77523, Number of features: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 13, Duration: 95.585 s, Score: 0.77411, Number of features: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14, First fit yielded too low score, trying again (number of features attempted: 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 14, Duration: 184.799 s, Score: 0.77372 - Iteration failed, too large accuracy loss ([-0.00315033])\n",
      "Current feature reduction rate: 0.015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 15, Duration: 92.275 s, Score: 0.77411, Number of features: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 16, Duration: 91.453 s, Score: 0.77389, Number of features: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17, First fit yielded too low score, trying again (number of features attempted: 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "c:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17, Duration: 178.334 s, Score: 0.77366 - Iteration failed, too large accuracy loss ([-0.00320683])\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "def get_reduced_features(xgbc, feature_names, remove_n_features):\n",
    "\n",
    "    xgb_feature_imp = pd.DataFrame({'name':feature_names,\n",
    "                                    'importances':[val[0] for val in xgbc.feature_importances_.reshape(-1,1)]})\n",
    "\n",
    "    xgb_feature_imp = xgb_feature_imp.sort_values(by='importances', ascending=False)\n",
    "    \n",
    "    df_features = xgb_feature_imp.iloc[:-remove_n_features,:]\n",
    "    feature_ind = df_features.index\n",
    "    feature_names = df_features['name']\n",
    "\n",
    "\n",
    "    return feature_ind, feature_names\n",
    "\n",
    "\n",
    "\n",
    "def fit_predict(feature_ind, feature_names, remove_n_features, X_train, y_train, X_test, y_test):\n",
    "    xgbc = xgb.XGBClassifier(use_label_encoder=False).fit(X_train[feature_ind], y_train, verbose=0, eval_metric='logloss')\n",
    "    \n",
    "    y_pred_a_xgb = pd.DataFrame({'customer_id':X_test.index.values,\n",
    "                            'statement_age':X_test.iloc[:,-1].values,\n",
    "                            'prediction':[val[1] for val in xgbc.predict_proba(X_test[feature_ind])]})\n",
    "    \n",
    "    last_proba_xgb = y_pred_a_xgb[y_pred_a_xgb['statement_age']==1].set_index('customer_id')\n",
    "    score = [amex_metric(y_test.groupby(y_test.index).max().rename(columns={0:'target'}), last_proba_xgb)]\n",
    "\n",
    "    feature_ind, feature_names = get_reduced_features(xgbc, feature_names, remove_n_features)\n",
    "\n",
    "    return score, feature_ind, feature_names, xgbc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_reduction_rate = 0.25 # Attempt to remove 25% of remaining features in each loop iteration\n",
    "accuracy_loss = 0.003 # accepted accuracy loss c.f. max accuracy\n",
    "\n",
    "scores = []\n",
    "feature_ind = [list(range(0,(X_train.shape[1]-1)))]\n",
    "feature_names = [cols_list]\n",
    "remove_n_features = [int(len(feature_ind[0])*feature_reduction_rate)]\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    cscore, cfeature_ind, cfeature_names, cxgb = fit_predict(feature_ind[i], feature_names[i], remove_n_features[i], X_train, y_train, X_test, y_test) # \"c\" denotes current\n",
    "    scores += cscore # add score to list\n",
    "\n",
    "    if i > 0:\n",
    "        if (max(scores) - scores[i]) >= accuracy_loss: # Maximum residual between max and current score\n",
    "            \n",
    "            print(f'Iter {i+1}, First fit yielded too low score, trying again (number of features attempted: {len(feature_ind[i])}')\n",
    "            cscore, cfeature_ind, cfeature_names, cxgb = fit_predict(feature_ind[i], feature_names[i], remove_n_features[i], X_train, y_train, X_test, y_test) # \"c\" denotes current\n",
    "\n",
    "            if (max(scores) - cscore) >= accuracy_loss: # use same criterion again\n",
    "                \n",
    "                # print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Iteration failed, too large accuracy loss ({cscore-max(scores)}), removing fewer features')\n",
    "                print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Score: {round(scores[i],5)} - Iteration failed, too large accuracy loss ({cscore-max(scores)})')\n",
    "                feature_ind += [feature_ind[i-1]] # add list of feature indices to list\n",
    "                feature_names += [feature_names[i-1]] # add list of feature names to list\n",
    "\n",
    "                feature_reduction_rate = feature_reduction_rate*0.5 # decrease feature reduction rate if iteration failed\n",
    "\n",
    "                if int(len(feature_ind[i])*feature_reduction_rate) >= 1: # Check that at least 1 feature can be removed\n",
    "                    remove_n_features += [int(len(feature_ind[i])*feature_reduction_rate)]\n",
    "                    print(f'Current feature reduction rate: {feature_reduction_rate}')\n",
    "                    i += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print('Completed')\n",
    "                    break\n",
    "\n",
    "\n",
    "            else:\n",
    "                scores[i] = cscore # Overwrite first fit score with 2nd fit score\n",
    "                print(f'Iter {i+1},  Retry successful')\n",
    "\n",
    "\n",
    "\n",
    "    feature_ind += [cfeature_ind] # add list of feature indices to list\n",
    "    feature_names += [cfeature_names] # add list of feature names to list\n",
    "    remove_n_features += [int(len(feature_ind[i])*feature_reduction_rate)]\n",
    "    \n",
    "    print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Score: {round(scores[i],5)}, Number of features: {len(feature_ind[i])}')\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"indices\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(feature_ind[i], fp)\n",
    "   \n",
    "with open(\"names\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(feature_names[i], fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  0,   2,  31,   9,  21,   3,   4,  22, 167,   6,  13,   1, 141,\n",
      "              5,  23, 127, 183, 162, 126,  10,   8, 123,  17, 111,  20,  11,\n",
      "              7,  14,  16,  30, 144, 159,  28, 152, 181,  12,  76,  19,  66,\n",
      "            196, 150,  25,  26,  50, 122,  47,  37, 198,  45,  32,  81,  59,\n",
      "            193, 112,  64,  79,  48,  27,  24,  90,  34,  15, 114,  60,  46,\n",
      "            153,  44,  89],\n",
      "           dtype='int64')\n",
      "0        p_2\n",
      "2        b_1\n",
      "31       r_4\n",
      "9       d_44\n",
      "21      d_51\n",
      "       ...  \n",
      "60      s_15\n",
      "46      d_65\n",
      "153    d_141\n",
      "44      s_11\n",
      "89      s_18\n",
      "Name: name, Length: 68, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with open(\"indices\", \"rb\") as fp:   # Unpickling\n",
    "    ind = pickle.load(fp)\n",
    "    \n",
    "print(ind)\n",
    "\n",
    "with open(\"names\", \"rb\") as fp:   # Unpickling\n",
    "    namez = pickle.load(fp)\n",
    "    \n",
    "print(namez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p_2', 'b_1', 'r_4', 'd_44', 'd_51', 'b_2', 'r_1', 'b_9', 'd_66_1', 'd_41', 'r_2', 'd_39', 'd_129', 's_3', 'r_3', 'd_112', 'b_38_4', 'd_64_0', 'd_111', 'b_4', 'd_43', 'r_27', 'd_49', 's_23', 'b_8', 'd_45', 'b_3', 'd_46', 'd_48', 'd_54', 'd_131', 'd_63_3', 'b_11', 'd_140', 'b_38_2', 'b_5', 'r_11', 'b_7', 'r_7', 'd_117_6', 'd_138', 'p_3', 'b_10', 'b_20', 'r_26', 'b_16', 'r_5', 'd_120_0', 'd_62', 's_7', 'r_12', 'd_72', 'd_117_3', 's_24', 'd_75', 'd_82', 'b_18', 's_5', 'd_52', 'd_86', 's_8', 'd_47', 's_26', 's_15', 'd_65', 'd_141', 's_11', 's_18']\n"
     ]
    }
   ],
   "source": [
    "print(namez.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring with the final set of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  0,   2,  31,   9,  21,   3,   4,  22, 167,   6,  13,   1, 141,\n",
      "              5,  23, 127, 183, 162, 126,  10,   8, 123,  17, 111,  20,  11,\n",
      "              7,  14,  16,  30, 144, 159,  28, 152, 181,  12,  76,  19,  66,\n",
      "            196, 150,  25,  26,  50, 122,  47,  37, 198,  45,  32,  81,  59,\n",
      "            193, 112,  64,  79,  48,  27,  24,  90,  34,  15, 114,  60,  46,\n",
      "            153,  44,  89],\n",
      "           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Read in the column names we get from feature reduction\n",
    "with open(\"C:/Users/joebu/programming_directory/DSBA_6156_SERJ/data/indices\", \"rb\") as fp:   # Unpickling\n",
    "    ind = pickle.load(fp)\n",
    "\n",
    "# ind = ind.tolist()\n",
    "# ind = ind.append('statement_age')\n",
    "print(ind)\n",
    "X_processed = X_processed[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier(use_label_encoder=False).fit(X_processed, y_processed, verbose=0, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet('C:/Users/joebu/programming_directory/DSBA_6156_SERJ/ignore/train.parquet')\n",
    "df_test.columns = df_test.columns.str.lower()\n",
    "df_test = df_test.sort_values(['customer_id', 's_2'])\n",
    "df_test = df_test.set_index('customer_id')\n",
    "\n",
    "df_test['last_statement_flag'] = (df_test.groupby(df_test.index)['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(int)\n",
    "                   )\n",
    "\n",
    "df_test = df_test.rename(columns={'last_statement_flag':'statement_age'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep test data\n",
    "# def_prep_df: Preparing the TRAINING data for creating and testing the model.\n",
    "def prep_df(df):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "    # save statement_age column\n",
    "    statement_age_s = df['statement_age']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=[\"s_2\", 'statement_age'])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    over_threshold\n",
    "    \n",
    "\n",
    "    df.drop(over_threshold.index, \n",
    "            axis=1, \n",
    "            inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols_all = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_66', 'd_68', 'b_31', 'd_87']\n",
    "    cat_cols = [col for col in X.columns.str.lower() if col in cat_cols_all]\n",
    "    num_cols = [col for col in X.columns.str.lower() if col not in cat_cols]\n",
    "    \n",
    "    # get dummies for categorical variables\n",
    "    Xcat = pd.get_dummies(X[cat_cols], columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    X = pd.concat([X[num_cols],Xcat], axis=1)\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_age_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "            y.values.reshape(-1, 1)\n",
    "            )\n",
    "    y_processed = pd.DataFrame(y_processed, index=df_index)\n",
    "\n",
    "    \n",
    "    return X_processed, y_processed, cols_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dcb6b7e2f26f70306738e96e7445a78b658183e0554cf15e38890769044d88c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
