{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "rand_state = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prep_df function modified to discard categorical features and impute values before delta calcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Pipelines: Defining the categorical imputation and one-hot encoder for categorical variables.\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "        # (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)), #Commented out because the categorical variables won't play nice with dummies between test/train. Retry when we do a full train model. Can impute values on test_data.csv if necessary.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining the numerical imputation and standard scaler for numerical variables.\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\"))] \n",
    "        #    (\"scale\", StandardScaler())] # don't scale prior to feature engineering\n",
    "           #(\"scale\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# def_prep_df: Preparing the TRAINING data for creating and testing the model.\n",
    "def prep_df(df, target, target_to_drop):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "    # save statement_age & oldest_statement columns\n",
    "    statement_age_s = df['statement_age']\n",
    "    oldest_statement_s = df['oldest_statement']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=[\"s_2\", 'statement_age', 'oldest_statement', target_to_drop])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    over_threshold\n",
    "    \n",
    "\n",
    "    df.drop(over_threshold.index, \n",
    "            axis=1, \n",
    "            inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols_all = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_66', 'd_68', 'b_31', 'd_87']\n",
    "    cat_cols = [col for col in X.columns.str.lower() if col in cat_cols_all]\n",
    "    num_cols = [col for col in X.columns.str.lower() if col not in cat_cols]\n",
    "    \n",
    "    # get dummies for categorical variables\n",
    "    # Xcat = pd.get_dummies(X[cat_cols], columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    # X = pd.concat([X[num_cols],Xcat], axis=1)\n",
    "\n",
    "    X = X[num_cols]\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    # cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols)\n",
    "        # (\"categorical\", categorical_pipeline, cat_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Apply preprocessing (impute)\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_age_s, oldest_statement_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    y_processed = pd.DataFrame(y, index=df_index)\n",
    "    print(y_processed.shape)\n",
    " \n",
    "    \n",
    "    return X_processed, y_processed, cols_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prep_df_test(df):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "    # save statement_age & oldest_statement columns\n",
    "    statement_age_s = df['statement_age']\n",
    "    oldest_statement_s = df['oldest_statement']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=[\"s_2\", 'statement_age', 'oldest_statement'])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    \n",
    "\n",
    "    df.drop(over_threshold.index, \n",
    "            axis=1, \n",
    "            inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols_all = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_66', 'd_68', 'b_31', 'd_87']\n",
    "    cat_cols = [col for col in X.columns.str.lower() if col in cat_cols_all]\n",
    "    num_cols = [col for col in X.columns.str.lower() if col not in cat_cols]\n",
    "    \n",
    "    # get dummies for categorical variables\n",
    "    # Xcat = pd.get_dummies(X[cat_cols], columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    # X = pd.concat([X[num_cols],Xcat], axis=1)\n",
    "\n",
    "    X = X[num_cols]\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    # cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols)\n",
    "        # (\"categorical\", categorical_pipeline, cat_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Apply preprocessing (impute)\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_age_s, oldest_statement_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    y_processed = pd.DataFrame(y, index=df_index)\n",
    "    print(y_processed.shape)\n",
    " \n",
    "    \n",
    "    return X_processed, y_processed, cols_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create initial train_df to be further processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = pd.read_parquet('./../ignore/train.parquet')\n",
    "df_train_x.columns = df_train_x.columns.str.lower()\n",
    "df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "df_train_x = df_train_x.set_index('customer_id')\n",
    "\n",
    "df_train_y = pd.read_csv('./../ignore/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.merge(df_train_x, df_train_y, left_index=True, right_on='customer_id', how='left')\n",
    "\n",
    "df_train['statement_age'] = (df_train.groupby(df_train.index)['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(int))\n",
    "\n",
    "oldest_statement = df_train.groupby(df_train.index)['statement_age'].max().rename('oldest_statement')\n",
    "df_train =  df_train.join(oldest_statement, how='left')\n",
    "                      \n",
    "\n",
    "df_train['last_statement_target'] = df_train['target']*df_train['statement_age'].apply(lambda x: 1 if x==1 else 0)\n",
    "## df_train = df_train.rename(columns={'last_statement_flag':'statement_age'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['oldest_statement'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute train_df values before calculating deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531451, 159)\n",
      "(5531451, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prep the dataframe\n",
    "# Note that the last column 'statement_age' is left in the dataframes for scoring, not for predicting!\n",
    "\n",
    "# Impute numerical and drop categorical values\n",
    "X_processed, y_processed, cols_list = prep_df(df_train, target='target', target_to_drop='last_statement_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create deltas (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta between last (statement_age == 1) and first (statement_age >= 2)\n",
    "# also remove customers with only one statement\n",
    "\n",
    "delta1 = X_processed[((X_processed['statement_age']==1) & (X_processed['oldest_statement'] >=2)) |\n",
    "                                    ((X_processed['statement_age'] == X_processed['oldest_statement']) & (X_processed['oldest_statement'] >=2))]\n",
    "delta1 = delta1.diff(periods=1)\n",
    "delta1 = delta1[delta1['statement_age'] < 0]\n",
    "delta1['statement_delta'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# # Delta between last and 2nd last statement (1 & 2), 2 & 3, 3 & 4\n",
    "# # also remove customers with only one statement\n",
    "# delta2 = X_processed[((X_processed['statement_age']==1) & (X_processed['oldest_statement'] >=2)) |\n",
    "#                                     ((X_processed['statement_age'] == 2) & (X_processed['oldest_statement'] >=2))]\n",
    "# delta2 = delta2.diff(periods=1)\n",
    "# delta2 = delta2[delta2['statement_age'] < 0]\n",
    "# delta2['statement_delta'] = 1\n",
    "# delta_df = pd.concat([delta1, delta2], axis=0)\n",
    "\n",
    "\n",
    "# delta3 = X_processed[((X_processed['statement_age']==2) & (X_processed['oldest_statement'] >=2)) |\n",
    "#                                     ((X_processed['statement_age'] == 3) & (X_processed['oldest_statement'] >=2))]\n",
    "# delta3 = delta3.diff(periods=1)\n",
    "# delta3 = delta3[delta3['statement_age'] < 0]\n",
    "# delta3['statement_delta'] = 2\n",
    "# delta_df = pd.concat([delta_df, delta3], axis=0)\n",
    "\n",
    "\n",
    "# delta4 = X_processed[((X_processed['statement_age']==3) & (X_processed['oldest_statement'] >=2)) |\n",
    "#                                     ((X_processed['statement_age'] == 4) & (X_processed['oldest_statement'] >=2))]\n",
    "# delta4 = delta4.diff(periods=1)\n",
    "# delta4 = delta4[delta4['statement_age'] < 0]\n",
    "# delta4['statement_delta'] = 3\n",
    "# delta_df = pd.concat([delta_df, delta4], axis=0).sort_values(by=['customer_id', 'statement_delta'])\n",
    "\n",
    "# delta_df = delta_df.drop(columns=['oldest_statement', 'statement_age'])\n",
    "delta_df = delta1.drop(columns=['oldest_statement', 'statement_age'])\n",
    "\n",
    "cols_list_mod = cols_list \n",
    "i=0\n",
    "for val in cols_list_mod:\n",
    "    cols_list_mod[i] = val+ '_dfl' # delta first last\n",
    "    i+=1\n",
    "    \n",
    "cols_list_mod = cols_list + ['statement_delta']\n",
    "delta_df.columns = cols_list_mod\n",
    "\n",
    "delta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df.to_parquet('./../ignore/train_dfl.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet('./../ignore/test.parquet')\n",
    "df_test.columns = df_test.columns.str.lower()\n",
    "df_test = df_test.sort_values(['customer_id', 's_2'])\n",
    "df_test = df_test.set_index('customer_id')\n",
    "\n",
    "df_test['statement_age'] = (df_test.groupby(df_test.index)['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(int))\n",
    "\n",
    "oldest_statement = df_test.groupby(df_test.index)['statement_age'].max().rename('oldest_statement')\n",
    "df_test =  df_test.join(oldest_statement, how='left')\n",
    "\n",
    "\n",
    "\n",
    "# save indices\n",
    "df_index = df_test.index\n",
    "# save statement_age & oldest_statement columns\n",
    "statement_age_s = df_test['statement_age']\n",
    "oldest_statement_s = df_test['oldest_statement']\n",
    "\n",
    "# Drop columns that shouldn't be scaled or imputed\n",
    "df_test.drop(columns=[\"s_2\", 'statement_age', 'oldest_statement'], inplace=True)\n",
    "\n",
    "# Filter out categorical columns\n",
    "cat_cols_to_remove = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_66', 'd_68', 'b_31', 'd_87']\n",
    "cols_list = [col for col in df_test.columns.str.lower() if col not in cat_cols_to_remove]\n",
    "df_test = df_test[cols_list]\n",
    "\n",
    "# Missing values handling\n",
    "missing_props = df_test.isna().mean(axis=0)\n",
    "over_threshold = missing_props[missing_props >= 0.4]\n",
    "df_test.drop(over_threshold.index, axis=1, inplace=True)\n",
    "\n",
    "cols_list = list(df_test.columns.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute 20 columns at a time as not enough memory can be allocated to do it all at once\n",
    "sublist_size = 20\n",
    "sublists_cols = [cols_list[x:x+sublist_size] for x in range(0, len(cols_list), sublist_size)]\n",
    "\n",
    "def impute_numerical(df):\n",
    "    imputer=SimpleImputer(strategy=\"mean\")\n",
    "    X = pd.DataFrame(imputer.fit_transform(df), columns=df.columns, index=df.index)\n",
    "    return X\n",
    "\n",
    "i = 0\n",
    "for sublist in sublists_cols:\n",
    "    if i == 0:\n",
    "        X = impute_numerical(df_test[sublist])\n",
    "    else:\n",
    "        X = pd.concat([X, impute_numerical(df_test[sublist])], axis=1)\n",
    "    i +=1\n",
    "\n",
    "X_processed = pd.concat([X, statement_age_s, oldest_statement_s], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers with 2 or more statements: 918794\n",
      "Number of customers with 1 statement: 5827\n"
     ]
    }
   ],
   "source": [
    "customers = list(X_processed[X_processed['oldest_statement']!=1].index.unique())\n",
    "customers_1stmt = list(X_processed[X_processed['oldest_statement']==1].index.unique())\n",
    "# customers = [val for val in customers if val not in customers_1stmt]\n",
    "\n",
    "sublist_size = 300000\n",
    "sublists_cust = [customers[x:x+sublist_size] for x in range(0, len(customers), sublist_size)]\n",
    "print('Number of customers with 2 or more statements:', len(customers))\n",
    "print('Number of customers with 1 statement:', len(customers_1stmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lag(df,stmnt2,stmnt1): # delta = stmnt2 - stmnt1\n",
    "    if stmnt1 =='oldest':\n",
    "        delta = df[(df['statement_age']==stmnt2) |(df['statement_age'] == df['oldest_statement'])]\n",
    "        delta = delta.diff(periods=1)\n",
    "        delta = delta[delta['statement_age'] <= 0]\n",
    "        delta['statement_delta'] = 0\n",
    "    else:\n",
    "        delta = df[(df['statement_age']==stmnt2) |(df['statement_age'] == stmnt1)]\n",
    "        delta = delta.diff(periods=1)\n",
    "        delta = delta[delta['statement_age'] <= 0]\n",
    "        delta['statement_delta'] = stmnt2\n",
    "    return delta\n",
    "\n",
    "\n",
    "stmnt1 = 'oldest'\n",
    "stmnt2 = 1 # 1=most recent\n",
    "\n",
    "i=0\n",
    "for sublist in sublists_cust:\n",
    "    if i == 0:\n",
    "        delta_df = calc_lag(X_processed[X_processed.index.isin(sublist)], stmnt2, stmnt1)\n",
    "    else:\n",
    "        delta_df = pd.concat([delta_df, calc_lag(X_processed[X_processed.index.isin(sublist)], stmnt2, stmnt1)], axis=0)\n",
    "    i +=1\n",
    "\n",
    "# Drop columns\n",
    "delta_df.drop(columns=['statement_age', 'oldest_statement'], inplace=True)\n",
    "\n",
    "# Add rows with all nulls for the customers with only 1 statement\n",
    "delta_df = pd.concat([delta_df, pd.DataFrame(index=customers_1stmt, columns=cols_list)], axis=0)\n",
    "\n",
    "# Impute values\n",
    "imputer=SimpleImputer(strategy=\"mean\")\n",
    "out = imputer.fit_transform(delta_df)\n",
    "\n",
    "cols_list_mod = [val+'_dfl' for val in cols_list] + ['statement_delta']\n",
    "delta_df = pd.DataFrame(out, index=delta_df.index, columns=cols_list_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df.to_parquet('./../ignore/test_dfl.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca7b95862a80fe182a4f5fb98c5ac1654efde8b126ad722bfd53b4f5adb0e8de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
