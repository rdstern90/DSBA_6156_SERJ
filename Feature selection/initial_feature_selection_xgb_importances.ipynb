{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "rand_state = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipelines and functions \n",
    "1. Preprocessing 2. Sampling 3. Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines: Defining the categorical imputation and one-hot encoder for categorical variables.\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining the numerical imputation and standard scaler for numerical variables.\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# def_prep_df: Preparing the TRAINING data for creating and testing the model.\n",
    "def prep_df(df, target, target_to_drop):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "    # save statement_age column\n",
    "    statement_age_s = df['statement_age']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=[\"s_2\", 'statement_age', target_to_drop])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    over_threshold\n",
    "    \n",
    "\n",
    "    df.drop(over_threshold.index, axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols_all = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_66', 'd_68', 'b_31', 'd_87']\n",
    "    cat_cols = [col for col in X.columns.str.lower() if col in cat_cols_all]\n",
    "    num_cols = [col for col in X.columns.str.lower() if col not in cat_cols]\n",
    "    \n",
    "    # get dummies for categorical variables\n",
    "    Xcat = pd.get_dummies(X[cat_cols], columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    X = pd.concat([X[num_cols],Xcat], axis=1)\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_age_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "            y.values.reshape(-1, 1)\n",
    "            )\n",
    "    y_processed = pd.DataFrame(y_processed, index=df_index)\n",
    "\n",
    "    \n",
    "    return X_processed, y_processed, cols_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction):\n",
    "    n = 100\n",
    "    ids = np.array(df_train_y.index)\n",
    "    target = np.array(df_train_y['target'])\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=rand_state)\n",
    "    skf.get_n_splits(ids, target)\n",
    "\n",
    "    i = 0\n",
    "    id_subsets = [None]*n\n",
    "    for _, subset in skf.split(ids, target):\n",
    "        id_subsets[i] =list(ids[subset])\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    list1 = list(np.arange(0, int(usefraction[0]*100), 1))\n",
    "    list2 = list(np.arange(int(usefraction[0]*100), int(usefraction[0]*100)+int(usefraction[1]*100), 1))\n",
    "\n",
    "\n",
    "    train_ids = []\n",
    "    for i in list1:\n",
    "        train_ids.extend(id_subsets[i])\n",
    "    test_ids = []\n",
    "    for i in list2:\n",
    "        test_ids.extend(id_subsets[i])\n",
    "\n",
    "\n",
    "    X_train = X_processed[df_train.index.isin(train_ids)]\n",
    "    y_train = y_processed[df_train.index.isin(train_ids)]\n",
    "    X_test = X_processed[df_train.index.isin(test_ids)]\n",
    "    y_test = y_processed[df_train.index.isin(test_ids)]\n",
    "\n",
    "\n",
    "    print(f'Train data obs.: {len(X_train)}')\n",
    "    print(f'Test data obs: {len(X_test)}')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create initial df to be further processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = pd.read_parquet('./../ignore/train.parquet')\n",
    "df_train_x.columns = df_train_x.columns.str.lower()\n",
    "df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "df_train_x = df_train_x.set_index('customer_id')\n",
    "\n",
    "df_train_y = pd.read_csv('./../ignore/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.merge(df_train_x, df_train_y, left_index=True, right_on='customer_id', how='left')\n",
    "\n",
    "df_train['last_statement_flag'] = (df_train.groupby(df_train.index)['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(int)\n",
    "                   )\n",
    "\n",
    "df_train['last_statement_target'] = df_train['target']*df_train['last_statement_flag'].apply(lambda x: 1 if x==1 else 0)\n",
    "df_train = df_train.rename(columns={'last_statement_flag':'statement_age'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which statements to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_statements = [1,2,3]\n",
    "df_train = df_train[df_train['statement_age'].isin(use_statements)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the data after selecting statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360401, 204)\n"
     ]
    }
   ],
   "source": [
    "# Prep the dataframe\n",
    "# Note that the last column 'statement_age' is left in the dataframes for scoring, not for predicting!\n",
    "X_processed, y_processed, cols_list = prep_df(df_train, target='target', target_to_drop='last_statement_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get samples for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data obs.: 952359\n",
      "Test data obs: 408042\n"
     ]
    }
   ],
   "source": [
    "# First vale of \"usefraction\" specifies the train size and the second, the test size (fraction of total train data available)\n",
    "X_train, X_test, y_train, y_test = get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction = [0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB\n",
    "feature selection loop using feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1, Duration: 221.322 s, Score: 0.77797, Number of features: 203\n",
      "Iter 2, Duration: 213.88 s, Score: 0.77827, Number of features: 173\n",
      "Iter 3, Duration: 185.148 s, Score: 0.77926, Number of features: 143\n",
      "Iter 4, Duration: 166.304 s, Score: 0.78018, Number of features: 118\n",
      "Iter 5, First fit yielded too low score, trying again (number of features attempted: 97\n",
      "Iter 5, Duration: 275.991 s, Score: 0.77705 - Iteration failed, too large accuracy loss ([-0.00312886])\n",
      "Current feature reduction rate: 0.075\n",
      "Iter 6, Duration: 167.509 s, Score: 0.78018, Number of features: 118\n",
      "Iter 7, First fit yielded too low score, trying again (number of features attempted: 111\n",
      "Iter 7, Duration: 307.593 s, Score: 0.7789 - Iteration failed, too large accuracy loss ([-0.00128165])\n",
      "Current feature reduction rate: 0.0375\n",
      "Iter 8, Duration: 162.897 s, Score: 0.78018, Number of features: 118\n",
      "Iter 9, First fit yielded too low score, trying again (number of features attempted: 114\n",
      "Iter 9, Duration: 324.193 s, Score: 0.77868 - Iteration failed, too large accuracy loss ([-0.00150475])\n",
      "Current feature reduction rate: 0.01875\n",
      "Iter 10, Duration: 170.939 s, Score: 0.78018, Number of features: 118\n",
      "Iter 11, First fit yielded too low score, trying again (number of features attempted: 116\n",
      "Iter 11, Duration: 320.861 s, Score: 0.77789 - Iteration failed, too large accuracy loss ([-0.00229616])\n",
      "Current feature reduction rate: 0.009375\n",
      "Iter 12, Duration: 156.354 s, Score: 0.78018, Number of features: 118\n",
      "Iter 13, Duration: 160.942 s, Score: 0.77954, Number of features: 117\n",
      "Iter 14, First fit yielded too low score, trying again (number of features attempted: 116\n",
      "Iter 14, Duration: 312.0 s, Score: 0.77891 - Iteration failed, too large accuracy loss ([-0.00127622])\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "feature_reduction_rate = 0.15 # Attempt to remove 25% of remaining features in each loop iteration\n",
    "accuracy_loss = 0.001 # accepted accuracy loss c.f. max accuracy\n",
    "\n",
    "scores = []\n",
    "feature_ind = [list(range(0,(X_train.shape[1]-1)))]\n",
    "feature_names = [cols_list]\n",
    "remove_n_features = [int(len(feature_ind[0])*feature_reduction_rate)]\n",
    "i = 0\n",
    "\n",
    "\n",
    "def get_reduced_features(xgbc, feature_names, remove_n_features):\n",
    "\n",
    "    xgb_feature_imp = pd.DataFrame({'name':feature_names,\n",
    "                                    'importances':[val[0] for val in xgbc.feature_importances_.reshape(-1,1)]})\n",
    "\n",
    "    xgb_feature_imp = xgb_feature_imp.sort_values(by='importances', ascending=False)\n",
    "    \n",
    "    df_features = xgb_feature_imp.iloc[:-remove_n_features,:]\n",
    "    feature_ind = df_features.index\n",
    "    feature_names = df_features['name']\n",
    "\n",
    "\n",
    "    return feature_ind, feature_names\n",
    "\n",
    "\n",
    "\n",
    "def fit_predict(feature_ind, feature_names, remove_n_features, X_train, y_train, X_test, y_test):\n",
    "    xgbc = xgb.XGBClassifier(use_label_encoder=False).fit(X_train[feature_ind], y_train, verbose=0, eval_metric='logloss')\n",
    "    \n",
    "    y_pred_a_xgb = pd.DataFrame({'customer_id':X_test.index.values,\n",
    "                            'statement_age':X_test.iloc[:,-1].values,\n",
    "                            'prediction':[val[1] for val in xgbc.predict_proba(X_test[feature_ind])]})\n",
    "    \n",
    "    last_proba_xgb = y_pred_a_xgb[y_pred_a_xgb['statement_age']==1].set_index('customer_id')\n",
    "    score = [amex_metric(y_test.groupby(y_test.index).max().rename(columns={0:'target'}), last_proba_xgb)]\n",
    "\n",
    "    feature_ind, feature_names = get_reduced_features(xgbc, feature_names, remove_n_features)\n",
    "\n",
    "    return score, feature_ind, feature_names, xgbc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    cscore, cfeature_ind, cfeature_names, cxgb = fit_predict(feature_ind[i], feature_names[i], remove_n_features[i], X_train, y_train, X_test, y_test) # \"c\" denotes current\n",
    "    scores += cscore # add score to list\n",
    "\n",
    "    if i > 0:\n",
    "        if (max(scores) - scores[i]) >= accuracy_loss: # Maximum residual between max and current score\n",
    "            \n",
    "            print(f'Iter {i+1}, First fit yielded too low score, trying again (number of features attempted: {len(feature_ind[i])}')\n",
    "            cscore, cfeature_ind, cfeature_names, cxgb = fit_predict(feature_ind[i], feature_names[i], remove_n_features[i], X_train, y_train, X_test, y_test) # \"c\" denotes current\n",
    "\n",
    "            if (max(scores) - cscore) >= accuracy_loss: # use same criterion again\n",
    "                \n",
    "                # print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Iteration failed, too large accuracy loss ({cscore-max(scores)}), removing fewer features')\n",
    "                print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Score: {round(scores[i],5)} - Iteration failed, too large accuracy loss ({cscore-max(scores)})')\n",
    "                feature_ind += [feature_ind[i-1]] # add list of feature indices to list\n",
    "                feature_names += [feature_names[i-1]] # add list of feature names to list\n",
    "\n",
    "                feature_reduction_rate = feature_reduction_rate*0.5 # decrease feature reduction rate if iteration failed\n",
    "\n",
    "                if int(len(feature_ind[i])*feature_reduction_rate) >= 1: # Check that at least 1 feature can be removed\n",
    "                    remove_n_features += [int(len(feature_ind[i])*feature_reduction_rate)]\n",
    "                    print(f'Current feature reduction rate: {feature_reduction_rate}')\n",
    "                    i += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print('Completed')\n",
    "                    break\n",
    "\n",
    "\n",
    "            else:\n",
    "                scores[i] = cscore # Overwrite first fit score with 2nd fit score\n",
    "                print(f'Iter {i+1},  Retry successful')\n",
    "\n",
    "\n",
    "\n",
    "    feature_ind += [cfeature_ind] # add list of feature indices to list\n",
    "    feature_names += [cfeature_names] # add list of feature names to list\n",
    "    remove_n_features += [int(len(feature_ind[i])*feature_reduction_rate)]\n",
    "    \n",
    "    print(f'Iter {i+1}, Duration: {round((time.time() - start_time),3)} s, Score: {round(scores[i],5)}, Number of features: {len(feature_ind[i])}')\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "savelist = list(feature_names[len(feature_names)-2])\n",
    "\n",
    "# remove the suffix from dummy variables and make sure the list elements are unique\n",
    "savelist = list(np.unique(np.array([re.findall(r'(\\w_\\d+)', x)[0].upper() for x in savelist])))\n",
    "\n",
    "f = open('features2.pickle', 'wb')\n",
    "pickle.dump(savelist, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('features.pickle', 'rb')\n",
    "loadlist = pickle.load(f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('streamlit_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2e507d48191a2742459f290adaaddea3e140f63b4e1a7aa6933fe991de94601"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
