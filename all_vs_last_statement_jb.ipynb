{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring how to flag the target for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies:\n",
    "# Data Wrangling:\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Modeling Packages:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import glob\n",
    "rand_state = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL cloud server credentials:\n",
    "# server ip: 34.75.124.150\n",
    "# username: user\n",
    "# password: DeEJNEAhy\n",
    "# Data is in materialized views train_data and train_labels\n",
    "engine = create_engine('postgresql://user:DeEJNEAhy@34.75.124.150/postgres')\n",
    "sql_df = pd.read_sql(\"\"\"\n",
    "                 WITH BASE AS (\n",
    "                    SELECT *\n",
    "                    ,ROW_NUMBER() OVER      (\n",
    "                                            PARTITION BY customer_id \n",
    "                                            ORDER BY s_2\n",
    "                                            )\n",
    "                    ,ROW_NUMBER() OVER      (\n",
    "                                            PARTITION BY customer_id\n",
    "                                            ORDER BY s_2 DESC\n",
    "                                            ) last_statement_flag_drop\n",
    "                    FROM TRAIN_DATA_random\n",
    "                    )\n",
    "\n",
    "\n",
    "                    SELECT *\n",
    "                    ,CASE WHEN last_statement_flag_drop = 1 then 1 else 0 end as last_statement_flag\n",
    "                    ,CASE WHEN (target = 1 AND last_statement_flag_drop = 1) then 1 else 0 end as last_statement_target\n",
    "                    FROM BASE B\n",
    "                    LEFT JOIN train_labels_random L\n",
    "                    ON B.customer_id = L.customer_id\n",
    "                 \"\"\", engine) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing positive flag on all statements, or only last statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>s_2</th>\n",
       "      <th>p_2</th>\n",
       "      <th>d_39</th>\n",
       "      <th>b_1</th>\n",
       "      <th>b_2</th>\n",
       "      <th>r_1</th>\n",
       "      <th>s_3</th>\n",
       "      <th>d_41</th>\n",
       "      <th>b_3</th>\n",
       "      <th>...</th>\n",
       "      <th>d_142</th>\n",
       "      <th>d_143</th>\n",
       "      <th>d_144</th>\n",
       "      <th>d_145</th>\n",
       "      <th>row_number</th>\n",
       "      <th>last_statement_flag_drop</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>target</th>\n",
       "      <th>last_statement_flag</th>\n",
       "      <th>last_statement_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>2017-03-22</td>\n",
       "      <td>0.871053</td>\n",
       "      <td>0.059789</td>\n",
       "      <td>0.123999</td>\n",
       "      <td>1.000394</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>-0.080754</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>0.874837</td>\n",
       "      <td>0.442610</td>\n",
       "      <td>0.142999</td>\n",
       "      <td>1.008372</td>\n",
       "      <td>0.009232</td>\n",
       "      <td>-0.065012</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.006783</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>0.846435</td>\n",
       "      <td>0.008253</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>1.000745</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>-0.072598</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>2017-06-09</td>\n",
       "      <td>0.819525</td>\n",
       "      <td>0.624685</td>\n",
       "      <td>0.034141</td>\n",
       "      <td>1.000321</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>-0.069551</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>2017-07-27</td>\n",
       "      <td>0.876368</td>\n",
       "      <td>0.214070</td>\n",
       "      <td>0.020723</td>\n",
       "      <td>0.964219</td>\n",
       "      <td>0.006177</td>\n",
       "      <td>-0.078409</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>000548e99fa24cef8377e68e602e4bd70d30500a007999...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id         s_2       p_2  \\\n",
       "0  000548e99fa24cef8377e68e602e4bd70d30500a007999...  2017-03-22  0.871053   \n",
       "1  000548e99fa24cef8377e68e602e4bd70d30500a007999...  2017-04-04  0.874837   \n",
       "2  000548e99fa24cef8377e68e602e4bd70d30500a007999...  2017-05-15  0.846435   \n",
       "3  000548e99fa24cef8377e68e602e4bd70d30500a007999...  2017-06-09  0.819525   \n",
       "4  000548e99fa24cef8377e68e602e4bd70d30500a007999...  2017-07-27  0.876368   \n",
       "\n",
       "       d_39       b_1       b_2       r_1       s_3      d_41       b_3  ...  \\\n",
       "0  0.059789  0.123999  1.000394  0.009311 -0.080754  0.004721  0.009264  ...   \n",
       "1  0.442610  0.142999  1.008372  0.009232 -0.065012  0.008361  0.004687  ...   \n",
       "2  0.008253  0.012431  1.000745  0.001761 -0.072598  0.003944  0.000801  ...   \n",
       "3  0.624685  0.034141  1.000321  0.001910 -0.069551  0.001054  0.009942  ...   \n",
       "4  0.214070  0.020723  0.964219  0.006177 -0.078409  0.008469  0.000250  ...   \n",
       "\n",
       "   d_142     d_143     d_144     d_145  row_number  last_statement_flag_drop  \\\n",
       "0    NaN  0.008065  0.003143  0.009226           1                        13   \n",
       "1    NaN  0.002657  0.008400  0.006783           2                        12   \n",
       "2    NaN  0.009037  0.002422  0.005595           3                        11   \n",
       "3    NaN  0.001113  0.002910  0.002041           4                        10   \n",
       "4    NaN  0.006028  0.000716  0.000856           5                         9   \n",
       "\n",
       "                                         customer_id  target  \\\n",
       "0  000548e99fa24cef8377e68e602e4bd70d30500a007999...       0   \n",
       "1  000548e99fa24cef8377e68e602e4bd70d30500a007999...       0   \n",
       "2  000548e99fa24cef8377e68e602e4bd70d30500a007999...       0   \n",
       "3  000548e99fa24cef8377e68e602e4bd70d30500a007999...       0   \n",
       "4  000548e99fa24cef8377e68e602e4bd70d30500a007999...       0   \n",
       "\n",
       "   last_statement_flag  last_statement_target  \n",
       "0                    0                      0  \n",
       "1                    0                      0  \n",
       "2                    0                      0  \n",
       "3                    0                      0  \n",
       "4                    0                      0  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sql_df\n",
    "df.head(5)\n",
    "# Dropping columns because the dummy variables \n",
    "df = df.drop(labels=['d_63', 'd_64'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all imputation and categorical/numerical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categorical imputation and one-hot encoder for categorical variables.\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "        # (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)), #Commented out because the categorical variables won't play nice with dummies between test/train. Retry when we do a full train model. Can impute values on test_data.csv if necessary.\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the numerical imputation and standard scaler for numerical variables.\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing the full_processor in one-step for numerical and categorical pipelines.\n",
    "\n",
    "full_processor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the TRAINING data for creating the model.\n",
    "def prep_df(df, target, target_to_drop):\n",
    "    # Set index\n",
    "    print(df.columns.to_list())\n",
    "    # df = df.loc[:,~df.T.duplicated(keep='first')]\n",
    "    #preview the df\n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "\n",
    "    print(df.columns.to_list())\n",
    "    # Drop unecessary columns\n",
    "    if 'last_statement_flag' in df:\n",
    "        df = df.drop(columns=[\"customer_id\", \"row_number\",\"last_statement_flag_drop\",\"last_statement_flag\", \"s_2\", target_to_drop])\n",
    "    else:\n",
    "        df = df.drop(columns=[\"customer_id\", \"row_number\",\"last_statement_flag_drop\", \"s_2\", target_to_drop])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    over_threshold\n",
    "\n",
    "\n",
    "    df.drop(over_threshold.index, \n",
    "            axis=1, \n",
    "            inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    \n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "    \n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols = X.select_dtypes(exclude=\"number\").columns\n",
    "    num_cols = X.select_dtypes(include=\"number\").columns\n",
    "    \n",
    "    full_processor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "    ]\n",
    "    )   \n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    print(X_processed.shape)\n",
    "    y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "            y.values.reshape(-1, 1)\n",
    "            )\n",
    "    return X_processed, y_processed, cols_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the test_data.csv so it's values can be fed into the built model.\n",
    "def prep_test_df(df, keep_cols):\n",
    "    \n",
    "    # Handling case-sensitivity\n",
    "    keep_cols = keep_cols\n",
    "    # # Drop columns not used in model training\n",
    "    df = df[keep_cols]\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    \n",
    "    X = df\n",
    "    \n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols = X.select_dtypes(exclude=\"number\").columns\n",
    "    num_cols = X.select_dtypes(include=\"number\").columns\n",
    "    \n",
    "    full_processor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed_test = full_processor.fit_transform(X)\n",
    "    return X_processed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%script` not found.\n"
     ]
    }
   ],
   "source": [
    "# Deprecated due to iterating over pre-split files.\n",
    "# Now we just read in chunks from the entire test_data.csv file.\n",
    "\n",
    "%%script false\n",
    "def score_split_files(directory, model, keep_cols, test_data_col_names):\n",
    "    mdf = pd.DataFrame(columns=['customer_id', 's_2', 'pred', 'proba'])\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            print(\"Working on \" + f)\n",
    "            df_pred = pd.read_csv(f)\n",
    "            df_pred.columns = test_data_col_names\n",
    "            X_processed_test = prep_test_df(df_pred, keep_cols=keep_cols)\n",
    "            preds = model.predict(X_processed_test)\n",
    "            proba = model.predict_proba(X_processed_test)\n",
    "            df_c = df_pred[['customer_id', 's_2']]\n",
    "            df_c = pd.concat([df_c, pd.DataFrame(preds, columns=['pred']), pd.DataFrame(proba, columns=['proba_inv', 'proba'])], axis=1)\n",
    "            mdf = pd.concat([mdf, df_c])\n",
    "            del [[df_c,df_pred]]\n",
    "    return mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeding the test_data into the model, tabulating results, and building a df. Then saving the df to a .csv file.\n",
    "def score_split_files(path, model, keep_cols, split_num_lines=500000):\n",
    "    current_position = 0 #defines starting position and keeps track of where in file to read\n",
    "    df_columns = None #object to hold the col names collected from the first df chunk\n",
    "    \n",
    "    # Define the result mdf\n",
    "    mdf = pd.DataFrame(columns=['customer_id', 's_2', 'pred', 'proba'])\n",
    "    \n",
    "    # Get chunks from the test_data.csv and send them to the model\n",
    "    while True:\n",
    "        try:\n",
    "            df_chunk = pd.read_csv(path, skiprows=current_position, nrows=split_num_lines)\n",
    "            df_chunk.columns = df_chunk.columns.str.lower()\n",
    "            if current_position == 0:\n",
    "                df_columns = df_chunk.columns\n",
    "            else:\n",
    "                df_chunk.columns = df_columns\n",
    "\n",
    "            # Function to prep the test_data\n",
    "            X_processed_test = prep_test_df(df_chunk, keep_cols=keep_cols)\n",
    "            # Predicting outcomes from test_data\n",
    "            preds = model.predict(X_processed_test)\n",
    "            #Predicting probabilities from test_data\n",
    "            proba = model.predict_proba(X_processed_test)\n",
    "            # Creating df to concat later. Getting date and customer_id from original df read in from .csv\n",
    "            df_c = df_chunk[['customer_id', 's_2']]\n",
    "            # Concating the np arrays to df_c\n",
    "            df_c = pd.concat([df_c, pd.DataFrame(preds, columns=['pred']), pd.DataFrame(proba, columns=['proba_inv', 'proba'])], axis=1)\n",
    "            mdf = pd.concat([mdf, df_c])\n",
    "            # Deleting the temp dfs to free up memory.\n",
    "            del [[df_c,df_chunk]]\n",
    "\n",
    "            current_position += split_num_lines #increments position by chunk size for the next loop\n",
    "        except pd.errors.EmptyDataError:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    return mdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120488, 155)\n"
     ]
    }
   ],
   "source": [
    "# Prep the dataframe\n",
    "X_processed, y_processed, cols_list = prep_df(df, target='target', target_to_drop='last_statement_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_processed, stratify=y_processed, random_state=rand_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9218511387026094"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Run the model\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "proba = xgb_cl.predict_proba(X_test)\n",
    "\n",
    "# Score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test data, every statement flagged as target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120488, 155)\n"
     ]
    }
   ],
   "source": [
    "# Prep the dataframe\n",
    "X_processed, y_processed, cols_list = prep_df(df, target='target', target_to_drop='last_statement_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init classifier\n",
    "xgb_cla = xgb.XGBClassifier()\n",
    "\n",
    "# Fit\n",
    "xgb_cla.fit(X_processed, y_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\ignore\\test_data.csv'\n",
    "# test_data_col_names = pd.read_csv(r'C:\\Users\\joebu\\programming_directory\\large_data_files\\amex-default-prediction\\test_data.csv', nrows=0, index_col=False).columns.str.lower()\n",
    "\n",
    "\n",
    "results_df = score_split_files(path=path, model=xgb_cla, keep_cols=cols_list)\n",
    "results_df.to_csv('./ignore/XGB_target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test data, last_staement_target as outcome:\n",
    "Here will will only flag the last statement as default for each customer that did default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120488, 155)\n"
     ]
    }
   ],
   "source": [
    "# Prep the data with last_statement_target as outcome\n",
    "X_processed, y_processed, cols_list = prep_df(df, target='last_statement_target', target_to_drop='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with new outcome\n",
    "# Init classifier\n",
    "xgb_cla = xgb.XGBClassifier()\n",
    "\n",
    "# Fit\n",
    "xgb_cla.fit(X_processed, y_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = score_split_files(path=path, model=xgb_cla, keep_cols=cols_list)\n",
    "results_df.to_csv('./ignore/XGB_last_statement_target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only training on last statements:\n",
    "Here we will only consider the last statements when training the model, and drop all the other statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 's_2', 'p_2', 'd_39', 'b_1', 'b_2', 'r_1', 's_3', 'd_41', 'b_3', 'd_42', 'd_43', 'd_44', 'b_4', 'd_45', 'b_5', 'r_2', 'd_46', 'd_47', 'd_48', 'd_49', 'b_6', 'b_7', 'b_8', 'd_50', 'd_51', 'b_9', 'r_3', 'd_52', 'p_3', 'b_10', 'd_53', 's_5', 'b_11', 's_6', 'd_54', 'r_4', 's_7', 'b_12', 's_8', 'd_55', 'd_56', 'b_13', 'r_5', 'd_58', 's_9', 'b_14', 'd_59', 'd_60', 'd_61', 'b_15', 's_11', 'd_62', 'd_63', 'd_64', 'd_65', 'b_16', 'b_17', 'b_18', 'b_19', 'd_66', 'b_20', 'd_68', 's_12', 'r_6', 's_13', 'b_21', 'd_69', 'b_22', 'd_70', 'd_71', 'd_72', 's_15', 'b_23', 'd_73', 'p_4', 'd_74', 'd_75', 'd_76', 'b_24', 'r_7', 'd_77', 'b_25', 'b_26', 'd_78', 'd_79', 'r_8', 'r_9', 's_16', 'd_80', 'r_10', 'r_11', 'b_27', 'd_81', 'd_82', 's_17', 'r_12', 'b_28', 'r_13', 'd_83', 'r_14', 'r_15', 'd_84', 'r_16', 'b_29', 'b_30', 's_18', 'd_86', 'd_87', 'r_17', 'r_18', 'd_88', 'b_31', 's_19', 'r_19', 'b_32', 's_20', 'r_20', 'r_21', 'b_33', 'd_89', 'r_22', 'r_23', 'd_91', 'd_92', 'd_93', 'd_94', 'r_24', 'r_25', 'd_96', 's_22', 's_23', 's_24', 's_25', 's_26', 'd_102', 'd_103', 'd_104', 'd_105', 'd_106', 'd_107', 'b_36', 'b_37', 'r_26', 'r_27', 'b_38', 'd_108', 'd_109', 'd_110', 'd_111', 'b_39', 'd_112', 'b_40', 's_27', 'd_113', 'd_114', 'd_115', 'd_116', 'd_117', 'd_118', 'd_119', 'd_120', 'd_121', 'd_122', 'd_123', 'd_124', 'd_125', 'd_126', 'd_127', 'd_128', 'd_129', 'b_41', 'b_42', 'd_130', 'd_131', 'd_132', 'd_133', 'r_28', 'd_134', 'd_135', 'd_136', 'd_137', 'd_138', 'd_139', 'd_140', 'd_141', 'd_142', 'd_143', 'd_144', 'd_145', 'row_number', 'last_statement_flag_drop', 'customer_id', 'target', 'last_statement_flag', 'last_statement_target']\n",
      "['customer_id', 's_2', 'p_2', 'd_39', 'b_1', 'b_2', 'r_1', 's_3', 'd_41', 'b_3', 'd_42', 'd_43', 'd_44', 'b_4', 'd_45', 'b_5', 'r_2', 'd_46', 'd_47', 'd_48', 'd_49', 'b_6', 'b_7', 'b_8', 'd_50', 'd_51', 'b_9', 'r_3', 'd_52', 'p_3', 'b_10', 'd_53', 's_5', 'b_11', 's_6', 'd_54', 'r_4', 's_7', 'b_12', 's_8', 'd_55', 'd_56', 'b_13', 'r_5', 'd_58', 's_9', 'b_14', 'd_59', 'd_60', 'd_61', 'b_15', 's_11', 'd_62', 'd_63', 'd_64', 'd_65', 'b_16', 'b_17', 'b_18', 'b_19', 'd_66', 'b_20', 'd_68', 's_12', 'r_6', 's_13', 'b_21', 'd_69', 'b_22', 'd_70', 'd_71', 'd_72', 's_15', 'b_23', 'd_73', 'p_4', 'd_74', 'd_75', 'd_76', 'b_24', 'r_7', 'd_77', 'b_25', 'b_26', 'd_78', 'd_79', 'r_8', 'r_9', 's_16', 'd_80', 'r_10', 'r_11', 'b_27', 'd_81', 'd_82', 's_17', 'r_12', 'b_28', 'r_13', 'd_83', 'r_14', 'r_15', 'd_84', 'r_16', 'b_29', 'b_30', 's_18', 'd_86', 'd_87', 'r_17', 'r_18', 'd_88', 'b_31', 's_19', 'r_19', 'b_32', 's_20', 'r_20', 'r_21', 'b_33', 'd_89', 'r_22', 'r_23', 'd_91', 'd_92', 'd_93', 'd_94', 'r_24', 'r_25', 'd_96', 's_22', 's_23', 's_24', 's_25', 's_26', 'd_102', 'd_103', 'd_104', 'd_105', 'd_106', 'd_107', 'b_36', 'b_37', 'r_26', 'r_27', 'b_38', 'd_108', 'd_109', 'd_110', 'd_111', 'b_39', 'd_112', 'b_40', 's_27', 'd_113', 'd_114', 'd_115', 'd_116', 'd_117', 'd_118', 'd_119', 'd_120', 'd_121', 'd_122', 'd_123', 'd_124', 'd_125', 'd_126', 'd_127', 'd_128', 'd_129', 'b_41', 'b_42', 'd_130', 'd_131', 'd_132', 'd_133', 'r_28', 'd_134', 'd_135', 'd_136', 'd_137', 'd_138', 'd_139', 'd_140', 'd_141', 'd_142', 'd_143', 'd_144', 'd_145', 'row_number', 'last_statement_flag_drop', 'target', 'last_statement_flag', 'last_statement_target']\n",
      "(10000, 163)\n"
     ]
    }
   ],
   "source": [
    "ls_df = sql_df[sql_df['last_statement_flag'] == 1]\n",
    "\n",
    "# Prep the data with last_statement_target as outcome\n",
    "X_processed, y_processed, cols_list = prep_df(ls_df, target='last_statement_target', target_to_drop='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init classifier\n",
    "xgb_cla = xgb.XGBClassifier()\n",
    "\n",
    "# Fit the model\n",
    "xgb_cla.fit(X_processed, y_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = score_split_files(path=path, model=xgb_cla, keep_cols=cols_list)\n",
    "results_df.to_csv('./ignore/XGB_only_last_statements.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dcb6b7e2f26f70306738e96e7445a78b658183e0554cf15e38890769044d88c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
