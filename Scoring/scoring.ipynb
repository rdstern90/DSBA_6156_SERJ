{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies:\n",
    "# Data Wrangling:\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Modeling Packages:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import glob\n",
    "rand_state = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL cloud server credentials:\n",
    "# server ip: 34.75.124.150\n",
    "# username: user\n",
    "# password: DeEJNEAhy\n",
    "# Data is in materialized views train_data and train_labels\n",
    "engine = create_engine('postgresql://user:DeEJNEAhy@34.75.124.150/postgres')\n",
    "sql_df = pd.read_sql(\"\"\"\n",
    "                 WITH BASE AS (\n",
    "                    SELECT *\n",
    "                    ,ROW_NUMBER() OVER      (\n",
    "                                            PARTITION BY customer_id \n",
    "                                            ORDER BY s_2\n",
    "                                            )\n",
    "                    ,ROW_NUMBER() OVER      (\n",
    "                                            PARTITION BY customer_id\n",
    "                                            ORDER BY s_2 DESC\n",
    "                                            ) last_statement_flag_drop\n",
    "                    FROM TRAIN_DATA_random\n",
    "                    )\n",
    "\n",
    "\n",
    "                    SELECT *\n",
    "                    ,CASE WHEN last_statement_flag_drop = 1 then 1 else 0 end as last_statement_flag\n",
    "                    ,CASE WHEN (target = 1 AND last_statement_flag_drop = 1) then 1 else 0 end as last_statement_target\n",
    "                    FROM BASE B\n",
    "                    LEFT JOIN train_labels_random L\n",
    "                    ON B.customer_id = L.customer_id\n",
    "                 \"\"\", engine) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2652099"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = sql_df\n",
    "df = pd.read_csv('df_savepoint.csv')\n",
    "df.head(5)\n",
    "# Dropping columns because the dummy variables \n",
    "# df = df.drop(labels=['d_63', 'd_64'], axis=1)\n",
    "df.head()\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categorical imputation and one-hot encoder for categorical variables.\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "        # (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)), #Commented out because the categorical variables won't play nice with dummies between test/train. Retry when we do a full train model. Can impute values on test_data.csv if necessary.\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the numerical imputation and standard scaler for numerical variables.\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the TRAINING data for creating the model.\n",
    "def prep_df(df, target, target_to_drop):\n",
    "    # Set index\n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "\n",
    "    # Drop unecessary columns\n",
    "#     df = df.drop(columns=[\"customer_id\", \"row_number\",\"last_statement_flag_drop\",\"last_statement_flag\", \"s_2\", target_to_drop])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    over_threshold\n",
    "\n",
    "\n",
    "    df.drop(over_threshold.index, \n",
    "            axis=1, \n",
    "            inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    \n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "    \n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols = X.select_dtypes(exclude=\"number\").columns\n",
    "    num_cols = X.select_dtypes(include=\"number\").columns\n",
    "    \n",
    "    full_processor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "    ]\n",
    "    )   \n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    print(X_processed.shape)\n",
    "    y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "            y.values.reshape(-1, 1)\n",
    "            )\n",
    "    return X_processed, y_processed, cols_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the test_data.csv so it's values can be fed into the built model.\n",
    "def prep_test_df(df, keep_cols):\n",
    "    \n",
    "    # Handling case-sensitivity\n",
    "    keep_cols = keep_cols\n",
    "    # # Drop columns not used in model training\n",
    "    df = df[keep_cols]\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    \n",
    "    X = df\n",
    "    \n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols = X.select_dtypes(exclude=\"number\").columns\n",
    "    num_cols = X.select_dtypes(include=\"number\").columns\n",
    "    \n",
    "    full_processor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed_test = full_processor.fit_transform(X)\n",
    "    return X_processed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeding the test_data into the model, tabulating results, and building a df. Then saving the df to a .csv file.\n",
    "def score_split_files(path, model, keep_cols, split_num_lines=3500000):\n",
    "    current_position = 0 #defines starting position and keeps track of where in file to read\n",
    "    df_columns = None #object to hold the col names collected from the first df chunk\n",
    "    \n",
    "    # Define the result mdf\n",
    "    mdf = pd.DataFrame(columns=['customer_id', 's_2', 'pred', 'proba'])\n",
    "    \n",
    "    # Get chunks from the test_data.csv and send them to the model\n",
    "    while True:\n",
    "        try:\n",
    "            df_chunk = pd.read_csv(path, skiprows=current_position, nrows=split_num_lines)\n",
    "            df_chunk.columns = df_chunk.columns.str.lower()\n",
    "            if current_position == 0:\n",
    "                df_columns = df_chunk.columns\n",
    "            else:\n",
    "                df_chunk.columns = df_columns\n",
    "\n",
    "            # Function to prep the test_data\n",
    "            # keep_cols_new = keep_cols.remove['last_statement_target']\n",
    "            X_processed_test = prep_test_df(df_chunk, keep_cols=keep_cols)\n",
    "            # Predicting outcomes from test_data\n",
    "            preds = model.predict(X_processed_test)\n",
    "            #Predicting probabilities from test_data\n",
    "            proba = model.predict_proba(X_processed_test)\n",
    "            # Creating df to concat later. Getting date and customer_id from original df read in from .csv\n",
    "            df_c = df_chunk[['customer_id', 's_2']]\n",
    "            # Concating the np arrays to df_c\n",
    "            df_c = pd.concat([df_c, pd.DataFrame(preds, columns=['pred']), pd.DataFrame(proba, columns=['proba_inv', 'proba'])], axis=1)\n",
    "            mdf = pd.concat([mdf, df_c])\n",
    "            # Deleting the temp dfs to free up memory.\n",
    "            del [[df_c,df_chunk]]\n",
    "\n",
    "            current_position += split_num_lines #increments position by chunk size for the next loop\n",
    "        except pd.errors.EmptyDataError:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    return mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        p_2\n",
      "2        b_1\n",
      "31       r_4\n",
      "9       d_44\n",
      "21      d_51\n",
      "       ...  \n",
      "60      s_15\n",
      "46      d_65\n",
      "153    d_141\n",
      "44      s_11\n",
      "89      s_18\n",
      "Name: name, Length: 68, dtype: object\n",
      "['p_2', 'b_1', 'r_4', 'd_44', 'd_51', 'b_2', 'r_1', 'b_9', 'd_66_1', 'd_41', 'r_2', 'd_39', 'd_129', 's_3', 'r_3', 'd_112', 'b_38_4', 'd_64_0', 'd_111', 'b_4', 'd_43', 'r_27', 'd_49', 's_23', 'b_8', 'd_45', 'b_3', 'd_46', 'd_48', 'd_54', 'd_131', 'd_63_3', 'b_11', 'd_140', 'b_38_2', 'b_5', 'r_11', 'b_7', 'r_7', 'd_117_6', 'd_138', 'p_3', 'b_10', 'b_20', 'r_26', 'b_16', 'r_5', 'd_120_0', 'd_62', 's_7', 'r_12', 'd_72', 'd_117_3', 's_24', 'd_75', 'd_82', 'b_18', 's_5', 'd_52', 'd_86', 's_8', 'd_47', 's_26', 's_15', 'd_65', 'd_141', 's_11', 's_18', 'last_statement_target']\n",
      "['p_2', 'b_1', 'r_4', 'd_44', 'd_51', 'b_2', 'r_1', 'b_9', 'd_41', 'r_2', 'd_39', 'd_129', 's_3', 'r_3', 'd_112', 'd_111', 'b_4', 'd_43', 'r_27', 'd_49', 's_23', 'b_8', 'd_45', 'b_3', 'd_46', 'd_48', 'd_54', 'd_131', 'b_11', 'd_140', 'b_5', 'r_11', 'b_7', 'r_7', 'd_138', 'p_3', 'b_10', 'b_20', 'r_26', 'b_16', 'r_5', 'd_62', 's_7', 'r_12', 'd_72', 's_24', 'd_75', 'd_82', 'b_18', 's_5', 'd_52', 'd_86', 's_8', 'd_47', 's_26', 's_15', 'd_65', 'd_141', 's_11', 's_18']\n"
     ]
    }
   ],
   "source": [
    "# Read in the column names we get from feature reduction\n",
    "import pickle\n",
    "with open(\"C:/Users/joebu/programming_directory/DSBA_6156_SERJ/data/names\", \"rb\") as fp:   # Unpickling\n",
    "    names = pickle.load(fp)\n",
    "\n",
    "print(names)\n",
    "\n",
    "names = names.tolist()\n",
    "names.append('last_statement_target')\n",
    "print(names)\n",
    "remove_list = ['d_66_1', 'b_38_4', 'd_64_0', 'd_63_3', 'b_38_2', 'd_117_6', 'd_120_0', 'd_117_3']\n",
    "remove_list_2 = ['last_statement_target']\n",
    "names_pre = [i for i in names if i not in remove_list]\n",
    "names_pre_no_t = [i for i in names_pre if i not in remove_list_2]\n",
    "\n",
    "# df = df[names_pre]\n",
    "\n",
    "print(names_pre_no_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_savepoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2652099, 56)\n"
     ]
    }
   ],
   "source": [
    "# Prep the data with last_statement_target as outcome\n",
    "X_processed, y_processed, cols_list = prep_df(df, target='last_statement_target', target_to_drop='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with new outcome\n",
    "# Init classifier\n",
    "xgb_cla = xgb.XGBClassifier()\n",
    "\n",
    "# Fit\n",
    "xgb_cla.fit(X_processed, y_processed)\n",
    "\n",
    "import pickle\n",
    "file_name = \"xgb_reg.pkl\"\n",
    "\n",
    "# save\n",
    "pickle.dump(xgb_cla, open(file_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the model in from the .pkl file\n",
    "import pickle\n",
    "file_name = \"xgb_reg.pkl\"\n",
    "xgb_cla = pickle.load(open(file_name, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 56, got 60",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mjoebu\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mprogramming_directory\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDSBA_6156_SERJ\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mtest_data.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m results_df \u001b[39m=\u001b[39m score_split_files(path\u001b[39m=\u001b[39;49mpath, model\u001b[39m=\u001b[39;49mxgb_cla, keep_cols\u001b[39m=\u001b[39;49mnames_pre_no_t)\n\u001b[0;32m      3\u001b[0m results_df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39m./ignore/XGB_last_statement_target.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m, in \u001b[0;36mscore_split_files\u001b[1;34m(path, model, keep_cols, split_num_lines)\u001b[0m\n\u001b[0;32m     21\u001b[0m X_processed_test \u001b[39m=\u001b[39m prep_test_df(df_chunk, keep_cols\u001b[39m=\u001b[39mkeep_cols)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Predicting outcomes from test_data\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_processed_test)\n\u001b[0;32m     24\u001b[0m \u001b[39m#Predicting probabilities from test_data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(X_processed_test)\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1551\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m   1542\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1543\u001b[0m     X: ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     iteration_range: Optional[Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1549\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1550\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(verbosity\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbosity):\n\u001b[1;32m-> 1551\u001b[0m         class_probs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m   1552\u001b[0m             X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1553\u001b[0m             output_margin\u001b[39m=\u001b[39;49moutput_margin,\n\u001b[0;32m   1554\u001b[0m             ntree_limit\u001b[39m=\u001b[39;49mntree_limit,\n\u001b[0;32m   1555\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1556\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1557\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1558\u001b[0m         )\n\u001b[0;32m   1559\u001b[0m         \u001b[39mif\u001b[39;00m output_margin:\n\u001b[0;32m   1560\u001b[0m             \u001b[39m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m             \u001b[39mreturn\u001b[39;00m class_probs\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1140\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1139\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m         predts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster()\u001b[39m.\u001b[39;49minplace_predict(\n\u001b[0;32m   1141\u001b[0m             data\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1142\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1143\u001b[0m             predict_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmargin\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m output_margin \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1144\u001b[0m             missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m   1145\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1146\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1147\u001b[0m         )\n\u001b[0;32m   1148\u001b[0m         \u001b[39mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[0;32m   1149\u001b[0m             \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m  \u001b[39m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\venv\\venv\\lib\\site-packages\\xgboost\\core.py:2268\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2264\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2265\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2266\u001b[0m         )\n\u001b[0;32m   2267\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features() \u001b[39m!=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m-> 2268\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2269\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeature shape mismatch, expected: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2270\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2271\u001b[0m         )\n\u001b[0;32m   2273\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m   2274\u001b[0m     _array_interface,\n\u001b[0;32m   2275\u001b[0m     _is_cudf_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2278\u001b[0m     _transform_pandas_df,\n\u001b[0;32m   2279\u001b[0m )\n\u001b[0;32m   2281\u001b[0m enable_categorical \u001b[39m=\u001b[39m _has_categorical(\u001b[39mself\u001b[39m, data)\n",
      "\u001b[1;31mValueError\u001b[0m: Feature shape mismatch, expected: 56, got 60"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\joebu\\programming_directory\\DSBA_6156_SERJ\\ignore\\test_data.csv'\n",
    "results_df = score_split_files(path=path, model=xgb_cla, keep_cols=names_pre_no_t)\n",
    "results_df.to_csv('./ignore/XGB_last_statement_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dcb6b7e2f26f70306738e96e7445a78b658183e0554cf15e38890769044d88c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
