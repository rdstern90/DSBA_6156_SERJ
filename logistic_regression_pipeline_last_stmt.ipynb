{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "engine = create_engine('postgresql://user:DeEJNEAhy@34.75.124.150/postgres')\n",
    "\n",
    "rand_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df = pd.read_sql(\"\"\"\n",
    "                 WITH BASE AS (\n",
    "                    SELECT *\n",
    "                    ,ROW_NUMBER() OVER      (\n",
    "                                            PARTITION BY customer_id\n",
    "                                            ORDER BY s_2\n",
    "                                            )\n",
    "                    ,ROW_NUMBER() OVER      (\n",
    "                                            PARTITION BY customer_id\n",
    "                                            ORDER BY s_2 DESC\n",
    "                                            ) last_statement_flag_drop\n",
    "                    FROM TRAIN_DATA_random\n",
    "                    )\n",
    "\n",
    "\n",
    "                    SELECT B.* , L.target\n",
    "                    ,CASE WHEN last_statement_flag_drop = 1 then 1 else 0 end as last_statement_flag\n",
    "                    ,CASE WHEN (target = 1 AND last_statement_flag_drop = 1) then 1 else 0 end as last_statement_target\n",
    "                    FROM BASE B                 \n",
    "                  INNER JOIN train_labels_random AS L\n",
    "                    ON B.customer_ID = L.customer_ID\n",
    "                 \"\"\", engine) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120488, 195)\n"
     ]
    }
   ],
   "source": [
    "# Moving the prepared dataframe to other dataframe to keep the original dataframe intact\n",
    "df = sql_df.copy()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 's_2', 'p_2', 'd_39', 'b_1', 'b_2', 'r_1', 's_3', 'd_41', 'b_3', 'd_42', 'd_43', 'd_44', 'b_4', 'd_45', 'b_5', 'r_2', 'd_46', 'd_47', 'd_48', 'd_49', 'b_6', 'b_7', 'b_8', 'd_50', 'd_51', 'b_9', 'r_3', 'd_52', 'p_3', 'b_10', 'd_53', 's_5', 'b_11', 's_6', 'd_54', 'r_4', 's_7', 'b_12', 's_8', 'd_55', 'd_56', 'b_13', 'r_5', 'd_58', 's_9', 'b_14', 'd_59', 'd_60', 'd_61', 'b_15', 's_11', 'd_62', 'd_63', 'd_64', 'd_65', 'b_16', 'b_17', 'b_18', 'b_19', 'd_66', 'b_20', 'd_68', 's_12', 'r_6', 's_13', 'b_21', 'd_69', 'b_22', 'd_70', 'd_71', 'd_72', 's_15', 'b_23', 'd_73', 'p_4', 'd_74', 'd_75', 'd_76', 'b_24', 'r_7', 'd_77', 'b_25', 'b_26', 'd_78', 'd_79', 'r_8', 'r_9', 's_16', 'd_80', 'r_10', 'r_11', 'b_27', 'd_81', 'd_82', 's_17', 'r_12', 'b_28', 'r_13', 'd_83', 'r_14', 'r_15', 'd_84', 'r_16', 'b_29', 'b_30', 's_18', 'd_86', 'd_87', 'r_17', 'r_18', 'd_88', 'b_31', 's_19', 'r_19', 'b_32', 's_20', 'r_20', 'r_21', 'b_33', 'd_89', 'r_22', 'r_23', 'd_91', 'd_92', 'd_93', 'd_94', 'r_24', 'r_25', 'd_96', 's_22', 's_23', 's_24', 's_25', 's_26', 'd_102', 'd_103', 'd_104', 'd_105', 'd_106', 'd_107', 'b_36', 'b_37', 'r_26', 'r_27', 'b_38', 'd_108', 'd_109', 'd_110', 'd_111', 'b_39', 'd_112', 'b_40', 's_27', 'd_113', 'd_114', 'd_115', 'd_116', 'd_117', 'd_118', 'd_119', 'd_120', 'd_121', 'd_122', 'd_123', 'd_124', 'd_125', 'd_126', 'd_127', 'd_128', 'd_129', 'b_41', 'b_42', 'd_130', 'd_131', 'd_132', 'd_133', 'r_28', 'd_134', 'd_135', 'd_136', 'd_137', 'd_138', 'd_139', 'd_140', 'd_141', 'd_142', 'd_143', 'd_144', 'd_145', 'row_number', 'last_statement_flag_drop', 'target', 'last_statement_flag', 'last_statement_target']\n"
     ]
    }
   ],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[])\n",
    "# Categorical column D_66 was removed as it has missing values greater than 40% and would be taken care\n",
    "categorical_cols = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_68', 'b_31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop, categorical_cols):\n",
    "        # Creating the pipeline for the categorical variables and continuous variables\n",
    "        # Defining the categorical imputation for categorical variables.\n",
    "        self.categorical_pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"impute_na\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"impute_none\", SimpleImputer(missing_values=None, strategy=\"most_frequent\")) \n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # defining the numerical imputation and standard scaler for numerical variables.\n",
    "        self.numeric_pipeline = Pipeline(\n",
    "            steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "                (\"scale\", StandardScaler())]\n",
    "        )\n",
    "\n",
    "        self.cols_to_drop = cols_to_drop + [\"customer_id\", \"s_2\" , \"row_number\",\"last_statement_flag_drop\"]\n",
    "        \n",
    "        \n",
    "        self.categorical_cols = categorical_cols\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        # Get the list of columns that have missing values greater than equal to 40%\n",
    "        missing_perc = round((df.isnull().sum() / len(df)) * 100,2)\n",
    "        # Prepare final List of columns to drop\n",
    "        self.cols_to_drop = self.cols_to_drop + missing_perc[missing_perc.ge(40)].index.tolist()\n",
    "        # print(self.cols_to_drop)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Get the clean dataframe with columns to work\n",
    "        df = X.drop(columns=self.cols_to_drop)\n",
    "\n",
    "        numeric_cols = df.drop(columns=self.categorical_cols + ['last_statement_target']).columns.tolist()\n",
    "        \n",
    "        # Apply the tranformation defined in pipeline\n",
    "        full_transformer = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"numeric\", self.numeric_pipeline, numeric_cols),\n",
    "                (\"categorical\", self.categorical_pipeline, self.categorical_cols),\n",
    "            ],\n",
    "            # to keep the target column and just passthrough it, instead of dropping\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        transformed = full_transformer.fit_transform(df)\n",
    "        # Converted the array to dataframe\n",
    "        df_transformed = pd.DataFrame(data=transformed, columns= numeric_cols + self.categorical_cols + ['last_statement_target'])\n",
    "        \n",
    "        # Convert the data type except for the columns which have categorical (text) values to float64\n",
    "        list1 = [col for col in df_transformed.columns.tolist() if col != 'd_63' and col != 'd_64']\n",
    "        df_transformed[list1] = df_transformed[list1].astype('float64')\n",
    "\n",
    "        # OneHotEncoding for d_63 and d_64. If any value is missing in the test data, for that value 0 would be encoded\n",
    "        categories = [['CL', 'CO', 'CR', 'XL', 'XM', 'XZ'], ['-1', 'O', 'R', 'U']]\n",
    "        enc = OneHotEncoder(categories=categories, sparse=False, drop='first')\n",
    "        encoded = enc.fit_transform(df_transformed[['d_63', 'd_64']].values)\n",
    "        df_transformed = pd.concat([\n",
    "            df_transformed.drop(columns=['d_63', 'd_64']),\n",
    "            pd.DataFrame(encoded, columns=enc.get_feature_names_out(['d_63', 'd_64']))\n",
    "        ], axis=1)\n",
    "\n",
    "        return df_transformed\n",
    "\n",
    "# Keeping just the last statement of each customer which will have the label associated with that customer\n",
    "df = df[df['last_statement_flag'] == 1]\n",
    "preprocessing = PreProcessing([\"last_statement_flag\",\"target\"], categorical_cols)\n",
    "df_processed = preprocessing.fit_transform(df)\n",
    "\n",
    "pipeline.steps.append(('preprocessing', preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_2</th>\n",
       "      <th>d_39</th>\n",
       "      <th>b_1</th>\n",
       "      <th>b_2</th>\n",
       "      <th>r_1</th>\n",
       "      <th>s_3</th>\n",
       "      <th>d_41</th>\n",
       "      <th>b_3</th>\n",
       "      <th>d_43</th>\n",
       "      <th>d_44</th>\n",
       "      <th>...</th>\n",
       "      <th>b_31</th>\n",
       "      <th>last_statement_target</th>\n",
       "      <th>d_63_CO</th>\n",
       "      <th>d_63_CR</th>\n",
       "      <th>d_63_XL</th>\n",
       "      <th>d_63_XM</th>\n",
       "      <th>d_63_XZ</th>\n",
       "      <th>d_64_O</th>\n",
       "      <th>d_64_R</th>\n",
       "      <th>d_64_U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.821040</td>\n",
       "      <td>-0.273666</td>\n",
       "      <td>-0.496190</td>\n",
       "      <td>1.010406</td>\n",
       "      <td>-0.376954</td>\n",
       "      <td>-0.801188</td>\n",
       "      <td>-0.300113</td>\n",
       "      <td>-0.605466</td>\n",
       "      <td>-0.547374</td>\n",
       "      <td>-0.559686</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.216442</td>\n",
       "      <td>-0.509883</td>\n",
       "      <td>-0.511701</td>\n",
       "      <td>0.549955</td>\n",
       "      <td>-0.357259</td>\n",
       "      <td>-0.288385</td>\n",
       "      <td>-0.321199</td>\n",
       "      <td>-0.592774</td>\n",
       "      <td>-0.055546</td>\n",
       "      <td>-0.021229</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.254334</td>\n",
       "      <td>-0.282455</td>\n",
       "      <td>-0.517167</td>\n",
       "      <td>1.000490</td>\n",
       "      <td>-0.376818</td>\n",
       "      <td>-0.803876</td>\n",
       "      <td>-0.325074</td>\n",
       "      <td>-0.586713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558677</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.447220</td>\n",
       "      <td>-0.486744</td>\n",
       "      <td>-0.350866</td>\n",
       "      <td>-1.427398</td>\n",
       "      <td>0.533245</td>\n",
       "      <td>3.194008</td>\n",
       "      <td>0.874783</td>\n",
       "      <td>-0.058887</td>\n",
       "      <td>1.643804</td>\n",
       "      <td>2.619380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.768281</td>\n",
       "      <td>0.192289</td>\n",
       "      <td>-0.441046</td>\n",
       "      <td>1.000144</td>\n",
       "      <td>-0.386847</td>\n",
       "      <td>-0.543592</td>\n",
       "      <td>-0.300252</td>\n",
       "      <td>-0.572569</td>\n",
       "      <td>-0.659591</td>\n",
       "      <td>-0.551113</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        p_2      d_39       b_1       b_2       r_1       s_3      d_41  \\\n",
       "0  0.821040 -0.273666 -0.496190  1.010406 -0.376954 -0.801188 -0.300113   \n",
       "1 -0.216442 -0.509883 -0.511701  0.549955 -0.357259 -0.288385 -0.321199   \n",
       "2  1.254334 -0.282455 -0.517167  1.000490 -0.376818 -0.803876 -0.325074   \n",
       "3 -1.447220 -0.486744 -0.350866 -1.427398  0.533245  3.194008  0.874783   \n",
       "4  0.768281  0.192289 -0.441046  1.000144 -0.386847 -0.543592 -0.300252   \n",
       "\n",
       "        b_3      d_43      d_44  ...  b_31  last_statement_target  d_63_CO  \\\n",
       "0 -0.605466 -0.547374 -0.559686  ...   1.0                    0.0      1.0   \n",
       "1 -0.592774 -0.055546 -0.021229  ...   1.0                    0.0      1.0   \n",
       "2 -0.586713  0.000000 -0.558677  ...   1.0                    0.0      1.0   \n",
       "3 -0.058887  1.643804  2.619380  ...   0.0                    1.0      1.0   \n",
       "4 -0.572569 -0.659591 -0.551113  ...   1.0                    0.0      1.0   \n",
       "\n",
       "   d_63_CR  d_63_XL  d_63_XM  d_63_XZ  d_64_O  d_64_R  d_64_U  \n",
       "0      0.0      0.0      0.0      0.0     1.0     0.0     0.0  \n",
       "1      0.0      0.0      0.0      0.0     0.0     0.0     1.0  \n",
       "2      0.0      0.0      0.0      0.0     0.0     0.0     1.0  \n",
       "3      0.0      0.0      0.0      0.0     0.0     0.0     1.0  \n",
       "4      0.0      0.0      0.0      0.0     0.0     0.0     1.0  \n",
       "\n",
       "[5 rows x 165 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p_2',\n",
       " 'd_39',\n",
       " 'b_1',\n",
       " 'b_2',\n",
       " 'r_1',\n",
       " 's_3',\n",
       " 'd_41',\n",
       " 'b_3',\n",
       " 'd_43',\n",
       " 'd_44',\n",
       " 'b_4',\n",
       " 'd_45',\n",
       " 'b_5',\n",
       " 'r_2',\n",
       " 'd_46',\n",
       " 'd_47',\n",
       " 'd_48',\n",
       " 'b_6',\n",
       " 'b_7',\n",
       " 'b_8',\n",
       " 'd_51',\n",
       " 'b_9',\n",
       " 'r_3',\n",
       " 'd_52',\n",
       " 'p_3',\n",
       " 'b_10',\n",
       " 's_5',\n",
       " 'b_11',\n",
       " 's_6',\n",
       " 'd_54',\n",
       " 'r_4',\n",
       " 's_7',\n",
       " 'b_12',\n",
       " 's_8',\n",
       " 'd_55',\n",
       " 'b_13',\n",
       " 'r_5',\n",
       " 'd_58',\n",
       " 's_9',\n",
       " 'b_14',\n",
       " 'd_59',\n",
       " 'd_60',\n",
       " 'd_61',\n",
       " 'b_15',\n",
       " 's_11',\n",
       " 'd_62',\n",
       " 'd_65',\n",
       " 'b_16',\n",
       " 'b_18',\n",
       " 'b_19',\n",
       " 'b_20',\n",
       " 's_12',\n",
       " 'r_6',\n",
       " 's_13',\n",
       " 'b_21',\n",
       " 'd_69',\n",
       " 'b_22',\n",
       " 'd_70',\n",
       " 'd_71',\n",
       " 'd_72',\n",
       " 's_15',\n",
       " 'b_23',\n",
       " 'p_4',\n",
       " 'd_74',\n",
       " 'd_75',\n",
       " 'b_24',\n",
       " 'r_7',\n",
       " 'b_25',\n",
       " 'b_26',\n",
       " 'd_78',\n",
       " 'd_79',\n",
       " 'r_8',\n",
       " 's_16',\n",
       " 'd_80',\n",
       " 'r_10',\n",
       " 'r_11',\n",
       " 'b_27',\n",
       " 'd_81',\n",
       " 's_17',\n",
       " 'r_12',\n",
       " 'b_28',\n",
       " 'r_13',\n",
       " 'd_83',\n",
       " 'r_14',\n",
       " 'r_15',\n",
       " 'd_84',\n",
       " 'r_16',\n",
       " 's_18',\n",
       " 'd_86',\n",
       " 'r_17',\n",
       " 'r_18',\n",
       " 's_19',\n",
       " 'r_19',\n",
       " 'b_32',\n",
       " 's_20',\n",
       " 'r_20',\n",
       " 'r_21',\n",
       " 'b_33',\n",
       " 'd_89',\n",
       " 'r_22',\n",
       " 'r_23',\n",
       " 'd_91',\n",
       " 'd_92',\n",
       " 'd_93',\n",
       " 'd_94',\n",
       " 'r_24',\n",
       " 'r_25',\n",
       " 'd_96',\n",
       " 's_22',\n",
       " 's_23',\n",
       " 's_24',\n",
       " 's_25',\n",
       " 's_26',\n",
       " 'd_102',\n",
       " 'd_103',\n",
       " 'd_104',\n",
       " 'd_107',\n",
       " 'b_36',\n",
       " 'b_37',\n",
       " 'r_27',\n",
       " 'd_109',\n",
       " 'd_112',\n",
       " 'b_40',\n",
       " 's_27',\n",
       " 'd_113',\n",
       " 'd_115',\n",
       " 'd_118',\n",
       " 'd_119',\n",
       " 'd_121',\n",
       " 'd_122',\n",
       " 'd_123',\n",
       " 'd_124',\n",
       " 'd_125',\n",
       " 'd_127',\n",
       " 'd_128',\n",
       " 'd_129',\n",
       " 'b_41',\n",
       " 'd_130',\n",
       " 'd_131',\n",
       " 'd_133',\n",
       " 'r_28',\n",
       " 'd_139',\n",
       " 'd_140',\n",
       " 'd_141',\n",
       " 'd_143',\n",
       " 'd_144',\n",
       " 'd_145',\n",
       " 'b_30',\n",
       " 'b_38',\n",
       " 'd_114',\n",
       " 'd_116',\n",
       " 'd_117',\n",
       " 'd_120',\n",
       " 'd_126',\n",
       " 'd_68',\n",
       " 'b_31',\n",
       " 'last_statement_target',\n",
       " 'd_63_CO',\n",
       " 'd_63_CR',\n",
       " 'd_63_XL',\n",
       " 'd_63_XM',\n",
       " 'd_63_XZ',\n",
       " 'd_64_O',\n",
       " 'd_64_R',\n",
       " 'd_64_U']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the correlation and VIF, we will consider the numerical columns only. We have done dummy encoding for d_63 & d_64, so need to consider those column names also\n",
    "cols_to_drop = [col for col in df_processed.columns.tolist() if col.startswith('d_63') or col.startswith('d_64')] + categorical_cols + ['last_statement_target']\n",
    "# Removing d_63 & d_64 from the list, as we have already dropped those columns from the dataframe\n",
    "cols_to_drop.remove('d_63')\n",
    "cols_to_drop.remove('d_64')\n",
    "#Numerical columns dataframe\n",
    "df_numerical = df_processed.drop(columns=cols_to_drop).copy()\n",
    "\n",
    "# Do the correlation after transposing the array\n",
    "df_corr = df_numerical.corr()\n",
    "df_corr.to_csv(\"./ignore/LR_last_stmt/num_corr_results_last_stmt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF on the numerical columns\n",
    "vif_data1 = pd.DataFrame()\n",
    "vif_data1[\"feature\"] = df_numerical.columns\n",
    "# print(len(vif_data1.feature.unique()))\n",
    "\n",
    "# calculating VIF for each feature\n",
    "vif_data1[\"VIF\"] = [variance_inflation_factor(df_numerical.values, i) for i in range(len(df_numerical.columns))]\n",
    "vif_data1.to_csv(\"./ignore/LR_last_stmt/num_VIF_data_results_last_stmt.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCATransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vif_data):\n",
    "        self.vif_data = vif_data\n",
    "        \n",
    "        # Get the list of the variables with the higher VIF value\n",
    "        self.vif_variable_lst = vif_data1[vif_data1['VIF'] >= 11]['feature'].tolist() \n",
    "\n",
    "        # List of columns that were considered in pca\n",
    "        self.pca_cols_to_drop = set()\n",
    "\n",
    "        self.pca_models = []\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Looping on VIF list\n",
    "        for i in self.vif_variable_lst:   \n",
    "            # Check if the VIF variable is already in the set from the earlier pass\n",
    "            bool1 = i in self.pca_cols_to_drop\n",
    "            # print(bool1)\n",
    "            if bool1 == True:   \n",
    "                continue\n",
    "            else: \n",
    "                # Get the list of correlated columns for the current vif column processed\n",
    "                pca_list = df_corr[(df_corr[i] != 1 ) & ((df_corr[i] >= 0.7) | (df_corr[i] <= -0.7 ))].index.tolist()\n",
    "                # Perform the below logic, only when any correlated column found\n",
    "                if len(pca_list) != 0:\n",
    "\n",
    "                    # Add the processed VIF column also\n",
    "                    pca_list = [i] + pca_list\n",
    "                    # print(pca_list)\n",
    "\n",
    "                    # Append this list to set, as these columns at the later stage needs to be dropped off from the main dataframe, because then the PCA values will be used instead of original column\n",
    "                    self.pca_cols_to_drop.update(pca_list)\n",
    "                    \n",
    "                    # Create the dataframe of only those columns that are correlated with the column processed in the loop\n",
    "                    df_pca = X.loc[:,pca_list]\n",
    "                    # Create instance of PCA model\n",
    "                    pca = PCA(random_state=rand_state)\n",
    "                    pca.fit_transform(df_pca)\n",
    "\n",
    "                    # Create eigen value array\n",
    "                    eigen_arr = pca.explained_variance_\n",
    "                    # Create a filter array where the eigen value should be >= 1\n",
    "                    filter_arr = eigen_arr >=1\n",
    "                    # No. of components with eigen value >= 1\n",
    "                    no_of_components = len(eigen_arr[filter_arr])\n",
    "                    \n",
    "                    # Run the PCA again with the no_of_components found\n",
    "                    pca = PCA(n_components = no_of_components, random_state=rand_state)\n",
    "                    pca.fit(df_pca)\n",
    "                    # append the tuple of the columns went for PCA , no. of components, and the instance of the fitted PCA\n",
    "                    self.pca_models.append((pca_list, no_of_components, pca))\n",
    "\n",
    "                    \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for pca_list, no_of_components, pca in self.pca_models:\n",
    "            df_pca = X.loc[:,pca_list]\n",
    "            PCA_values = pca.transform(df_pca)\n",
    "\n",
    "            # The number of columns to create for the final PCA dataframe\n",
    "            pca_columns = []\n",
    "            for val in range(1, no_of_components + 1):\n",
    "                a = pca_list[0] + '_pca_' + str(val)\n",
    "                pca_columns += [a]\n",
    "                    \n",
    "            # Create the final PCA dataframe that will be concatenated to original dataframe\n",
    "            finalpca_df = pd.DataFrame(data = PCA_values, columns=pca_columns)\n",
    "\n",
    "            # Append this dataframe to main one\n",
    "            X = pd.concat([X, finalpca_df], axis=1)   \n",
    "\n",
    "            # Clean-up RAM & memory\n",
    "            del [[df_pca, finalpca_df]]\n",
    "\n",
    "        \n",
    "        # Now remove the columns for which pca was done\n",
    "        X.drop(columns=self.pca_cols_to_drop, inplace=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "pca_transform = PCATransform(vif_data1)\n",
    "df_pca = pca_transform.fit_transform(df_processed)\n",
    "\n",
    "pipeline.steps.append(('pca', pca_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_2</th>\n",
       "      <th>d_39</th>\n",
       "      <th>s_3</th>\n",
       "      <th>d_41</th>\n",
       "      <th>d_43</th>\n",
       "      <th>d_45</th>\n",
       "      <th>b_5</th>\n",
       "      <th>r_2</th>\n",
       "      <th>d_46</th>\n",
       "      <th>d_47</th>\n",
       "      <th>...</th>\n",
       "      <th>b_2_pca_1</th>\n",
       "      <th>r_1_pca_1</th>\n",
       "      <th>b_7_pca_1</th>\n",
       "      <th>r_5_pca_1</th>\n",
       "      <th>d_58_pca_1</th>\n",
       "      <th>b_14_pca_1</th>\n",
       "      <th>s_22_pca_1</th>\n",
       "      <th>d_103_pca_1</th>\n",
       "      <th>d_118_pca_1</th>\n",
       "      <th>d_139_pca_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.821040</td>\n",
       "      <td>-0.273666</td>\n",
       "      <td>-0.801188</td>\n",
       "      <td>-0.300113</td>\n",
       "      <td>-0.547374</td>\n",
       "      <td>0.959506</td>\n",
       "      <td>0.103385</td>\n",
       "      <td>-0.281595</td>\n",
       "      <td>-1.748215e-01</td>\n",
       "      <td>0.715993</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.126416</td>\n",
       "      <td>-0.431681</td>\n",
       "      <td>-0.989118</td>\n",
       "      <td>-0.067251</td>\n",
       "      <td>-1.936266</td>\n",
       "      <td>-0.248595</td>\n",
       "      <td>-0.367136</td>\n",
       "      <td>-1.666647</td>\n",
       "      <td>-0.847364</td>\n",
       "      <td>-0.793911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.216442</td>\n",
       "      <td>-0.509883</td>\n",
       "      <td>-0.288385</td>\n",
       "      <td>-0.321199</td>\n",
       "      <td>-0.055546</td>\n",
       "      <td>-0.908882</td>\n",
       "      <td>-0.268766</td>\n",
       "      <td>-0.306242</td>\n",
       "      <td>1.800694e+00</td>\n",
       "      <td>0.800819</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.410151</td>\n",
       "      <td>-0.412173</td>\n",
       "      <td>0.279263</td>\n",
       "      <td>-0.128608</td>\n",
       "      <td>1.512027</td>\n",
       "      <td>-0.480894</td>\n",
       "      <td>-0.229168</td>\n",
       "      <td>1.461384</td>\n",
       "      <td>-1.388189</td>\n",
       "      <td>-0.808847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.254334</td>\n",
       "      <td>-0.282455</td>\n",
       "      <td>-0.803876</td>\n",
       "      <td>-0.325074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.772884</td>\n",
       "      <td>-0.123025</td>\n",
       "      <td>-0.305016</td>\n",
       "      <td>-3.499986e-16</td>\n",
       "      <td>-0.181164</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.101578</td>\n",
       "      <td>-0.420334</td>\n",
       "      <td>-1.100035</td>\n",
       "      <td>-0.164469</td>\n",
       "      <td>-2.200544</td>\n",
       "      <td>-0.435514</td>\n",
       "      <td>-0.311676</td>\n",
       "      <td>2.982145</td>\n",
       "      <td>-0.214846</td>\n",
       "      <td>3.747307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.447220</td>\n",
       "      <td>-0.486744</td>\n",
       "      <td>3.194008</td>\n",
       "      <td>0.874783</td>\n",
       "      <td>1.643804</td>\n",
       "      <td>-0.878589</td>\n",
       "      <td>-0.259249</td>\n",
       "      <td>-0.290064</td>\n",
       "      <td>3.979660e+00</td>\n",
       "      <td>-1.575048</td>\n",
       "      <td>...</td>\n",
       "      <td>1.682061</td>\n",
       "      <td>0.209589</td>\n",
       "      <td>0.953404</td>\n",
       "      <td>-0.484654</td>\n",
       "      <td>4.035638</td>\n",
       "      <td>-0.431171</td>\n",
       "      <td>-0.404856</td>\n",
       "      <td>1.333705</td>\n",
       "      <td>-1.794753</td>\n",
       "      <td>-0.796990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.768281</td>\n",
       "      <td>0.192289</td>\n",
       "      <td>-0.543592</td>\n",
       "      <td>-0.300252</td>\n",
       "      <td>-0.659591</td>\n",
       "      <td>-0.064931</td>\n",
       "      <td>1.976733</td>\n",
       "      <td>-0.274824</td>\n",
       "      <td>4.958046e-02</td>\n",
       "      <td>-0.139017</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.715999</td>\n",
       "      <td>-0.431012</td>\n",
       "      <td>-0.847344</td>\n",
       "      <td>-0.176376</td>\n",
       "      <td>-1.254610</td>\n",
       "      <td>0.086678</td>\n",
       "      <td>-0.395240</td>\n",
       "      <td>-1.662515</td>\n",
       "      <td>-0.844693</td>\n",
       "      <td>-0.817297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        p_2      d_39       s_3      d_41      d_43      d_45       b_5  \\\n",
       "0  0.821040 -0.273666 -0.801188 -0.300113 -0.547374  0.959506  0.103385   \n",
       "1 -0.216442 -0.509883 -0.288385 -0.321199 -0.055546 -0.908882 -0.268766   \n",
       "2  1.254334 -0.282455 -0.803876 -0.325074  0.000000 -0.772884 -0.123025   \n",
       "3 -1.447220 -0.486744  3.194008  0.874783  1.643804 -0.878589 -0.259249   \n",
       "4  0.768281  0.192289 -0.543592 -0.300252 -0.659591 -0.064931  1.976733   \n",
       "\n",
       "        r_2          d_46      d_47  ...  b_2_pca_1  r_1_pca_1  b_7_pca_1  \\\n",
       "0 -0.281595 -1.748215e-01  0.715993  ...  -2.126416  -0.431681  -0.989118   \n",
       "1 -0.306242  1.800694e+00  0.800819  ...  -1.410151  -0.412173   0.279263   \n",
       "2 -0.305016 -3.499986e-16 -0.181164  ...  -2.101578  -0.420334  -1.100035   \n",
       "3 -0.290064  3.979660e+00 -1.575048  ...   1.682061   0.209589   0.953404   \n",
       "4 -0.274824  4.958046e-02 -0.139017  ...  -1.715999  -0.431012  -0.847344   \n",
       "\n",
       "   r_5_pca_1  d_58_pca_1  b_14_pca_1  s_22_pca_1  d_103_pca_1  d_118_pca_1  \\\n",
       "0  -0.067251   -1.936266   -0.248595   -0.367136    -1.666647    -0.847364   \n",
       "1  -0.128608    1.512027   -0.480894   -0.229168     1.461384    -1.388189   \n",
       "2  -0.164469   -2.200544   -0.435514   -0.311676     2.982145    -0.214846   \n",
       "3  -0.484654    4.035638   -0.431171   -0.404856     1.333705    -1.794753   \n",
       "4  -0.176376   -1.254610    0.086678   -0.395240    -1.662515    -0.844693   \n",
       "\n",
       "   d_139_pca_1  \n",
       "0    -0.793911  \n",
       "1    -0.808847  \n",
       "2     3.747307  \n",
       "3    -0.796990  \n",
       "4    -0.817297  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.236914\n",
      "         Iterations 10\n",
      "                             Logit Regression Results                            \n",
      "=================================================================================\n",
      "Dep. Variable:     last_statement_target   No. Observations:                 7000\n",
      "Model:                             Logit   Df Residuals:                     6862\n",
      "Method:                              MLE   Df Model:                          137\n",
      "Date:                   Sun, 30 Oct 2022   Pseudo R-squ.:                  0.5897\n",
      "Time:                           21:13:30   Log-Likelihood:                -1658.4\n",
      "converged:                          True   LL-Null:                       -4041.4\n",
      "Covariance Type:               nonrobust   LLR p-value:                     0.000\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "p_2            -1.0380      0.110     -9.462      0.000      -1.253      -0.823\n",
      "d_39            0.3997      0.070      5.714      0.000       0.263       0.537\n",
      "s_3             0.0103      0.099      0.103      0.918      -0.184       0.205\n",
      "d_41            0.1911      0.100      1.912      0.056      -0.005       0.387\n",
      "d_43            0.1157      0.067      1.724      0.085      -0.016       0.247\n",
      "d_45           -0.0880      0.103     -0.858      0.391      -0.289       0.113\n",
      "b_5            -0.1881      0.114     -1.647      0.099      -0.412       0.036\n",
      "r_2            -0.1061      0.104     -1.019      0.308      -0.310       0.098\n",
      "d_46            0.1855      0.058      3.216      0.001       0.072       0.299\n",
      "d_47           -0.1707      0.080     -2.125      0.034      -0.328      -0.013\n",
      "d_48            0.1465      0.092      1.589      0.112      -0.034       0.327\n",
      "b_6            -0.0210      0.125     -0.167      0.867      -0.267       0.225\n",
      "b_8             0.1785      0.081      2.197      0.028       0.019       0.338\n",
      "d_51           -0.2134      0.114     -1.876      0.061      -0.436       0.010\n",
      "b_9             0.0605      0.061      0.997      0.319      -0.059       0.180\n",
      "r_3             0.1856      0.060      3.077      0.002       0.067       0.304\n",
      "d_52            0.1273      0.053      2.385      0.017       0.023       0.232\n",
      "p_3             0.0936      0.060      1.548      0.122      -0.025       0.212\n",
      "b_10            0.0777      0.049      1.587      0.112      -0.018       0.174\n",
      "s_5            -0.0302      0.051     -0.587      0.557      -0.131       0.071\n",
      "s_6            -0.0002      0.062     -0.002      0.998      -0.121       0.121\n",
      "d_54           -0.0451      0.063     -0.719      0.472      -0.168       0.078\n",
      "s_7             0.1975      0.102      1.934      0.053      -0.003       0.398\n",
      "b_12            0.2382      0.185      1.288      0.198      -0.124       0.601\n",
      "s_8             0.1270      0.115      1.104      0.270      -0.098       0.352\n",
      "b_13           -0.1539      0.185     -0.830      0.406      -0.517       0.209\n",
      "s_9             0.0814      0.037      2.211      0.027       0.009       0.154\n",
      "d_59            0.0752      0.055      1.368      0.171      -0.033       0.183\n",
      "d_60           -0.0657      0.081     -0.808      0.419      -0.225       0.094\n",
      "d_61            0.0114      0.092      0.123      0.902      -0.169       0.192\n",
      "s_11           -0.0249      0.054     -0.457      0.647      -0.131       0.082\n",
      "d_62           -0.2559      0.106     -2.414      0.016      -0.464      -0.048\n",
      "d_65            0.1096      0.152      0.723      0.470      -0.187       0.407\n",
      "s_12            0.1048      0.044      2.355      0.019       0.018       0.192\n",
      "r_6            -0.1235      0.125     -0.991      0.322      -0.368       0.121\n",
      "s_13           -0.0481      0.088     -0.549      0.583      -0.220       0.124\n",
      "b_21            0.0074      0.087      0.084      0.933      -0.164       0.179\n",
      "d_69            0.0704      0.044      1.600      0.110      -0.016       0.157\n",
      "b_22           -0.0197      0.072     -0.272      0.785      -0.162       0.122\n",
      "d_70            0.0789      0.051      1.554      0.120      -0.021       0.178\n",
      "d_71            0.0641      0.119      0.537      0.591      -0.170       0.298\n",
      "d_72           -0.0772      0.072     -1.071      0.284      -0.219       0.064\n",
      "s_15            0.1830      0.081      2.261      0.024       0.024       0.342\n",
      "p_4             0.0303      0.053      0.575      0.565      -0.073       0.134\n",
      "b_24           -0.0323      0.093     -0.346      0.730      -0.215       0.151\n",
      "r_7             0.2117      0.187      1.131      0.258      -0.155       0.578\n",
      "b_25            0.0769      0.063      1.223      0.222      -0.046       0.200\n",
      "b_26            0.0939      0.072      1.308      0.191      -0.047       0.234\n",
      "d_78            0.0005      0.062      0.009      0.993      -0.121       0.122\n",
      "d_79            0.0702      0.086      0.817      0.414      -0.098       0.239\n",
      "s_16           -0.0289      0.048     -0.606      0.544      -0.122       0.065\n",
      "d_80           -0.0087      0.052     -0.168      0.867      -0.111       0.093\n",
      "r_10            0.0973      0.067      1.446      0.148      -0.035       0.229\n",
      "r_11            0.1193      0.042      2.821      0.005       0.036       0.202\n",
      "b_27           -0.0857      0.046     -1.857      0.063      -0.176       0.005\n",
      "d_81           -0.0156      0.078     -0.201      0.841      -0.168       0.136\n",
      "s_17            0.0302      0.040      0.757      0.449      -0.048       0.108\n",
      "r_12           -0.0093      0.062     -0.150      0.881      -0.130       0.112\n",
      "b_28            0.0725      0.108      0.668      0.504      -0.140       0.285\n",
      "d_83            0.0020      0.032      0.062      0.951      -0.062       0.066\n",
      "r_14            0.2135      0.167      1.282      0.200      -0.113       0.540\n",
      "r_15           -0.0408      0.061     -0.672      0.502      -0.160       0.078\n",
      "d_84            0.0724      0.123      0.590      0.555      -0.168       0.313\n",
      "r_16           -0.0437      0.061     -0.719      0.472      -0.163       0.075\n",
      "s_18           -0.0222      0.048     -0.460      0.645      -0.117       0.073\n",
      "d_86           -0.1665      0.077     -2.160      0.031      -0.318      -0.015\n",
      "r_17            0.2057      0.100      2.066      0.039       0.011       0.401\n",
      "r_18            0.0071      0.045      0.160      0.873      -0.080       0.095\n",
      "s_19            0.0368      0.044      0.836      0.403      -0.049       0.123\n",
      "r_19           -0.1330      0.046     -2.894      0.004      -0.223      -0.043\n",
      "b_32            0.0154      0.039      0.394      0.693      -0.061       0.092\n",
      "s_20            0.1765      0.061      2.898      0.004       0.057       0.296\n",
      "r_20            0.1004      0.136      0.738      0.461      -0.166       0.367\n",
      "r_21           -0.0453      0.067     -0.678      0.498      -0.176       0.086\n",
      "d_89            0.0703      0.133      0.528      0.598      -0.191       0.331\n",
      "r_22           -0.0442      0.034     -1.285      0.199      -0.112       0.023\n",
      "r_23            0.0542      0.045      1.193      0.233      -0.035       0.143\n",
      "d_91           -0.0654      0.094     -0.697      0.486      -0.249       0.119\n",
      "d_92           -0.3413      0.147     -2.324      0.020      -0.629      -0.054\n",
      "d_93           -0.0370      0.075     -0.491      0.623      -0.185       0.111\n",
      "d_94            0.0690      0.103      0.673      0.501      -0.132       0.270\n",
      "r_24           -0.0450      0.081     -0.553      0.581      -0.204       0.115\n",
      "r_25           -0.0199      0.080     -0.250      0.802      -0.176       0.136\n",
      "d_96           -0.0825      0.066     -1.253      0.210      -0.212       0.047\n",
      "s_23            0.0711      0.061      1.173      0.241      -0.048       0.190\n",
      "s_25            0.0149      0.039      0.382      0.702      -0.061       0.091\n",
      "s_26           -0.2229      0.171     -1.300      0.194      -0.559       0.113\n",
      "b_36            0.0362      0.038      0.945      0.344      -0.039       0.111\n",
      "r_27           -0.1417      0.042     -3.336      0.001      -0.225      -0.058\n",
      "d_109           0.0038      0.066      0.058      0.954      -0.126       0.134\n",
      "d_112          -0.1288      0.059     -2.178      0.029      -0.245      -0.013\n",
      "b_40            0.0195      0.061      0.319      0.750      -0.100       0.139\n",
      "s_27            0.0011      0.039      0.027      0.978      -0.075       0.078\n",
      "d_113          -0.0729      0.067     -1.092      0.275      -0.204       0.058\n",
      "d_121           0.3281      0.090      3.658      0.000       0.152       0.504\n",
      "d_122           0.0190      0.066      0.287      0.774      -0.111       0.149\n",
      "d_123          -0.0297      0.058     -0.510      0.610      -0.144       0.084\n",
      "d_124           0.0328      0.059      0.553      0.580      -0.084       0.149\n",
      "d_125           0.0109      0.062      0.174      0.862      -0.111       0.133\n",
      "d_127          -0.1615      0.142     -1.134      0.257      -0.441       0.118\n",
      "d_128          -0.0410      0.081     -0.506      0.613      -0.200       0.118\n",
      "d_129          -0.1322      0.080     -1.655      0.098      -0.289       0.024\n",
      "b_41            0.0624      0.053      1.184      0.236      -0.041       0.166\n",
      "d_130          -0.0496      0.058     -0.851      0.395      -0.164       0.065\n",
      "d_131           0.2340      0.097      2.418      0.016       0.044       0.424\n",
      "d_133          -0.1737      0.055     -3.152      0.002      -0.282      -0.066\n",
      "r_28           -0.1006      0.216     -0.467      0.641      -0.523       0.322\n",
      "d_140           0.0405      0.042      0.961      0.336      -0.042       0.123\n",
      "d_144          -0.0433      0.057     -0.759      0.448      -0.155       0.068\n",
      "d_145          -0.0224      0.054     -0.418      0.676      -0.128       0.083\n",
      "b_30            0.2053      0.154      1.333      0.183      -0.097       0.507\n",
      "b_38           -0.0768      0.039     -1.952      0.051      -0.154       0.000\n",
      "d_114          -0.3374      0.125     -2.706      0.007      -0.582      -0.093\n",
      "d_116           0.6080      0.626      0.972      0.331      -0.618       1.834\n",
      "d_117          -0.0646      0.023     -2.822      0.005      -0.109      -0.020\n",
      "d_120           0.0001      0.129      0.001      0.999      -0.254       0.254\n",
      "d_126          -0.0132      0.124     -0.107      0.915      -0.256       0.230\n",
      "d_68           -0.1102      0.053     -2.063      0.039      -0.215      -0.006\n",
      "b_31           -1.4071      1.229     -1.145      0.252      -3.816       1.002\n",
      "d_63_CO         0.0532      0.201      0.264      0.792      -0.341       0.448\n",
      "d_63_CR         0.2057      0.226      0.908      0.364      -0.238       0.649\n",
      "d_63_XL         0.0269      1.165      0.023      0.982      -2.257       2.311\n",
      "d_63_XM        -0.1052      0.859     -0.122      0.903      -1.789       1.579\n",
      "d_63_XZ         0.6528      0.524      1.245      0.213      -0.375       1.680\n",
      "d_64_O          0.1690      1.278      0.132      0.895      -2.337       2.675\n",
      "d_64_R          0.1842      1.277      0.144      0.885      -2.319       2.688\n",
      "d_64_U          0.0727      1.276      0.057      0.955      -2.427       2.573\n",
      "b_1_pca_1       0.2236      0.040      5.584      0.000       0.145       0.302\n",
      "b_2_pca_1       0.0835      0.047      1.761      0.078      -0.009       0.176\n",
      "r_1_pca_1       0.3446      0.105      3.295      0.001       0.140       0.550\n",
      "b_7_pca_1       0.0544      0.052      1.045      0.296      -0.048       0.156\n",
      "r_5_pca_1      -0.0721      0.078     -0.920      0.357      -0.226       0.081\n",
      "d_58_pca_1      0.0775      0.055      1.420      0.156      -0.030       0.184\n",
      "b_14_pca_1      0.0497      0.065      0.769      0.442      -0.077       0.176\n",
      "s_22_pca_1      0.0132      0.018      0.734      0.463      -0.022       0.049\n",
      "d_103_pca_1    -0.0380      0.026     -1.437      0.151      -0.090       0.014\n",
      "d_118_pca_1    -0.0211      0.048     -0.444      0.657      -0.114       0.072\n",
      "d_139_pca_1    -0.0306      0.041     -0.751      0.453      -0.110       0.049\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression model\n",
    "Xdf_SG = df_pca.drop(columns=['last_statement_target']) \n",
    "ydf_SG = df_pca['last_statement_target']\n",
    "# Split the data using stratify method, to avoid only one class data seep in train\n",
    "train_XSG, val_XSG, train_ySG, val_ySG = train_test_split(Xdf_SG,ydf_SG,test_size=0.3,random_state=rand_state,stratify = ydf_SG)\n",
    "\n",
    "# Model\n",
    "regression1_SG = sm.Logit(train_ySG,train_XSG).fit()\n",
    "print(regression1_SG.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2080,  128],\n",
       "       [ 189,  603]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "prediction_probab = regression1_SG.predict(val_XSG)\n",
    "prediction = list(map(round,prediction_probab))\n",
    "confusion_matrix(val_ySG,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic : ROC AUC = 0.956\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate roc metric \n",
    "regression1_SG_auc = roc_auc_score(val_ySG,prediction_probab)\n",
    "print('Logistic : ROC AUC = %.3f' % (regression1_SG_auc))\n",
    "\n",
    "regression1_SG_fpr,regression1_SG_tpr,_ = roc_curve(val_ySG,prediction_probab)\n",
    "plt.plot(regression1_SG_fpr,regression1_SG_tpr,marker = '.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93      2208\n",
      "         1.0       0.82      0.76      0.79       792\n",
      "\n",
      "    accuracy                           0.89      3000\n",
      "   macro avg       0.87      0.85      0.86      3000\n",
      "weighted avg       0.89      0.89      0.89      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Recall and Precision\n",
    "print(classification_report(val_ySG, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is: 0.8964285714285715\n",
      "Validation accuracy is: 0.8943333333333333\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score to find underfitting & overfitting\n",
    "pred_prob_score = regression1_SG.predict(train_XSG)\n",
    "pred = list(map(float,list(map(round,pred_prob_score))))\n",
    "\n",
    "print(f'Training accuracy is: {accuracy_score(train_ySG, pred )}')\n",
    "print(f'Validation accuracy is: {accuracy_score(val_ySG, prediction)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.252714\n",
      "         Iterations 8\n",
      "                             Logit Regression Results                            \n",
      "=================================================================================\n",
      "Dep. Variable:     last_statement_target   No. Observations:                 7000\n",
      "Model:                             Logit   Df Residuals:                     6968\n",
      "Method:                              MLE   Df Model:                           31\n",
      "Date:                   Sun, 30 Oct 2022   Pseudo R-squ.:                  0.5623\n",
      "Time:                           21:13:31   Log-Likelihood:                -1769.0\n",
      "converged:                          True   LL-Null:                       -4041.4\n",
      "Covariance Type:               nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "p_2           -1.2971      0.074    -17.486      0.000      -1.442      -1.152\n",
      "d_39           0.4583      0.064      7.173      0.000       0.333       0.583\n",
      "d_41           0.2349      0.086      2.743      0.006       0.067       0.403\n",
      "d_46           0.2292      0.048      4.758      0.000       0.135       0.324\n",
      "d_47          -0.1242      0.071     -1.759      0.079      -0.262       0.014\n",
      "b_8            0.1879      0.054      3.485      0.000       0.082       0.294\n",
      "d_51          -0.1813      0.081     -2.238      0.025      -0.340      -0.023\n",
      "r_3            0.2011      0.046      4.397      0.000       0.111       0.291\n",
      "d_52           0.1075      0.048      2.253      0.024       0.014       0.201\n",
      "s_7            0.1803      0.042      4.246      0.000       0.097       0.264\n",
      "s_9            0.0975      0.037      2.666      0.008       0.026       0.169\n",
      "d_62          -0.4470      0.094     -4.773      0.000      -0.631      -0.263\n",
      "s_12           0.0877      0.039      2.222      0.026       0.010       0.165\n",
      "s_15           0.1525      0.045      3.368      0.001       0.064       0.241\n",
      "r_11           0.1077      0.040      2.688      0.007       0.029       0.186\n",
      "b_27          -0.0724      0.044     -1.650      0.099      -0.158       0.014\n",
      "d_86          -0.1189      0.067     -1.767      0.077      -0.251       0.013\n",
      "r_17           0.2053      0.094      2.187      0.029       0.021       0.389\n",
      "r_19          -0.0262      0.036     -0.725      0.468      -0.097       0.045\n",
      "s_20           0.1484      0.043      3.423      0.001       0.063       0.233\n",
      "d_92          -0.2441      0.108     -2.263      0.024      -0.456      -0.033\n",
      "r_27          -0.1470      0.040     -3.687      0.000      -0.225      -0.069\n",
      "d_112         -0.1170      0.055     -2.122      0.034      -0.225      -0.009\n",
      "d_121          0.4898      0.061      7.968      0.000       0.369       0.610\n",
      "d_131          0.2652      0.054      4.923      0.000       0.160       0.371\n",
      "d_133         -0.1427      0.052     -2.730      0.006      -0.245      -0.040\n",
      "b_38          -0.0911      0.029     -3.133      0.002      -0.148      -0.034\n",
      "d_114         -0.6315      0.094     -6.746      0.000      -0.815      -0.448\n",
      "d_117         -0.0865      0.019     -4.529      0.000      -0.124      -0.049\n",
      "d_68          -0.2347      0.021    -10.941      0.000      -0.277      -0.193\n",
      "b_1_pca_1      0.3711      0.024     15.215      0.000       0.323       0.419\n",
      "r_1_pca_1      0.3104      0.053      5.827      0.000       0.206       0.415\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Remove the insignificant features and train the model again. I will keep the alpha level as 0.07\n",
    "logit_pvalues = round(regression1_SG.pvalues,3)\n",
    "high_pval_col = logit_pvalues.index[logit_pvalues > 0.07]\n",
    "\n",
    "# Drop these columns\n",
    "Xdf_SG_new = Xdf_SG.drop(columns=high_pval_col).copy()\n",
    "\n",
    "# Split the data using stratify method, to avoid only one class data seep in train\n",
    "train_XSG_new, val_XSG_new, train_ySG_new, val_ySG_new = train_test_split(Xdf_SG_new, ydf_SG, test_size=0.3, random_state=rand_state, stratify = ydf_SG)\n",
    "\n",
    "# Model\n",
    "regression2_SG = sm.Logit(train_ySG_new,train_XSG_new).fit()\n",
    "print(regression2_SG.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(Xdf_SG_new.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.886\n",
      "Logistic : ROC AUC = 0.950\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After removing the insignificant variables, the AUC score and validation accuracy is still maintained\n",
    "\n",
    "prediction_probab_new = regression2_SG.predict(val_XSG_new)\n",
    "prediction_new = list(map(round,prediction_probab_new))\n",
    "\n",
    "print(f'Validation accuracy is: {accuracy_score(val_ySG_new, prediction_new)}')\n",
    "\n",
    "# Calculate roc metric \n",
    "regression2_SG_auc = roc_auc_score(val_ySG_new,prediction_probab_new)\n",
    "print('Logistic : ROC AUC = %.3f' % (regression2_SG_auc))\n",
    "\n",
    "regression2_SG_fpr,regression2_SG_tpr,_ = roc_curve(val_ySG_new,prediction_probab_new)\n",
    "plt.plot(regression2_SG_fpr,regression2_SG_tpr,marker = '.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression using sklearn**\n",
    "\n",
    "*Consider features from the second statsmodel - after removing insignificant features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression(random_state=rand_state,  max_iter=100) #,  max_iter=200\n",
    "log_reg.fit(train_XSG_new,train_ySG_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8893333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_reg.predict(val_XSG_new)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(val_ySG_new,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(df):\n",
    "    # Just add extra columns with 0 value so that pipeline does not fail --> these are the extra columns that we had in the training data\n",
    "    extra_cols = ['row_number', 'last_statement_flag_drop', 'target', 'last_statement_flag', 'last_statement_target']\n",
    "    # Concatenate the dataframe of extra columns with the dataframe of the test data\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.DataFrame(np.zeros((df.shape[0], len(extra_cols))), columns=extra_cols)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Use the pipeline to transform\n",
    "    X = pipeline.transform(df)\n",
    "\n",
    "    # Drop target & the insignificant variables found during the training using statsmodel p-value\n",
    "    X.drop(columns=['last_statement_target'] + high_pval_col.tolist(), inplace=True)\n",
    "\n",
    "    # return log_reg.predict(X), log_reg.predict_proba(X)\n",
    "    # In the statsmodel predict will give the probability\n",
    "    return list(map(round,regression2_SG.predict(X))), regression2_SG.predict(X).tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1000001\n",
      "2000001\n",
      "3000001\n",
      "4000001\n",
      "5000001\n",
      "6000001\n",
      "7000001\n",
      "8000001\n",
      "9000001\n",
      "10000001\n",
      "11000001\n",
      "12000001\n"
     ]
    }
   ],
   "source": [
    "path = './ignore/test_data.csv/test_data.csv'\n",
    "\n",
    "current_position = 1 \n",
    "split_num_lines = 1000000\n",
    "columns = pd.read_csv(path, nrows=1).columns # Just read columns\n",
    "    \n",
    "# Define the result mdf\n",
    "mdf = pd.DataFrame(columns=['customer_id', 's_2', 'pred', 'proba'])\n",
    "\n",
    "# Get chunks from the test_data.csv and send them to the model\n",
    "while True:\n",
    "    print(current_position)        \n",
    "    df_chunk = pd.read_csv(path, skiprows=current_position, nrows=split_num_lines, header=None, names=columns)\n",
    "    if df_chunk.shape[0] == 0:\n",
    "        break\n",
    "    \n",
    "    df_chunk.columns= df_chunk.columns.str.lower()\n",
    "    y, y_proba = execute_model(df_chunk)\n",
    "    \n",
    "    mdf = pd.concat([\n",
    "        mdf,\n",
    "        pd.DataFrame({\n",
    "            'customer_id': df_chunk['customer_id'].values,\n",
    "            's_2': df_chunk['s_2'].values,\n",
    "            'pred': y,\n",
    "            'proba': y_proba\n",
    "        })\n",
    "    ])\n",
    "\n",
    "    current_position += split_num_lines\n",
    "    \n",
    "mdf.to_csv('./ignore/LR_last_stmt/logistic_regression_prediction_last_stmt.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the last statement probability of each customer (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>s_2</th>\n",
       "      <th>pred</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7</td>\n",
       "      <td>2019-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7</td>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.085414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7</td>\n",
       "      <td>2019-04-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.074820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7</td>\n",
       "      <td>2019-05-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7</td>\n",
       "      <td>2019-06-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        customer_id  \\\n",
       "0  00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7   \n",
       "1  00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7   \n",
       "2  00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7   \n",
       "3  00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7   \n",
       "4  00000469ba478561f23a92a868bd366de6f6527a684c9a2e78fb826dcac3b9b7   \n",
       "\n",
       "         s_2  pred     proba  \n",
       "0 2019-02-19     0  0.097385  \n",
       "1 2019-03-25     0  0.085414  \n",
       "2 2019-04-25     0  0.074820  \n",
       "3 2019-05-20     0  0.086087  \n",
       "4 2019-06-15     0  0.119202  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the probabilities given by the model on test data\n",
    "df_results_all = pd.read_csv('./ignore/LR_last_stmt/logistic_regression_prediction_last_stmt.csv', parse_dates=['s_2'])\n",
    "df_results_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924621"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_all['customer_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last statement probability of each of the customer\n",
    "df_result_last = df_results_all.sort_values(by = 's_2').groupby('customer_id')[['customer_id','proba']].tail(1)\n",
    "df_result_last.rename(columns= {'proba' : 'prediction'},inplace=True)\n",
    "df_result_last.head()\n",
    "df_result_last.to_csv('./ignore/LR_last_stmt/last_stmt_submission_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the mean of all the probabilities of the customer's statements (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7788240</th>\n",
       "      <td>af9da794a3ec613f9db1321559ffffcf6d5e0324b6f995b9bc5a83a3aba0d882</td>\n",
       "      <td>0.828375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312152</th>\n",
       "      <td>615acaa0b35859ab76b13b3d5156acc6a208b8ec7b3b57da8288fbe82107af5b</td>\n",
       "      <td>0.379929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9146615</th>\n",
       "      <td>ce0e53cf2a7a180578e81a5e2446f8aac58e04a70d3f97bafde07a60be60167b</td>\n",
       "      <td>0.159641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11221584</th>\n",
       "      <td>fcc9e53e0d05e55eb177e7ac5c9712c32671749013b65787b2a01ab90b2c76b4</td>\n",
       "      <td>0.098254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3591564</th>\n",
       "      <td>50f5d28c9700f06722794aae605a1d784a1702db845b8d0b7bc90edda9c93539</td>\n",
       "      <td>0.013991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               customer_id  \\\n",
       "7788240   af9da794a3ec613f9db1321559ffffcf6d5e0324b6f995b9bc5a83a3aba0d882   \n",
       "4312152   615acaa0b35859ab76b13b3d5156acc6a208b8ec7b3b57da8288fbe82107af5b   \n",
       "9146615   ce0e53cf2a7a180578e81a5e2446f8aac58e04a70d3f97bafde07a60be60167b   \n",
       "11221584  fcc9e53e0d05e55eb177e7ac5c9712c32671749013b65787b2a01ab90b2c76b4   \n",
       "3591564   50f5d28c9700f06722794aae605a1d784a1702db845b8d0b7bc90edda9c93539   \n",
       "\n",
       "             proba  \n",
       "7788240   0.828375  \n",
       "4312152   0.379929  \n",
       "9146615   0.159641  \n",
       "11221584  0.098254  \n",
       "3591564   0.013991  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df_results_all.sort_values(by = 's_2')[['customer_id','proba']]\n",
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allstmt_mean_prob = df_tmp.groupby('customer_id').mean()\n",
    "df_allstmt_mean_prob.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allstmt_mean_prob.rename(columns= {'proba' : 'prediction'},inplace=True)\n",
    "df_allstmt_mean_prob.to_csv('./ignore/LR_last_stmt/mean_stmt_submission_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted probabilities using Joe's Code (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the outcome weighting.\n",
    "\n",
    "def conditions(x):\n",
    "    # Customer has 3 statements:\n",
    "    if   x == 3:   return 0.1\n",
    "    elif x == 6:   return 0.15\n",
    "    elif x == 9:   return 0.75\n",
    "    \n",
    "    # Customer has 2 statements:\n",
    "    elif x == 2:   return 0.2\n",
    "    elif x == 4:   return 0.8\n",
    "    \n",
    "    # Customer has 1 statement:\n",
    "    elif x == 1:   return 1.0 \n",
    "    else:          return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the last 3 statements of each customer\n",
    "mdf = df_results_all.sort_values('s_2').groupby('customer_id').tail(3)\n",
    "# if the customer has last 3 stmts the ranking will be as - 1st to the older stmt and 3rd rank to the latest stmt. \n",
    "mdf[\"statement_num\"] = mdf.groupby(\"customer_id\")[\"s_2\"].rank(method=\"first\", ascending=True)\n",
    "# The statement_count variable will give the count of the statements for each customer (i.e. to know if they have all the 3 or less than that)\n",
    "mdf['statement_count'] = mdf.groupby('customer_id')['statement_num'].transform('max')\n",
    "\n",
    "# Create a number so we can handle the case where a customer had only 1 or 2 statements. \n",
    "# Multiplied to give me a unique value for each case. See conditions() above.\n",
    "mdf['statement_checksum'] = (mdf['statement_count']) * mdf['statement_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the weights to the statements\n",
    "mdf['statement_weight'] = mdf['statement_checksum'].apply(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the weighted sum\n",
    "mdf ['prediction'] = mdf['proba'] * mdf['statement_weight']\n",
    "\n",
    "mdf = mdf[['customer_id', 'prediction']]\n",
    "\n",
    "# Grouping those weighted sums by customer_id to give granularity of 1 proba per customer\n",
    "mdf = mdf.groupby('customer_id').sum()\n",
    "# Bring the customer_id from index to column\n",
    "mdf.reset_index(inplace=True)\n",
    "# Send the data to the file\n",
    "mdf.to_csv('./ignore/LR_last_stmt/weighted_stmt_submission_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89a73c21ecc9236fdbb84984cd9e615404f96fb7d0e8948f841b3ff5dee670ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
