{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If customer has defaulted assigning all statements target value to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data \n",
    "df_train_x = pd.read_parquet('D:/Sakshi/DSBA_6156_SERJ/data/train.parquet')\n",
    "df_train_x.columns = df_train_x.columns.str.lower()\n",
    "# Read training data labels\n",
    "df_train_y = pd.read_csv('D:/Sakshi/DSBA_6156_SERJ/data/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "df_train = pd.merge(df_train_x, df_train_y, on='customer_id')\n",
    "del(df_train_x, df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>s_2</th>\n",
       "      <th>target</th>\n",
       "      <th>last_statement_flag</th>\n",
       "      <th>last_statement_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-04-14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-09-14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-10-14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-12-17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           customer_id         s_2  target  \\\n",
       "104  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-03-15       1   \n",
       "105  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-04-14       1   \n",
       "106  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-05-15       1   \n",
       "107  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-06-14       1   \n",
       "108  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-07-15       1   \n",
       "109  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-08-15       1   \n",
       "110  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-09-14       1   \n",
       "111  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-10-14       1   \n",
       "112  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-11-14       1   \n",
       "113  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-12-17       1   \n",
       "114  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2018-01-17       1   \n",
       "115  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2018-02-05       1   \n",
       "116  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2018-03-01       1   \n",
       "\n",
       "     last_statement_flag  last_statement_target  \n",
       "104                    0                      0  \n",
       "105                    0                      0  \n",
       "106                    0                      0  \n",
       "107                    0                      0  \n",
       "108                    0                      0  \n",
       "109                    0                      0  \n",
       "110                    0                      0  \n",
       "111                    0                      0  \n",
       "112                    0                      0  \n",
       "113                    0                      0  \n",
       "114                    0                      0  \n",
       "115                    0                      0  \n",
       "116                    1                      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['last_statement_flag'] = (df_train.groupby('customer_id')['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(int).apply(lambda x: 1 if x==1 else 0)  # Only the latest stmt of the customer will hold value 1 and others will be replaced by 0\n",
    "                   )\n",
    "# When customer has defaulted only the last stmt will get the actual value, other stmt would be 0                 \n",
    "df_train['last_statement_target'] = df_train['target'] * df_train['last_statement_flag']\n",
    "\n",
    "df_train[df_train['customer_id']== '0000f99513770170a1aba690daeeb8a96da4a39f11fc27da5c30a79db61c1e85']\\\n",
    "   [['customer_id','s_2','target','last_statement_flag','last_statement_target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes.to_csv('../ignore/final/output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_63     0.0\n",
       "d_64     0.0\n",
       "d_66     0.0\n",
       "d_68     0.0\n",
       "b_30     0.0\n",
       "b_31     0.0\n",
       "b_38     0.0\n",
       "d_114    0.0\n",
       "d_116    0.0\n",
       "d_117    0.0\n",
       "d_120    0.0\n",
       "d_126    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[])\n",
    "# 89% of d_66 column values were missing, but it has been filled with -1 while parquet generation  \n",
    "categorical_cols = ['d_63', 'd_64', 'd_66', 'd_68', 'b_30', 'b_31', 'b_38',\\\n",
    "                     'd_114', 'd_116', 'd_117', 'd_120', 'd_126']\n",
    "df_train[categorical_cols].isnull().sum() / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop, categorical_cols):\n",
    "    \n",
    "        self.cols_to_drop = cols_to_drop + [\"customer_id\", \"s_2\"]\n",
    "        self.categorical_cols = categorical_cols\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        # Get the list of columns that have missing values greater than equal to 40%\n",
    "        missing_perc = round((X.isnull().sum() / len(X)) * 100,2)\n",
    "        # Prepare final List of columns to drop\n",
    "        self.cols_to_drop = self.cols_to_drop + missing_perc[missing_perc.ge(40)].index.tolist()\n",
    " \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Get the clean dataframe with columns to work\n",
    "        X = X.drop(columns=self.cols_to_drop)\n",
    "\n",
    "        numeric_cols = list(set(X.columns.tolist(\n",
    "        )) - set(self.categorical_cols + ['target']))\n",
    "\n",
    "        numeric_cols = X.drop(columns=self.categorical_cols + ['target']).columns.tolist()\n",
    "        for col in numeric_cols:\n",
    "            # Impute col with mean\n",
    "            if X[col].isnull().any().any():\n",
    "                X[col] = X[col].fillna(X[col].mean())\n",
    "\n",
    "            # Scale\n",
    "            mean = X[col].mean()\n",
    "            std = X[col].std()\n",
    "            if std > 0:\n",
    "                X[col] = ((X[col] - mean) / std).astype('float32')\n",
    "        \n",
    "        # # Apply the transformation defined in pipeline\n",
    "        # full_transformer = ColumnTransformer(\n",
    "        #     transformers=[\n",
    "        #         (\"numeric\", self.numeric_pipeline, numeric_cols),\n",
    "        #         (\"categorical\", self.categorical_pipeline, self.categorical_cols),\n",
    "        #     ],\n",
    "        #     # to keep the target column and just passthrough it, instead of dropping\n",
    "        #     remainder='passthrough'\n",
    "        # )\n",
    "\n",
    "        # transformed = full_transformer.fit_transform(df)\n",
    "        # Converted the array to dataframe\n",
    "        # df_transformed = pd.DataFrame(data=transformed, columns= numeric_cols +\\\n",
    "        #                            self.categorical_cols + ['target']).astype(df.dtypes.to_dict())\n",
    "\n",
    "        return X\n",
    "\n",
    "# use all the statements of a customer where all stmts are marked with the same target value\n",
    "preprocessing = PreProcessing([\"last_statement_flag\", \"last_statement_target\"], categorical_cols)\n",
    "df_processed = preprocessing.fit_transform(df_train)\n",
    "\n",
    "pipeline.steps.append(('preprocessing', preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw dataframe shape : (5531451, 193)\n",
      "The raw dataframe shape : (5531451, 171)\n"
     ]
    }
   ],
   "source": [
    "print(f'The raw dataframe shape : {df_train.shape}')\n",
    "print(f'The raw dataframe shape : {df_processed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_2</th>\n",
       "      <th>d_39</th>\n",
       "      <th>b_1</th>\n",
       "      <th>b_2</th>\n",
       "      <th>r_1</th>\n",
       "      <th>s_3</th>\n",
       "      <th>d_41</th>\n",
       "      <th>b_3</th>\n",
       "      <th>d_43</th>\n",
       "      <th>d_44</th>\n",
       "      <th>...</th>\n",
       "      <th>d_136</th>\n",
       "      <th>d_137</th>\n",
       "      <th>d_138</th>\n",
       "      <th>d_139</th>\n",
       "      <th>d_140</th>\n",
       "      <th>d_141</th>\n",
       "      <th>d_143</th>\n",
       "      <th>d_144</th>\n",
       "      <th>d_145</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.158043</td>\n",
       "      <td>-0.548473</td>\n",
       "      <td>-0.543834</td>\n",
       "      <td>0.959979</td>\n",
       "      <td>-0.307314</td>\n",
       "      <td>-0.583097</td>\n",
       "      <td>-0.272179</td>\n",
       "      <td>-0.544072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.458047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174922</td>\n",
       "      <td>-0.190121</td>\n",
       "      <td>-0.176598</td>\n",
       "      <td>-0.374138</td>\n",
       "      <td>-0.083563</td>\n",
       "      <td>-0.462315</td>\n",
       "      <td>-0.373928</td>\n",
       "      <td>-0.284754</td>\n",
       "      <td>-0.283384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.150638</td>\n",
       "      <td>-0.548473</td>\n",
       "      <td>-0.561764</td>\n",
       "      <td>0.944571</td>\n",
       "      <td>-0.320903</td>\n",
       "      <td>-0.567548</td>\n",
       "      <td>-0.272179</td>\n",
       "      <td>-0.552566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.458047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174922</td>\n",
       "      <td>-0.190121</td>\n",
       "      <td>-0.176598</td>\n",
       "      <td>-0.374138</td>\n",
       "      <td>-0.083563</td>\n",
       "      <td>-0.462315</td>\n",
       "      <td>-0.373928</td>\n",
       "      <td>-0.257905</td>\n",
       "      <td>-0.283384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.222533</td>\n",
       "      <td>-0.221741</td>\n",
       "      <td>-0.482838</td>\n",
       "      <td>0.967039</td>\n",
       "      <td>-0.317970</td>\n",
       "      <td>-0.583432</td>\n",
       "      <td>-0.272179</td>\n",
       "      <td>-0.524010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.458047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174922</td>\n",
       "      <td>-0.190121</td>\n",
       "      <td>-0.176598</td>\n",
       "      <td>-0.374138</td>\n",
       "      <td>-0.083563</td>\n",
       "      <td>-0.462315</td>\n",
       "      <td>-0.373928</td>\n",
       "      <td>-0.249690</td>\n",
       "      <td>-0.283384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.247995</td>\n",
       "      <td>-0.548473</td>\n",
       "      <td>-0.520441</td>\n",
       "      <td>0.949671</td>\n",
       "      <td>-0.342011</td>\n",
       "      <td>-0.622420</td>\n",
       "      <td>-0.272179</td>\n",
       "      <td>-0.540573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.458047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174922</td>\n",
       "      <td>-0.190121</td>\n",
       "      <td>-0.176598</td>\n",
       "      <td>-0.374138</td>\n",
       "      <td>-0.083563</td>\n",
       "      <td>-0.462315</td>\n",
       "      <td>-0.373928</td>\n",
       "      <td>-0.252211</td>\n",
       "      <td>-0.283384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.194080</td>\n",
       "      <td>-0.548473</td>\n",
       "      <td>-0.513318</td>\n",
       "      <td>0.944755</td>\n",
       "      <td>-0.314483</td>\n",
       "      <td>-0.621529</td>\n",
       "      <td>-0.272179</td>\n",
       "      <td>-0.524483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.458047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174922</td>\n",
       "      <td>-0.190121</td>\n",
       "      <td>-0.176598</td>\n",
       "      <td>-0.374138</td>\n",
       "      <td>-0.083563</td>\n",
       "      <td>-0.462315</td>\n",
       "      <td>-0.373928</td>\n",
       "      <td>-0.243418</td>\n",
       "      <td>-0.283384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        p_2      d_39       b_1       b_2       r_1       s_3      d_41  \\\n",
       "0  1.158043 -0.548473 -0.543834  0.959979 -0.307314 -0.583097 -0.272179   \n",
       "1  1.150638 -0.548473 -0.561764  0.944571 -0.320903 -0.567548 -0.272179   \n",
       "2  1.222533 -0.221741 -0.482838  0.967039 -0.317970 -0.583432 -0.272179   \n",
       "3  1.247995 -0.548473 -0.520441  0.949671 -0.342011 -0.622420 -0.272179   \n",
       "4  1.194080 -0.548473 -0.513318  0.944755 -0.314483 -0.621529 -0.272179   \n",
       "\n",
       "        b_3  d_43      d_44  ...     d_136     d_137     d_138     d_139  \\\n",
       "0 -0.544072   0.0 -0.458047  ... -0.174922 -0.190121 -0.176598 -0.374138   \n",
       "1 -0.552566   0.0 -0.458047  ... -0.174922 -0.190121 -0.176598 -0.374138   \n",
       "2 -0.524010   0.0 -0.458047  ... -0.174922 -0.190121 -0.176598 -0.374138   \n",
       "3 -0.540573   0.0 -0.458047  ... -0.174922 -0.190121 -0.176598 -0.374138   \n",
       "4 -0.524483   0.0 -0.458047  ... -0.174922 -0.190121 -0.176598 -0.374138   \n",
       "\n",
       "      d_140     d_141     d_143     d_144     d_145  target  \n",
       "0 -0.083563 -0.462315 -0.373928 -0.284754 -0.283384       0  \n",
       "1 -0.083563 -0.462315 -0.373928 -0.257905 -0.283384       0  \n",
       "2 -0.083563 -0.462315 -0.373928 -0.249690 -0.283384       0  \n",
       "3 -0.083563 -0.462315 -0.373928 -0.252211 -0.283384       0  \n",
       "4 -0.083563 -0.462315 -0.373928 -0.243418 -0.283384       0  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(df_train)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the correlation and VIF, we will consider the numerical columns only.\n",
    "cols_to_drop =  categorical_cols + ['target']\n",
    "#Numerical columns dataframe\n",
    "df_numerical = df_processed.drop(columns=cols_to_drop).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def sklearn_vif(data):\n",
    "\n",
    "    # initialize dictionaries\n",
    "    result = {}\n",
    "\n",
    "    # form input data for each exogenous variable\n",
    "    exogs = data.columns.to_list()\n",
    "    for exog in exogs:\n",
    "        # print(exog)\n",
    "        not_exog = [i for i in exogs if i != exog]\n",
    "        \n",
    "        X, y = data[not_exog], data[exog]  # exog would be for which the VIF has to be calculated based on the combination of other columns\n",
    "\n",
    "        # extract r-squared from the fit\n",
    "        r_squared = LinearRegression(n_jobs=12).fit(X, y).score(X, y)\n",
    "\n",
    "        # calculate VIF\n",
    "        vif = 1/(1 - r_squared)\n",
    "        result[exog] = vif\n",
    "    return result\n",
    "\n",
    "vif_data1 = sklearn_vif(df_numerical)\n",
    "df_vif = pd.DataFrame({\n",
    "    'feature': vif_data1.keys(),\n",
    "    'VIF': vif_data1.values()\n",
    "})\n",
    "del(vif_data1)\n",
    "df_vif.to_csv(\"../ignore/final/num_VIF_data_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the correlation after transposing the array\n",
    "df_corr = df_numerical.corr()\n",
    "df_corr.to_csv(\"../ignore/final/num_corr_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to avoid re-running of the VIF and correlation calculation (to save time)\n",
    "df_corr = pd.read_csv(\"D:/Sakshi/DSBA_6156_SERJ/ignore/final/num_corr_results.csv\", index_col='Unnamed: 0')\n",
    "df_vif = pd.read_csv(\"D:/Sakshi/DSBA_6156_SERJ/ignore/final/num_VIF_data_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 2903\n",
    "class PCATransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vif_data):\n",
    "        self.vif_data = vif_data\n",
    "        \n",
    "        # Get the list of the variables with the higher VIF value\n",
    "        self.vif_variable_lst = vif_data[vif_data['VIF'] >= 11]['feature'].tolist() \n",
    "\n",
    "        # List of columns that were considered in pca\n",
    "        self.pca_cols_to_drop = set()\n",
    "\n",
    "        self.pca_models = []\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Looping on VIF list\n",
    "        for i in self.vif_variable_lst:   \n",
    "            # Check if the VIF variable is already in the set from the earlier pass\n",
    "            bool1 = i in self.pca_cols_to_drop\n",
    "            # print(bool1)\n",
    "            if bool1 == True:   \n",
    "                continue\n",
    "            else: \n",
    "                # Get the list of correlated columns for the current vif column processed\n",
    "                pca_list = df_corr[(df_corr[i] != 1 ) &\\\n",
    "                     ((df_corr[i] >= 0.7) | (df_corr[i] <= -0.7 ))].index.tolist()\n",
    "                # Perform the below logic, only when any correlated column found\n",
    "                if len(pca_list) != 0:\n",
    "\n",
    "                    # Add the processed VIF column also\n",
    "                    pca_list = [i] + pca_list\n",
    "                    # print(pca_list)\n",
    "\n",
    "                    # Append this list to set, as these columns at the later stage needs to be dropped off\n",
    "                    # from the main dataframe, because then the PCA values will be used instead of original column\n",
    "                    self.pca_cols_to_drop.update(pca_list)\n",
    "                    \n",
    "                    # Create the dataframe of only those columns that are correlated with the column processed in the loop\n",
    "                    df_pca = X.loc[:,pca_list]\n",
    "                    # Create instance of PCA model\n",
    "                    pca = PCA(random_state=rand_state)\n",
    "                    pca.fit_transform(df_pca)\n",
    "\n",
    "                    # Create eigen value array\n",
    "                    eigen_arr = pca.explained_variance_\n",
    "                    # Create a filter array where the eigen value should be >= 1\n",
    "                    filter_arr = eigen_arr >=1\n",
    "                    # No. of components with eigen value >= 1\n",
    "                    no_of_components = len(eigen_arr[filter_arr])\n",
    "                    \n",
    "                    # Run the PCA again with the no_of_components found\n",
    "                    pca = PCA(n_components = no_of_components, random_state=rand_state)\n",
    "                    pca.fit(df_pca)\n",
    "                    # append the tuple of the columns went for PCA , no. of components,\n",
    "                    # and the instance of the fitted PCA\n",
    "                    self.pca_models.append((pca_list, no_of_components, pca))\n",
    "                    \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for pca_list, no_of_components, pca in self.pca_models:\n",
    "            df_pca = X.loc[:,pca_list]\n",
    "            PCA_values = pca.transform(df_pca)\n",
    "\n",
    "            # The number of columns to create for the final PCA dataframe\n",
    "            pca_columns = []\n",
    "            for val in range(1, no_of_components + 1):\n",
    "                a = pca_list[0] + '_pca_' + str(val)\n",
    "                pca_columns += [a]\n",
    "                    \n",
    "            # Create the final PCA dataframe that will be concatenated to original dataframe\n",
    "            finalpca_df = pd.DataFrame(data = PCA_values, columns=pca_columns)\n",
    "\n",
    "            # Append this dataframe to main one\n",
    "            X = pd.concat([X, finalpca_df], axis=1)   \n",
    "\n",
    "            # Clean-up RAM & memory\n",
    "            del [[df_pca, finalpca_df]]\n",
    "\n",
    "        \n",
    "        # Now remove the columns for which pca was done\n",
    "        X.drop(columns=self.pca_cols_to_drop, inplace=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "pca_transform = PCATransform(df_vif)\n",
    "df_pca = pca_transform.fit_transform(df_processed)\n",
    "\n",
    "pipeline.steps.append(('pca', pca_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.285962\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:              3872015\n",
      "Model:                          Logit   Df Residuals:                  3871873\n",
      "Method:                           MLE   Df Model:                          141\n",
      "Date:                Tue, 13 Dec 2022   Pseudo R-squ.:                  0.4906\n",
      "Time:                        23:09:39   Log-Likelihood:            -1.1072e+06\n",
      "converged:                       True   LL-Null:                   -2.1735e+06\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "p_2            -0.8064      0.004   -215.335      0.000      -0.814      -0.799\n",
      "d_39            0.1111      0.002     56.403      0.000       0.107       0.115\n",
      "r_1             0.1181      0.005     26.030      0.000       0.109       0.127\n",
      "s_3             0.1025      0.004     28.105      0.000       0.095       0.110\n",
      "d_41            0.0927      0.003     35.076      0.000       0.088       0.098\n",
      "d_43            0.0987      0.002     40.465      0.000       0.094       0.103\n",
      "d_45           -0.1079      0.004    -28.468      0.000      -0.115      -0.101\n",
      "b_5            -0.0554      0.004    -14.254      0.000      -0.063      -0.048\n",
      "r_2             0.0262      0.003      7.740      0.000       0.020       0.033\n",
      "d_46            0.1440      0.002     73.257      0.000       0.140       0.148\n",
      "d_47           -0.1517      0.003    -50.317      0.000      -0.158      -0.146\n",
      "d_48            0.0698      0.003     21.882      0.000       0.064       0.076\n",
      "d_49            0.0533      0.002     23.400      0.000       0.049       0.058\n",
      "b_6            -0.1373      0.011    -12.584      0.000      -0.159      -0.116\n",
      "b_8             0.1990      0.003     67.523      0.000       0.193       0.205\n",
      "d_51           -0.2550      0.004    -62.666      0.000      -0.263      -0.247\n",
      "b_9             0.0548      0.002     24.377      0.000       0.050       0.059\n",
      "r_3             0.1783      0.002     76.389      0.000       0.174       0.183\n",
      "d_52           -0.0450      0.002    -21.009      0.000      -0.049      -0.041\n",
      "p_3             0.0613      0.002     30.661      0.000       0.057       0.065\n",
      "b_10           -0.0036      0.003     -1.174      0.241      -0.010       0.002\n",
      "s_5             0.0260      0.002     14.181      0.000       0.022       0.030\n",
      "s_6             0.0168      0.002      6.788      0.000       0.012       0.022\n",
      "d_54           -0.0422      0.002    -23.716      0.000      -0.046      -0.039\n",
      "r_4             0.0022      0.004      0.574      0.566      -0.005       0.010\n",
      "s_7             0.0755      0.004     20.481      0.000       0.068       0.083\n",
      "b_12           -0.1123      0.009    -13.170      0.000      -0.129      -0.096\n",
      "s_8            -0.1010      0.004    -24.904      0.000      -0.109      -0.093\n",
      "b_13            0.0665      0.005     13.024      0.000       0.056       0.076\n",
      "d_59           -0.0489      0.002    -25.973      0.000      -0.053      -0.045\n",
      "d_60            0.0724      0.003     25.217      0.000       0.067       0.078\n",
      "d_61            0.0858      0.005     17.747      0.000       0.076       0.095\n",
      "s_11           -0.1075      0.002    -51.993      0.000      -0.112      -0.103\n",
      "d_62           -0.2188      0.004    -57.826      0.000      -0.226      -0.211\n",
      "d_63           -0.0004      0.002     -0.189      0.850      -0.004       0.004\n",
      "d_64            0.0161      0.002     10.180      0.000       0.013       0.019\n",
      "d_65            0.0317      0.003     12.141      0.000       0.027       0.037\n",
      "d_66           -0.1647      0.003    -54.063      0.000      -0.171      -0.159\n",
      "d_68           -0.0248      0.002    -15.740      0.000      -0.028      -0.022\n",
      "s_12            0.0440      0.002     23.913      0.000       0.040       0.048\n",
      "r_6             0.0035      0.002      1.583      0.114      -0.001       0.008\n",
      "s_13            0.0772      0.003     24.459      0.000       0.071       0.083\n",
      "b_21            0.0175      0.003      6.516      0.000       0.012       0.023\n",
      "d_69            0.0006      0.001      0.581      0.561      -0.001       0.003\n",
      "b_22            0.0378      0.003     14.548      0.000       0.033       0.043\n",
      "d_70            0.0465      0.002     24.495      0.000       0.043       0.050\n",
      "d_71           -0.0274      0.006     -4.409      0.000      -0.040      -0.015\n",
      "d_72           -0.0107      0.002     -4.868      0.000      -0.015      -0.006\n",
      "s_15            0.0231      0.003      8.328      0.000       0.018       0.029\n",
      "p_4             0.1218      0.002     61.100      0.000       0.118       0.126\n",
      "b_24           -0.0118      0.003     -4.120      0.000      -0.017      -0.006\n",
      "r_7             0.0032      0.002      1.720      0.085      -0.000       0.007\n",
      "b_25            0.0029      0.002      1.370      0.171      -0.001       0.007\n",
      "b_26            0.0085      0.002      4.442      0.000       0.005       0.012\n",
      "d_78           -0.0250      0.002    -12.584      0.000      -0.029      -0.021\n",
      "d_79           -0.0148      0.003     -4.983      0.000      -0.021      -0.009\n",
      "r_9             0.0033      0.002      2.113      0.035       0.000       0.006\n",
      "s_16            0.0089      0.002      4.167      0.000       0.005       0.013\n",
      "d_80            0.0437      0.002     24.120      0.000       0.040       0.047\n",
      "r_10           -0.0090      0.003     -3.294      0.001      -0.014      -0.004\n",
      "r_11            0.0607      0.002     38.512      0.000       0.058       0.064\n",
      "b_27            0.0010      0.002      0.563      0.573      -0.002       0.004\n",
      "d_81            0.0025      0.002      1.061      0.289      -0.002       0.007\n",
      "d_82            0.0951      0.003     35.372      0.000       0.090       0.100\n",
      "s_17            0.0126      0.002      8.347      0.000       0.010       0.016\n",
      "r_12           -0.0277      0.002    -12.860      0.000      -0.032      -0.023\n",
      "d_83            0.0081      0.002      5.045      0.000       0.005       0.011\n",
      "r_14            0.0139      0.002      8.046      0.000       0.011       0.017\n",
      "r_15           -0.0023      0.002     -1.080      0.280      -0.006       0.002\n",
      "d_84           -0.0134      0.003     -3.900      0.000      -0.020      -0.007\n",
      "r_16           -0.0294      0.002    -14.642      0.000      -0.033      -0.025\n",
      "b_30            0.0313      0.006      5.206      0.000       0.020       0.043\n",
      "s_18            0.0405      0.002     21.370      0.000       0.037       0.044\n",
      "d_86           -0.0545      0.002    -22.396      0.000      -0.059      -0.050\n",
      "d_87           -0.0078      0.002     -4.534      0.000      -0.011      -0.004\n",
      "r_17            0.0049      0.002      2.122      0.034       0.000       0.009\n",
      "r_18           -0.0032      0.001     -2.287      0.022      -0.006      -0.000\n",
      "b_31           -2.0528      0.012   -167.592      0.000      -2.077      -2.029\n",
      "s_19            0.0082      0.002      4.923      0.000       0.005       0.011\n",
      "r_19           -0.0385      0.002    -24.279      0.000      -0.042      -0.035\n",
      "b_32           -0.0232      0.001    -15.772      0.000      -0.026      -0.020\n",
      "s_20            0.0235      0.002     10.081      0.000       0.019       0.028\n",
      "r_20           -0.0015      0.003     -0.533      0.594      -0.007       0.004\n",
      "r_21            0.0271      0.003      9.849      0.000       0.022       0.033\n",
      "d_89           -0.0408      0.002    -16.627      0.000      -0.046      -0.036\n",
      "r_22           -0.0152      0.001    -11.242      0.000      -0.018      -0.013\n",
      "r_23            0.0130      0.001      8.957      0.000       0.010       0.016\n",
      "d_91           -0.0525      0.003    -16.888      0.000      -0.059      -0.046\n",
      "d_92            0.0001      0.004      0.032      0.974      -0.008       0.008\n",
      "d_93            0.0244      0.002     10.125      0.000       0.020       0.029\n",
      "d_94           -0.0230      0.005     -4.958      0.000      -0.032      -0.014\n",
      "r_24           -0.0134      0.003     -4.601      0.000      -0.019      -0.008\n",
      "r_25           -0.0393      0.002    -18.638      0.000      -0.043      -0.035\n",
      "d_96           -0.0328      0.002    -13.810      0.000      -0.038      -0.028\n",
      "s_23            0.0331      0.005      7.273      0.000       0.024       0.042\n",
      "s_25            0.0010      0.001      0.715      0.474      -0.002       0.004\n",
      "s_26           -0.0241      0.004     -6.450      0.000      -0.031      -0.017\n",
      "d_102          -0.0494      0.002    -19.975      0.000      -0.054      -0.045\n",
      "d_106          -0.0026      0.002     -1.521      0.128      -0.006       0.001\n",
      "b_36            0.0106      0.003      4.163      0.000       0.006       0.016\n",
      "r_26            0.0189      0.002      8.710      0.000       0.015       0.023\n",
      "r_27           -0.1037      0.002    -60.467      0.000      -0.107      -0.100\n",
      "b_38           -0.0027      0.002     -1.790      0.074      -0.006       0.000\n",
      "d_108           0.0182      0.002     11.865      0.000       0.015       0.021\n",
      "d_109          -0.0039      0.002     -1.557      0.119      -0.009       0.001\n",
      "d_111           0.0688      0.003     27.107      0.000       0.064       0.074\n",
      "d_112          -0.1297      0.002    -61.002      0.000      -0.134      -0.126\n",
      "b_40            0.0007      0.002      0.466      0.641      -0.002       0.004\n",
      "s_27           -0.0291      0.001    -19.758      0.000      -0.032      -0.026\n",
      "d_113          -0.0103      0.002     -4.311      0.000      -0.015      -0.006\n",
      "d_114          -0.1337      0.005    -28.297      0.000      -0.143      -0.124\n",
      "d_116           0.0812      0.019      4.320      0.000       0.044       0.118\n",
      "d_117          -0.0255      0.001    -29.338      0.000      -0.027      -0.024\n",
      "d_120           0.0193      0.005      3.719      0.000       0.009       0.030\n",
      "d_121           0.1716      0.003     53.424      0.000       0.165       0.178\n",
      "d_122          -0.0132      0.003     -5.068      0.000      -0.018      -0.008\n",
      "d_123           0.0176      0.002      7.117      0.000       0.013       0.022\n",
      "d_124           0.0143      0.002      6.048      0.000       0.010       0.019\n",
      "d_125           0.0137      0.002      5.863      0.000       0.009       0.018\n",
      "d_126          -0.0157      0.003     -4.866      0.000      -0.022      -0.009\n",
      "d_127          -0.1077      0.004    -25.093      0.000      -0.116      -0.099\n",
      "d_128          -0.0329      0.003    -11.794      0.000      -0.038      -0.027\n",
      "d_129          -0.1408      0.003    -50.597      0.000      -0.146      -0.135\n",
      "b_41            0.0333      0.002     18.168      0.000       0.030       0.037\n",
      "d_130           0.0241      0.002     11.073      0.000       0.020       0.028\n",
      "d_131           0.1915      0.004     54.421      0.000       0.185       0.198\n",
      "d_133          -0.1173      0.002    -54.619      0.000      -0.122      -0.113\n",
      "r_28           -0.0040      0.001     -3.088      0.002      -0.007      -0.001\n",
      "d_140           0.0275      0.002     17.367      0.000       0.024       0.031\n",
      "d_144          -0.0094      0.002     -4.503      0.000      -0.014      -0.005\n",
      "d_145           0.0094      0.002      4.928      0.000       0.006       0.013\n",
      "b_1_pca_1      -0.1234      0.001    -85.891      0.000      -0.126      -0.121\n",
      "b_2_pca_1       0.0916      0.002     54.356      0.000       0.088       0.095\n",
      "b_7_pca_1      -0.0769      0.002    -42.715      0.000      -0.080      -0.073\n",
      "r_5_pca_1       0.0392      0.003     13.732      0.000       0.034       0.045\n",
      "d_58_pca_1      0.1041      0.002     63.452      0.000       0.101       0.107\n",
      "b_14_pca_1      0.0492      0.002     23.441      0.000       0.045       0.053\n",
      "s_22_pca_1     -0.0154      0.002     -8.048      0.000      -0.019      -0.012\n",
      "d_103_pca_1     0.0301      0.002     19.677      0.000       0.027       0.033\n",
      "d_118_pca_1    -0.0628      0.002    -35.092      0.000      -0.066      -0.059\n",
      "d_135_pca_1     0.0180      0.001     23.914      0.000       0.016       0.019\n",
      "d_139_pca_1    -0.0242      0.002    -15.787      0.000      -0.027      -0.021\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression model\n",
    "Xdf_SG = df_pca.drop(columns=['target'])\n",
    "ydf_SG = df_pca['target']\n",
    "# Split the data using stratify method, to avoid only one class data seep in train\n",
    "train_XSG, val_XSG, train_ySG, val_ySG = train_test_split(Xdf_SG,ydf_SG,\\\n",
    "                    test_size=0.3,random_state=rand_state,stratify = ydf_SG)\n",
    "\n",
    "# Model\n",
    "regression1_SG = sm.Logit(train_ySG,train_XSG).fit()\n",
    "print(regression1_SG.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1150677,   95398],\n",
       "       [ 123017,  290344]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "prediction_probab = regression1_SG.predict(val_XSG)\n",
    "prediction = list(map(round,prediction_probab))\n",
    "confusion_matrix(val_ySG,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic : ROC AUC = 0.931\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate roc metric \n",
    "regression1_SG_auc = roc_auc_score(val_ySG,prediction_probab)\n",
    "print('Logistic : ROC AUC = %.3f' % (regression1_SG_auc))\n",
    "\n",
    "regression1_SG_fpr,regression1_SG_tpr,_ = roc_curve(val_ySG,prediction_probab)\n",
    "plt.plot(regression1_SG_fpr,regression1_SG_tpr,marker = '.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91   1246075\n",
      "           1       0.75      0.70      0.73    413361\n",
      "\n",
      "    accuracy                           0.87   1659436\n",
      "   macro avg       0.83      0.81      0.82   1659436\n",
      "weighted avg       0.87      0.87      0.87   1659436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Recall and Precision\n",
    "print(classification_report(val_ySG, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is: 0.868973131560699\n",
      "Validation accuracy is: 0.8683799797039476\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score to find underfitting & overfitting\n",
    "pred_prob_score = regression1_SG.predict(train_XSG)\n",
    "pred = list(map(float,list(map(round,pred_prob_score))))\n",
    "\n",
    "print(f'Training accuracy is: {accuracy_score(train_ySG, pred )}')\n",
    "print(f'Validation accuracy is: {accuracy_score(val_ySG, prediction)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.285966\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:              3872015\n",
      "Model:                          Logit   Df Residuals:                  3871890\n",
      "Method:                           MLE   Df Model:                          124\n",
      "Date:                Tue, 13 Dec 2022   Pseudo R-squ.:                  0.4906\n",
      "Time:                        23:14:27   Log-Likelihood:            -1.1073e+06\n",
      "converged:                       True   LL-Null:                   -2.1735e+06\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "p_2            -0.8065      0.004   -218.531      0.000      -0.814      -0.799\n",
      "d_39            0.1110      0.002     56.431      0.000       0.107       0.115\n",
      "r_1             0.1171      0.003     34.836      0.000       0.110       0.124\n",
      "s_3             0.1026      0.004     28.140      0.000       0.095       0.110\n",
      "d_41            0.0927      0.003     35.145      0.000       0.088       0.098\n",
      "d_43            0.0987      0.002     40.471      0.000       0.094       0.103\n",
      "d_45           -0.1078      0.004    -28.534      0.000      -0.115      -0.100\n",
      "b_5            -0.0560      0.004    -14.452      0.000      -0.064      -0.048\n",
      "r_2             0.0292      0.003     10.423      0.000       0.024       0.035\n",
      "d_46            0.1440      0.002     73.627      0.000       0.140       0.148\n",
      "d_47           -0.1517      0.003    -50.346      0.000      -0.158      -0.146\n",
      "d_48            0.0697      0.003     21.910      0.000       0.063       0.076\n",
      "d_49            0.0525      0.002     23.821      0.000       0.048       0.057\n",
      "b_6            -0.1383      0.011    -12.720      0.000      -0.160      -0.117\n",
      "b_8             0.1991      0.003     67.660      0.000       0.193       0.205\n",
      "d_51           -0.2551      0.004    -69.213      0.000      -0.262      -0.248\n",
      "b_9             0.0551      0.002     26.042      0.000       0.051       0.059\n",
      "r_3             0.1782      0.002     76.434      0.000       0.174       0.183\n",
      "d_52           -0.0450      0.002    -21.019      0.000      -0.049      -0.041\n",
      "p_3             0.0614      0.002     31.105      0.000       0.058       0.065\n",
      "s_5             0.0263      0.002     14.310      0.000       0.023       0.030\n",
      "s_6             0.0173      0.002      7.081      0.000       0.013       0.022\n",
      "d_54           -0.0421      0.002    -23.704      0.000      -0.046      -0.039\n",
      "s_7             0.0754      0.004     20.480      0.000       0.068       0.083\n",
      "b_12           -0.1131      0.009    -13.264      0.000      -0.130      -0.096\n",
      "s_8            -0.1005      0.004    -24.820      0.000      -0.108      -0.093\n",
      "b_13            0.0662      0.005     12.979      0.000       0.056       0.076\n",
      "d_59           -0.0490      0.002    -26.038      0.000      -0.053      -0.045\n",
      "d_60            0.0725      0.003     25.454      0.000       0.067       0.078\n",
      "d_61            0.0860      0.005     17.786      0.000       0.077       0.095\n",
      "s_11           -0.1074      0.002    -52.001      0.000      -0.111      -0.103\n",
      "d_62           -0.2187      0.004    -57.843      0.000      -0.226      -0.211\n",
      "d_64            0.0160      0.002     10.170      0.000       0.013       0.019\n",
      "d_65            0.0318      0.003     12.165      0.000       0.027       0.037\n",
      "d_66           -0.1645      0.003    -54.019      0.000      -0.170      -0.159\n",
      "d_68           -0.0249      0.002    -15.778      0.000      -0.028      -0.022\n",
      "s_12            0.0440      0.002     23.895      0.000       0.040       0.048\n",
      "s_13            0.0779      0.003     24.806      0.000       0.072       0.084\n",
      "b_21            0.0175      0.003      6.513      0.000       0.012       0.023\n",
      "b_22            0.0382      0.003     14.971      0.000       0.033       0.043\n",
      "d_70            0.0466      0.002     24.593      0.000       0.043       0.050\n",
      "d_71           -0.0277      0.006     -4.453      0.000      -0.040      -0.015\n",
      "d_72           -0.0097      0.002     -4.762      0.000      -0.014      -0.006\n",
      "s_15            0.0232      0.003      8.368      0.000       0.018       0.029\n",
      "p_4             0.1215      0.002     61.256      0.000       0.118       0.125\n",
      "b_24           -0.0119      0.003     -4.142      0.000      -0.018      -0.006\n",
      "b_26            0.0084      0.002      4.425      0.000       0.005       0.012\n",
      "d_78           -0.0252      0.002    -12.700      0.000      -0.029      -0.021\n",
      "d_79           -0.0148      0.003     -4.995      0.000      -0.021      -0.009\n",
      "r_9             0.0030      0.002      1.925      0.054   -5.48e-05       0.006\n",
      "s_16            0.0090      0.002      4.200      0.000       0.005       0.013\n",
      "d_80            0.0440      0.002     24.330      0.000       0.040       0.047\n",
      "r_10           -0.0086      0.002     -3.661      0.000      -0.013      -0.004\n",
      "r_11            0.0607      0.002     38.525      0.000       0.058       0.064\n",
      "d_82            0.0949      0.002     40.302      0.000       0.090       0.100\n",
      "s_17            0.0127      0.002      8.384      0.000       0.010       0.016\n",
      "r_12           -0.0277      0.002    -12.865      0.000      -0.032      -0.023\n",
      "d_83            0.0081      0.002      5.094      0.000       0.005       0.011\n",
      "r_14            0.0150      0.002      9.304      0.000       0.012       0.018\n",
      "d_84           -0.0142      0.003     -4.656      0.000      -0.020      -0.008\n",
      "r_16           -0.0294      0.002    -14.653      0.000      -0.033      -0.025\n",
      "b_30            0.0300      0.006      5.052      0.000       0.018       0.042\n",
      "s_18            0.0407      0.002     21.558      0.000       0.037       0.044\n",
      "d_86           -0.0544      0.002    -22.374      0.000      -0.059      -0.050\n",
      "d_87           -0.0079      0.002     -4.544      0.000      -0.011      -0.004\n",
      "r_17            0.0054      0.002      2.343      0.019       0.001       0.010\n",
      "r_18           -0.0033      0.001     -2.344      0.019      -0.006      -0.001\n",
      "b_31           -2.0602      0.011   -195.982      0.000      -2.081      -2.040\n",
      "s_19            0.0082      0.002      4.924      0.000       0.005       0.011\n",
      "r_19           -0.0381      0.002    -25.131      0.000      -0.041      -0.035\n",
      "b_32           -0.0238      0.001    -16.689      0.000      -0.027      -0.021\n",
      "s_20            0.0235      0.002     10.092      0.000       0.019       0.028\n",
      "r_21            0.0274      0.003     10.474      0.000       0.022       0.033\n",
      "d_89           -0.0403      0.002    -18.022      0.000      -0.045      -0.036\n",
      "r_22           -0.0150      0.001    -11.290      0.000      -0.018      -0.012\n",
      "r_23            0.0130      0.001      8.972      0.000       0.010       0.016\n",
      "d_91           -0.0526      0.003    -17.141      0.000      -0.059      -0.047\n",
      "d_93            0.0243      0.002     10.141      0.000       0.020       0.029\n",
      "d_94           -0.0230      0.005     -4.941      0.000      -0.032      -0.014\n",
      "r_24           -0.0146      0.002     -6.452      0.000      -0.019      -0.010\n",
      "r_25           -0.0394      0.002    -18.784      0.000      -0.044      -0.035\n",
      "d_96           -0.0328      0.002    -13.799      0.000      -0.037      -0.028\n",
      "s_23            0.0336      0.005      7.396      0.000       0.025       0.042\n",
      "s_26           -0.0239      0.004     -6.431      0.000      -0.031      -0.017\n",
      "d_102          -0.0493      0.002    -19.948      0.000      -0.054      -0.044\n",
      "b_36            0.0110      0.003      4.376      0.000       0.006       0.016\n",
      "r_26            0.0189      0.002      8.701      0.000       0.015       0.023\n",
      "r_27           -0.1036      0.002    -60.455      0.000      -0.107      -0.100\n",
      "d_108           0.0182      0.002     11.868      0.000       0.015       0.021\n",
      "d_111           0.0687      0.003     27.091      0.000       0.064       0.074\n",
      "d_112          -0.1295      0.002    -61.023      0.000      -0.134      -0.125\n",
      "s_27           -0.0290      0.001    -19.736      0.000      -0.032      -0.026\n",
      "d_113          -0.0104      0.002     -4.354      0.000      -0.015      -0.006\n",
      "d_114          -0.1337      0.005    -28.296      0.000      -0.143      -0.124\n",
      "d_116           0.0801      0.019      4.261      0.000       0.043       0.117\n",
      "d_117          -0.0254      0.001    -29.321      0.000      -0.027      -0.024\n",
      "d_120           0.0193      0.005      3.701      0.000       0.009       0.029\n",
      "d_121           0.1717      0.003     53.469      0.000       0.165       0.178\n",
      "d_122          -0.0132      0.003     -5.075      0.000      -0.018      -0.008\n",
      "d_123           0.0175      0.002      7.110      0.000       0.013       0.022\n",
      "d_124           0.0144      0.002      6.110      0.000       0.010       0.019\n",
      "d_125           0.0136      0.002      5.858      0.000       0.009       0.018\n",
      "d_126          -0.0158      0.003     -4.897      0.000      -0.022      -0.009\n",
      "d_127          -0.1079      0.004    -25.184      0.000      -0.116      -0.099\n",
      "d_128          -0.0327      0.003    -11.707      0.000      -0.038      -0.027\n",
      "d_129          -0.1405      0.003    -50.542      0.000      -0.146      -0.135\n",
      "b_41            0.0333      0.002     18.188      0.000       0.030       0.037\n",
      "d_130           0.0240      0.002     11.055      0.000       0.020       0.028\n",
      "d_131           0.1917      0.004     54.525      0.000       0.185       0.199\n",
      "d_133          -0.1176      0.002    -54.971      0.000      -0.122      -0.113\n",
      "r_28           -0.0040      0.001     -3.086      0.002      -0.006      -0.001\n",
      "d_140           0.0275      0.002     17.369      0.000       0.024       0.031\n",
      "d_144          -0.0093      0.002     -4.452      0.000      -0.013      -0.005\n",
      "d_145           0.0093      0.002      4.917      0.000       0.006       0.013\n",
      "b_1_pca_1      -0.1234      0.001    -90.883      0.000      -0.126      -0.121\n",
      "b_2_pca_1       0.0906      0.002     57.506      0.000       0.088       0.094\n",
      "b_7_pca_1      -0.0771      0.002    -42.874      0.000      -0.081      -0.074\n",
      "r_5_pca_1       0.0416      0.003     16.031      0.000       0.037       0.047\n",
      "d_58_pca_1      0.1038      0.002     63.412      0.000       0.101       0.107\n",
      "b_14_pca_1      0.0499      0.002     24.069      0.000       0.046       0.054\n",
      "s_22_pca_1     -0.0153      0.002     -8.025      0.000      -0.019      -0.012\n",
      "d_103_pca_1     0.0301      0.002     19.693      0.000       0.027       0.033\n",
      "d_118_pca_1    -0.0628      0.002    -35.101      0.000      -0.066      -0.059\n",
      "d_135_pca_1     0.0179      0.001     23.896      0.000       0.016       0.019\n",
      "d_139_pca_1    -0.0242      0.002    -15.833      0.000      -0.027      -0.021\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Remove the insignificant features and train the model again. I will keep the alpha level as 0.07\n",
    "logit_pvalues = round(regression1_SG.pvalues,3)\n",
    "high_pval_col = logit_pvalues.index[logit_pvalues > 0.05]\n",
    "\n",
    "# Drop these columns\n",
    "Xdf_SG_new = Xdf_SG.drop(columns=high_pval_col).copy()\n",
    "\n",
    "# Split the data using stratify method, to avoid only one class data seep in train\n",
    "train_XSG_new, val_XSG_new, train_ySG_new, val_ySG_new = train_test_split(Xdf_SG_new, ydf_SG,\\\n",
    "                                     test_size=0.3, random_state=rand_state, stratify = ydf_SG)\n",
    "\n",
    "# Model\n",
    "regression2_SG = sm.Logit(train_ySG_new,train_XSG_new).fit()\n",
    "print(regression2_SG.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.8683715431025963\n",
      "Logistic : ROC AUC = 0.931\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After removing the insignificant variables, the AUC score and validation accuracy is still maintained\n",
    "\n",
    "prediction_probab_new = regression2_SG.predict(val_XSG_new)\n",
    "prediction_new = list(map(round,prediction_probab_new))\n",
    "\n",
    "print(f'Validation accuracy is: {accuracy_score(val_ySG_new, prediction_new)}')\n",
    "\n",
    "# Calculate roc metric \n",
    "regression2_SG_auc = roc_auc_score(val_ySG_new,prediction_probab_new)\n",
    "print('Logistic : ROC AUC = %.3f' % (regression2_SG_auc))\n",
    "\n",
    "regression2_SG_fpr,regression2_SG_tpr,_ = roc_curve(val_ySG_new,prediction_probab_new)\n",
    "plt.plot(regression2_SG_fpr,regression2_SG_tpr,marker = '.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=500, n_jobs=12, random_state=2903)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500, n_jobs=12, random_state=2903)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=500, n_jobs=12, random_state=2903)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn Logistic regression aftr removing insignificant variables\n",
    "log_reg = LogisticRegression(random_state=rand_state,  max_iter=500, n_jobs=12) \n",
    "log_reg.fit(train_XSG_new,train_ySG_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8685234019269198\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_reg.predict(val_XSG_new)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(val_ySG_new,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3872015, 125)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_XSG_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(df):\n",
    "    # Just add extra columns with 0 value so that pipeline does not fail --> these are the extra columns that we had in the training data\n",
    "    extra_cols = ['target', 'last_statement_flag', 'last_statement_target']\n",
    "    # Concatenate the dataframe of extra columns with the dataframe of the test data\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.DataFrame(np.zeros((df.shape[0], len(extra_cols))), columns=extra_cols)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Use the pipeline to transform\n",
    "    X = pipeline.transform(df)\n",
    "\n",
    "    # Drop target & the insignificant variables found during the training using statsmodel p-value\n",
    "    X.drop(columns=['target'] + high_pval_col.tolist(), inplace=True)\n",
    "\n",
    "    # return log_reg.predict(X), log_reg.predict_proba(X)\n",
    "    # In the statsmodel predict will give the probability\n",
    "    return list(map(round,regression2_SG.predict(X))), regression2_SG.predict(X).tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet('D:/Sakshi/DSBA_6156_SERJ/data/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns= df_test.columns.str.lower()\n",
    "# Define the result mdf\n",
    "mdf = pd.DataFrame(columns=['customer_id', 's_2', 'pred', 'proba'])\n",
    "y, y_proba = execute_model(df_test)\n",
    "\n",
    "mdf = pd.concat([\n",
    "    mdf,\n",
    "    pd.DataFrame({\n",
    "        'customer_id': df_test['customer_id'].values,\n",
    "        's_2': df_test['s_2'].values,\n",
    "        'pred': y,\n",
    "        'proba': y_proba\n",
    "    })\n",
    "]) \n",
    "# mdf.to_csv('../ignore/final/logistic_regression_prediction_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf['s_2'] = pd.to_datetime(mdf['s_2'])\n",
    "mdf['s_2'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just take the last statement probability of each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last statement probability of each of the customer\n",
    "df_result_last = mdf.sort_values(by = 's_2').groupby('customer_id')[['customer_id','proba']].tail(1)\n",
    "df_result_last.rename(columns= {'proba' : 'prediction'},inplace=True)\n",
    "df_result_last.head()\n",
    "df_result_last.to_csv('D:/Sakshi/DSBA_6156_SERJ/ignore/ppt_analysis/lgpca_last_stmt.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89a73c21ecc9236fdbb84984cd9e615404f96fb7d0e8948f841b3ff5dee670ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
