{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the customer has defaulted assigning all statements the value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data \n",
    "df_train_x = pd.read_parquet('D:/Sakshi/DSBA_6156_SERJ/data/train.parquet')\n",
    "df_train_x.columns = df_train_x.columns.str.lower()\n",
    "# Read training data labels\n",
    "df_train_y = pd.read_csv('D:/Sakshi/DSBA_6156_SERJ/data/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "df_train = pd.merge(df_train_x, df_train_y, on='customer_id')\n",
    "del(df_train_x, df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>s_2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-04-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-09-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-10-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-12-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           customer_id         s_2  target\n",
       "104  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-03-15       1\n",
       "105  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-04-14       1\n",
       "106  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-05-15       1\n",
       "107  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-06-14       1\n",
       "108  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-07-15       1\n",
       "109  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-08-15       1\n",
       "110  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-09-14       1\n",
       "111  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-10-14       1\n",
       "112  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-11-14       1\n",
       "113  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-12-17       1\n",
       "114  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2018-01-17       1\n",
       "115  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2018-02-05       1\n",
       "116  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2018-03-01       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['customer_id']== '0000f99513770170a1aba690daeeb8a96da4a39f11fc27da5c30a79db61c1e85'][['customer_id','s_2','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Before doing any transformation see the datatypes of the features\n",
    "# df_train.dtypes.to_csv('../ignore/final/before_transformations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_63     0.0\n",
       "d_64     0.0\n",
       "d_66     0.0\n",
       "d_68     0.0\n",
       "b_30     0.0\n",
       "b_31     0.0\n",
       "b_38     0.0\n",
       "d_114    0.0\n",
       "d_116    0.0\n",
       "d_117    0.0\n",
       "d_120    0.0\n",
       "d_126    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[])\n",
    "# 89% of d_66 column values were missing, but it has been filled with -1 while parquet generation.\n",
    "# Also assign the column names in sequence in which it appars in the file  \n",
    "categorical_cols = ['d_63', 'd_64', 'd_66', 'd_68', 'b_30', 'b_31', 'b_38', 'd_114', 'd_116',\n",
    "                     'd_117', 'd_120', 'd_126']\n",
    "df_train[categorical_cols].isnull().sum() / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the percentage of missing values\n",
    "# null_series = df_train.isna().sum() / df_train.shape[0]\n",
    "# null_series.to_csv('../ignore/final/column_null_values_prop.csv')\n",
    "# del null_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values for column d_63 is [0 3 4 1 2 5]\n",
      "The unique values for column d_64 is [ 0  2 -1  3  1]\n",
      "The unique values for column d_66 is [-1  1  0]\n",
      "The unique values for column d_68 is [ 6  2  3 -1  5  4  0  1]\n",
      "The unique values for column b_30 is [ 0  2  1 -1]\n",
      "The unique values for column b_31 is [1 0]\n",
      "The unique values for column b_38 is [ 2  1  3  5  6  7  4 -1]\n",
      "The unique values for column d_114 is [ 1  0 -1]\n",
      "The unique values for column d_116 is [ 0 -1  1]\n",
      "The unique values for column d_117 is [ 5  0  7  3  2 -1  4  6]\n",
      "The unique values for column d_120 is [ 0  1 -1]\n",
      "The unique values for column d_126 is [ 2 -1  1  0]\n"
     ]
    }
   ],
   "source": [
    "# Check for the unique values for all the categorical features\n",
    "for i in categorical_cols:\n",
    "    print(f'The unique values for column {i} is {df_train[i].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the original file is:(5531451, 191)\n"
     ]
    }
   ],
   "source": [
    "print(f'The shape of the original file is:{df_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_cols):\n",
    "        self.categorical_cols = categorical_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Get the list of columns that have missing values greater than equal to 40%\n",
    "        missing_perc = round((X.isnull().sum() / len(X)) * 100, 2)\n",
    "        # Prepare final List of columns to drop\n",
    "        self.cols_to_drop = missing_perc[missing_perc.ge(40)].index.tolist()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        numeric_cols = list(set(X.columns.tolist(\n",
    "        )) - set(self.categorical_cols + self.cols_to_drop + ['target', 'customer_id', 's_2']))\n",
    "\n",
    "        # Impute the mean of the numeric columns\n",
    "        for col in numeric_cols:\n",
    "            # Check if the column has any null value, then only apply the imputation\n",
    "            if X[col].isnull().any():\n",
    "                X[col] = X[col].fillna(X[col].mean())\n",
    "                            \n",
    "            # Scale\n",
    "            mean = X[col].mean()\n",
    "            std = X[col].std()\n",
    "            if std > 0:\n",
    "                X[col] = ((X[col] - mean) / std).astype('float32')\n",
    "\n",
    "        X = X.drop(columns = self.cols_to_drop)\n",
    "\n",
    "        return X\n",
    "\n",
    "# use all the statements of a customer where all stmts are marked with the same target value\n",
    "preprocessing = PreProcessing(categorical_cols)\n",
    "df_processed = preprocessing.fit_transform(df_train)\n",
    "\n",
    "pipeline.steps.append(('preprocessing', preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing the shape is :(5531451, 173)\n"
     ]
    }
   ],
   "source": [
    "del (df_train)\n",
    "print(f'After processing the shape is :{df_processed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation for the numeric columns\n",
    "df_corr = df_processed.drop(columns = categorical_cols + ['target','customer_id','s_2']).corr()\n",
    "df_corr.to_csv(\"../ignore/final/num_corr_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def sklearn_vif(data):\n",
    "\n",
    "    # initialize dictionaries\n",
    "    result = {}\n",
    "\n",
    "    # form input data for each exogenous variable\n",
    "    exogs = data.columns.to_list()\n",
    "    for exog in exogs:\n",
    "        # print(exog)\n",
    "        not_exog = [i for i in exogs if i != exog]\n",
    "        # exog would be for which the VIF has to be calculated based on the combination of other columns\n",
    "        X, y = data[not_exog], data[exog]  \n",
    "        # extract r-squared from the fit\n",
    "        r_squared = LinearRegression(n_jobs=12).fit(X, y).score(X, y)\n",
    "\n",
    "        # calculate VIF\n",
    "        vif = 1/(1 - r_squared)\n",
    "        result[exog] = vif\n",
    "    return result\n",
    "\n",
    "vif_data1 = sklearn_vif(df_processed.drop(columns = categorical_cols + ['target','customer_id','s_2']))\n",
    "# Convert the results from the dictionary to dataframe\n",
    "df_vif = pd.DataFrame({\n",
    "    'feature': vif_data1.keys(),\n",
    "    'VIF': vif_data1.values()\n",
    "})\n",
    "del(vif_data1)\n",
    "df_vif.to_csv(\"../ignore/final/num_VIF_data_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid the re-running of the VIF code above\n",
    "df_vif = pd.read_csv(\"D:/Sakshi/DSBA_6156_SERJ/ignore/final/num_VIF_data_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The remaining columns of the data after dropping columns with high VIF : 143\n"
     ]
    }
   ],
   "source": [
    "# Plainly drop all the columns with higher VIF values\n",
    "df_processed.drop(columns = df_vif[df_vif['VIF']> 10]['feature'].to_list(), inplace=True)\n",
    "print(f'The remaining columns of the data after dropping columns with high VIF : {df_processed.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_processed.drop(columns=['target','customer_id','s_2']), df_processed['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, random_state=2303, stratify = y)\n",
    "# del(df_processed, X, y)                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.285647\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:              4425160\n",
      "Model:                          Logit   Df Residuals:                  4425020\n",
      "Method:                           MLE   Df Model:                          139\n",
      "Date:                Tue, 13 Dec 2022   Pseudo R-squ.:                  0.4911\n",
      "Time:                        22:09:00   Log-Likelihood:            -1.2640e+06\n",
      "converged:                       True   LL-Null:                   -2.4840e+06\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "p_2           -0.8056      0.003   -230.915      0.000      -0.812      -0.799\n",
      "d_39           0.1245      0.002     67.614      0.000       0.121       0.128\n",
      "r_1            0.1073      0.004     25.118      0.000       0.099       0.116\n",
      "s_3            0.1184      0.003     34.756      0.000       0.112       0.125\n",
      "d_41           0.0994      0.002     39.938      0.000       0.094       0.104\n",
      "b_3            0.2512      0.003     95.758      0.000       0.246       0.256\n",
      "d_43           0.1019      0.002     44.769      0.000       0.097       0.106\n",
      "d_44           0.0085      0.002      3.661      0.000       0.004       0.013\n",
      "b_4            0.3381      0.002    150.661      0.000       0.334       0.342\n",
      "d_45          -0.1147      0.004    -32.146      0.000      -0.122      -0.108\n",
      "b_5           -0.0875      0.004    -20.882      0.000      -0.096      -0.079\n",
      "r_2            0.0281      0.003      9.035      0.000       0.022       0.034\n",
      "d_46           0.1433      0.002     76.010      0.000       0.140       0.147\n",
      "d_47          -0.1669      0.003    -59.104      0.000      -0.172      -0.161\n",
      "d_48           0.0894      0.004     25.306      0.000       0.083       0.096\n",
      "d_49           0.0578      0.002     26.904      0.000       0.054       0.062\n",
      "b_6           -0.2654      0.012    -21.599      0.000      -0.289      -0.241\n",
      "b_8            0.1768      0.003     63.938      0.000       0.171       0.182\n",
      "d_51          -0.2510      0.004    -66.033      0.000      -0.258      -0.244\n",
      "b_9            0.1085      0.002     51.519      0.000       0.104       0.113\n",
      "r_3            0.1828      0.002     83.805      0.000       0.178       0.187\n",
      "d_52          -0.0545      0.002    -27.242      0.000      -0.058      -0.051\n",
      "p_3            0.0553      0.002     29.505      0.000       0.052       0.059\n",
      "b_10          -0.0066      0.004     -1.747      0.081      -0.014       0.001\n",
      "s_5            0.0317      0.002     17.499      0.000       0.028       0.035\n",
      "s_6            0.0304      0.002     13.364      0.000       0.026       0.035\n",
      "d_54          -0.0402      0.002    -24.142      0.000      -0.043      -0.037\n",
      "r_4            0.0319      0.003      9.968      0.000       0.026       0.038\n",
      "s_7            0.0610      0.003     17.706      0.000       0.054       0.068\n",
      "b_12          -0.1131      0.008    -13.687      0.000      -0.129      -0.097\n",
      "s_8           -0.0905      0.004    -23.977      0.000      -0.098      -0.083\n",
      "d_55          -0.0208      0.004     -5.930      0.000      -0.028      -0.014\n",
      "b_13           0.0991      0.006     17.793      0.000       0.088       0.110\n",
      "d_59          -0.0458      0.002    -26.005      0.000      -0.049      -0.042\n",
      "d_60           0.0910      0.003     36.173      0.000       0.086       0.096\n",
      "d_61           0.1021      0.005     22.393      0.000       0.093       0.111\n",
      "s_11          -0.1030      0.002    -53.308      0.000      -0.107      -0.099\n",
      "d_62          -0.2791      0.004    -77.595      0.000      -0.286      -0.272\n",
      "d_63          -0.0225      0.002    -11.842      0.000      -0.026      -0.019\n",
      "d_64           0.0185      0.001     12.504      0.000       0.016       0.021\n",
      "d_65           0.0388      0.002     15.959      0.000       0.034       0.044\n",
      "b_16           0.2607      0.004     66.778      0.000       0.253       0.268\n",
      "b_19           0.0716      0.003     28.434      0.000       0.067       0.077\n",
      "d_66          -0.1736      0.003    -61.134      0.000      -0.179      -0.168\n",
      "b_20          -0.1814      0.003    -53.084      0.000      -0.188      -0.175\n",
      "d_68          -0.0332      0.001    -22.304      0.000      -0.036      -0.030\n",
      "s_12           0.0397      0.002     23.068      0.000       0.036       0.043\n",
      "r_6            0.0116      0.002      5.582      0.000       0.008       0.016\n",
      "s_13           0.0724      0.003     24.533      0.000       0.067       0.078\n",
      "b_21           0.0271      0.002     10.891      0.000       0.022       0.032\n",
      "d_69           0.0009      0.001      0.960      0.337      -0.001       0.003\n",
      "b_22           0.0469      0.002     18.954      0.000       0.042       0.052\n",
      "d_70           0.0656      0.002     40.041      0.000       0.062       0.069\n",
      "d_71          -0.0557      0.006     -8.653      0.000      -0.068      -0.043\n",
      "d_72          -0.0150      0.002     -7.342      0.000      -0.019      -0.011\n",
      "s_15           0.0272      0.003     10.467      0.000       0.022       0.032\n",
      "p_4            0.1539      0.002     77.693      0.000       0.150       0.158\n",
      "b_24          -0.0078      0.003     -2.881      0.004      -0.013      -0.003\n",
      "r_7            0.0023      0.002      1.328      0.184      -0.001       0.006\n",
      "b_26           0.0141      0.002      7.133      0.000       0.010       0.018\n",
      "d_78          -0.0187      0.002     -9.826      0.000      -0.022      -0.015\n",
      "d_79          -0.0138      0.003     -5.037      0.000      -0.019      -0.008\n",
      "r_9            0.0052      0.001      3.526      0.000       0.002       0.008\n",
      "s_16           0.0031      0.002      1.636      0.102      -0.001       0.007\n",
      "d_80           0.0379      0.002     22.315      0.000       0.035       0.041\n",
      "r_10          -0.0032      0.003     -1.244      0.214      -0.008       0.002\n",
      "r_11           0.0614      0.001     41.712      0.000       0.059       0.064\n",
      "b_27           0.0020      0.002      1.234      0.217      -0.001       0.005\n",
      "d_81          -0.0014      0.002     -0.634      0.526      -0.006       0.003\n",
      "d_82           0.0988      0.003     39.105      0.000       0.094       0.104\n",
      "s_17           0.0119      0.001      8.396      0.000       0.009       0.015\n",
      "r_12          -0.0221      0.002    -10.961      0.000      -0.026      -0.018\n",
      "d_83           0.0108      0.001      7.221      0.000       0.008       0.014\n",
      "r_14           0.0128      0.002      7.969      0.000       0.010       0.016\n",
      "r_15           0.0013      0.002      0.632      0.527      -0.003       0.005\n",
      "d_84           0.0069      0.003      2.252      0.024       0.001       0.013\n",
      "r_16          -0.0313      0.002    -16.728      0.000      -0.035      -0.028\n",
      "b_30           0.0554      0.006      9.553      0.000       0.044       0.067\n",
      "s_18           0.0342      0.002     19.285      0.000       0.031       0.038\n",
      "d_86          -0.0576      0.002    -25.269      0.000      -0.062      -0.053\n",
      "d_87          -0.0058      0.002     -3.523      0.000      -0.009      -0.003\n",
      "r_18          -0.0043      0.001     -3.342      0.001      -0.007      -0.002\n",
      "b_31          -2.0373      0.012   -175.744      0.000      -2.060      -2.015\n",
      "s_19           0.0081      0.002      5.231      0.000       0.005       0.011\n",
      "r_19          -0.0372      0.001    -25.195      0.000      -0.040      -0.034\n",
      "b_32          -0.0267      0.001    -19.266      0.000      -0.029      -0.024\n",
      "s_20           0.0196      0.002      8.964      0.000       0.015       0.024\n",
      "r_20          -0.0089      0.003     -3.377      0.001      -0.014      -0.004\n",
      "r_21           0.0324      0.003     12.537      0.000       0.027       0.037\n",
      "d_89          -0.0398      0.002    -18.351      0.000      -0.044      -0.036\n",
      "r_22          -0.0147      0.001    -11.648      0.000      -0.017      -0.012\n",
      "r_23           0.0138      0.001     10.315      0.000       0.011       0.016\n",
      "d_91          -0.0572      0.003    -19.596      0.000      -0.063      -0.051\n",
      "d_92          -0.0060      0.004     -1.585      0.113      -0.013       0.001\n",
      "d_93           0.0211      0.002      9.282      0.000       0.017       0.026\n",
      "d_94          -0.0161      0.004     -3.630      0.000      -0.025      -0.007\n",
      "r_24          -0.0124      0.003     -4.550      0.000      -0.018      -0.007\n",
      "r_25          -0.0364      0.002    -18.351      0.000      -0.040      -0.033\n",
      "d_96          -0.0275      0.002    -12.405      0.000      -0.032      -0.023\n",
      "s_23           0.0069      0.003      2.368      0.018       0.001       0.013\n",
      "s_25           0.0010      0.001      0.733      0.464      -0.002       0.004\n",
      "s_26          -0.0194      0.003     -6.172      0.000      -0.026      -0.013\n",
      "d_102         -0.0250      0.002    -12.300      0.000      -0.029      -0.021\n",
      "d_106          0.0011      0.003      0.385      0.700      -0.005       0.007\n",
      "d_107          0.0341      0.002     16.845      0.000       0.030       0.038\n",
      "b_36           0.0118      0.002      5.017      0.000       0.007       0.016\n",
      "r_26           0.0196      0.002      9.861      0.000       0.016       0.023\n",
      "r_27          -0.1027      0.002    -64.361      0.000      -0.106      -0.100\n",
      "b_38           0.0129      0.001      8.843      0.000       0.010       0.016\n",
      "d_108          0.0192      0.001     13.528      0.000       0.016       0.022\n",
      "d_109         -0.0127      0.002     -5.359      0.000      -0.017      -0.008\n",
      "d_111          0.0635      0.002     26.759      0.000       0.059       0.068\n",
      "d_112         -0.1295      0.002    -64.922      0.000      -0.133      -0.126\n",
      "b_40           0.0021      0.002      1.061      0.289      -0.002       0.006\n",
      "s_27          -0.0233      0.001    -16.821      0.000      -0.026      -0.021\n",
      "d_113         -0.0154      0.002     -6.893      0.000      -0.020      -0.011\n",
      "d_114         -0.1310      0.004    -29.581      0.000      -0.140      -0.122\n",
      "d_115         -0.0526      0.003    -19.196      0.000      -0.058      -0.047\n",
      "d_116          0.0245      0.018      1.390      0.165      -0.010       0.059\n",
      "d_117         -0.0217      0.001    -26.864      0.000      -0.023      -0.020\n",
      "d_120          0.0279      0.005      5.754      0.000       0.018       0.037\n",
      "d_121          0.1639      0.003     55.543      0.000       0.158       0.170\n",
      "d_122         -0.0161      0.002     -6.477      0.000      -0.021      -0.011\n",
      "d_123          0.0148      0.002      6.493      0.000       0.010       0.019\n",
      "d_124          0.0399      0.002     18.239      0.000       0.036       0.044\n",
      "d_125          0.0117      0.002      5.373      0.000       0.007       0.016\n",
      "d_126         -0.0142      0.003     -4.707      0.000      -0.020      -0.008\n",
      "d_127         -0.1099      0.004    -27.488      0.000      -0.118      -0.102\n",
      "d_128         -0.0186      0.003     -7.132      0.000      -0.024      -0.013\n",
      "d_129         -0.1354      0.003    -52.418      0.000      -0.140      -0.130\n",
      "b_41           0.0295      0.002     17.131      0.000       0.026       0.033\n",
      "d_130          0.0271      0.002     13.279      0.000       0.023       0.031\n",
      "d_131          0.1843      0.003     56.205      0.000       0.178       0.191\n",
      "d_133         -0.1106      0.002    -54.884      0.000      -0.115      -0.107\n",
      "r_28           0.0035      0.001      2.908      0.004       0.001       0.006\n",
      "d_136         -0.0028      0.002     -1.331      0.183      -0.007       0.001\n",
      "d_138          0.0358      0.002     17.232      0.000       0.032       0.040\n",
      "d_140          0.0172      0.001     12.552      0.000       0.014       0.020\n",
      "d_144         -0.0259      0.002    -15.991      0.000      -0.029      -0.023\n",
      "d_145         -0.0071      0.001     -5.029      0.000      -0.010      -0.004\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "sm_logit1 = sm.Logit(y_train,X_train).fit()\n",
    "print(sm_logit1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns remaining after removing insignificant ones : (5531451, 126)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.285650\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:              4425160\n",
      "Model:                          Logit   Df Residuals:                  4425034\n",
      "Method:                           MLE   Df Model:                          125\n",
      "Date:                Tue, 13 Dec 2022   Pseudo R-squ.:                  0.4911\n",
      "Time:                        22:12:17   Log-Likelihood:            -1.2640e+06\n",
      "converged:                       True   LL-Null:                   -2.4840e+06\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "p_2           -0.8049      0.003   -233.100      0.000      -0.812      -0.798\n",
      "d_39           0.1245      0.002     67.613      0.000       0.121       0.128\n",
      "r_1            0.1049      0.002     43.304      0.000       0.100       0.110\n",
      "s_3            0.1186      0.003     34.821      0.000       0.112       0.125\n",
      "d_41           0.0993      0.002     39.965      0.000       0.094       0.104\n",
      "b_3            0.2511      0.003     95.794      0.000       0.246       0.256\n",
      "d_43           0.1017      0.002     44.720      0.000       0.097       0.106\n",
      "d_44           0.0079      0.002      3.428      0.001       0.003       0.012\n",
      "b_4            0.3383      0.002    150.882      0.000       0.334       0.343\n",
      "d_45          -0.1145      0.004    -32.110      0.000      -0.121      -0.108\n",
      "b_5           -0.0879      0.004    -21.010      0.000      -0.096      -0.080\n",
      "r_2            0.0284      0.003      9.153      0.000       0.022       0.034\n",
      "d_46           0.1434      0.002     76.418      0.000       0.140       0.147\n",
      "d_47          -0.1667      0.003    -59.158      0.000      -0.172      -0.161\n",
      "d_48           0.0893      0.004     25.280      0.000       0.082       0.096\n",
      "d_49           0.0580      0.002     27.969      0.000       0.054       0.062\n",
      "b_6           -0.2684      0.012    -21.900      0.000      -0.292      -0.244\n",
      "b_8            0.1768      0.003     63.923      0.000       0.171       0.182\n",
      "d_51          -0.2539      0.003    -73.971      0.000      -0.261      -0.247\n",
      "b_9            0.1083      0.002     51.696      0.000       0.104       0.112\n",
      "r_3            0.1826      0.002     83.956      0.000       0.178       0.187\n",
      "d_52          -0.0547      0.002    -27.360      0.000      -0.059      -0.051\n",
      "p_3            0.0547      0.002     29.627      0.000       0.051       0.058\n",
      "s_5            0.0318      0.002     17.556      0.000       0.028       0.035\n",
      "s_6            0.0306      0.002     13.555      0.000       0.026       0.035\n",
      "d_54          -0.0403      0.002    -24.215      0.000      -0.044      -0.037\n",
      "r_4            0.0330      0.003     11.153      0.000       0.027       0.039\n",
      "s_7            0.0609      0.003     17.704      0.000       0.054       0.068\n",
      "b_12          -0.1131      0.008    -13.668      0.000      -0.129      -0.097\n",
      "s_8           -0.0905      0.004    -23.994      0.000      -0.098      -0.083\n",
      "d_55          -0.0206      0.004     -5.883      0.000      -0.028      -0.014\n",
      "b_13           0.0986      0.006     17.706      0.000       0.088       0.109\n",
      "d_59          -0.0457      0.002    -25.979      0.000      -0.049      -0.042\n",
      "d_60           0.0909      0.003     36.150      0.000       0.086       0.096\n",
      "d_61           0.1020      0.005     22.367      0.000       0.093       0.111\n",
      "s_11          -0.1029      0.002    -53.275      0.000      -0.107      -0.099\n",
      "d_62          -0.2790      0.004    -77.627      0.000      -0.286      -0.272\n",
      "d_63          -0.0226      0.002    -11.890      0.000      -0.026      -0.019\n",
      "d_64           0.0188      0.001     12.978      0.000       0.016       0.022\n",
      "d_65           0.0389      0.002     15.981      0.000       0.034       0.044\n",
      "b_16           0.2616      0.004     67.546      0.000       0.254       0.269\n",
      "b_19           0.0713      0.003     28.400      0.000       0.066       0.076\n",
      "d_66          -0.1739      0.003    -61.246      0.000      -0.179      -0.168\n",
      "b_20          -0.1818      0.003    -53.300      0.000      -0.188      -0.175\n",
      "d_68          -0.0325      0.001    -23.334      0.000      -0.035      -0.030\n",
      "s_12           0.0397      0.002     23.052      0.000       0.036       0.043\n",
      "r_6            0.0130      0.002      7.431      0.000       0.010       0.016\n",
      "s_13           0.0723      0.003     24.545      0.000       0.067       0.078\n",
      "b_21           0.0290      0.002     12.857      0.000       0.025       0.033\n",
      "b_22           0.0471      0.002     19.073      0.000       0.042       0.052\n",
      "d_70           0.0655      0.002     40.020      0.000       0.062       0.069\n",
      "d_71          -0.0561      0.006     -8.717      0.000      -0.069      -0.043\n",
      "d_72          -0.0155      0.002     -8.220      0.000      -0.019      -0.012\n",
      "s_15           0.0272      0.003     10.463      0.000       0.022       0.032\n",
      "p_4            0.1539      0.002     78.382      0.000       0.150       0.158\n",
      "b_24          -0.0080      0.003     -2.933      0.003      -0.013      -0.003\n",
      "b_26           0.0143      0.002      7.270      0.000       0.010       0.018\n",
      "d_78          -0.0194      0.002    -10.347      0.000      -0.023      -0.016\n",
      "d_79          -0.0136      0.003     -4.970      0.000      -0.019      -0.008\n",
      "r_9            0.0053      0.001      3.644      0.000       0.002       0.008\n",
      "d_80           0.0381      0.002     22.398      0.000       0.035       0.041\n",
      "r_11           0.0615      0.001     41.803      0.000       0.059       0.064\n",
      "d_82           0.0988      0.003     39.113      0.000       0.094       0.104\n",
      "s_17           0.0119      0.001      8.430      0.000       0.009       0.015\n",
      "r_12          -0.0221      0.002    -10.925      0.000      -0.026      -0.018\n",
      "d_83           0.0110      0.001      7.457      0.000       0.008       0.014\n",
      "r_14           0.0123      0.002      7.883      0.000       0.009       0.015\n",
      "d_84           0.0068      0.003      2.246      0.025       0.001       0.013\n",
      "r_16          -0.0311      0.002    -16.675      0.000      -0.035      -0.027\n",
      "b_30           0.0554      0.006      9.569      0.000       0.044       0.067\n",
      "s_18           0.0341      0.002     19.266      0.000       0.031       0.038\n",
      "d_86          -0.0577      0.002    -25.294      0.000      -0.062      -0.053\n",
      "d_87          -0.0058      0.002     -3.558      0.000      -0.009      -0.003\n",
      "r_18          -0.0042      0.001     -3.304      0.001      -0.007      -0.002\n",
      "b_31          -2.0441      0.010   -200.578      0.000      -2.064      -2.024\n",
      "s_19           0.0083      0.002      5.356      0.000       0.005       0.011\n",
      "r_19          -0.0368      0.001    -26.597      0.000      -0.040      -0.034\n",
      "b_32          -0.0267      0.001    -19.295      0.000      -0.029      -0.024\n",
      "s_20           0.0194      0.002      8.893      0.000       0.015       0.024\n",
      "r_20          -0.0092      0.003     -3.459      0.001      -0.014      -0.004\n",
      "r_21           0.0337      0.002     14.284      0.000       0.029       0.038\n",
      "d_89          -0.0401      0.002    -19.865      0.000      -0.044      -0.036\n",
      "r_22          -0.0146      0.001    -11.845      0.000      -0.017      -0.012\n",
      "r_23           0.0139      0.001     10.372      0.000       0.011       0.016\n",
      "d_91          -0.0557      0.003    -19.651      0.000      -0.061      -0.050\n",
      "d_93           0.0216      0.002      9.511      0.000       0.017       0.026\n",
      "d_94          -0.0161      0.004     -3.612      0.000      -0.025      -0.007\n",
      "r_24          -0.0121      0.003     -4.736      0.000      -0.017      -0.007\n",
      "r_25          -0.0364      0.002    -18.457      0.000      -0.040      -0.032\n",
      "d_96          -0.0275      0.002    -12.402      0.000      -0.032      -0.023\n",
      "s_23           0.0068      0.003      2.352      0.019       0.001       0.012\n",
      "s_26          -0.0194      0.003     -6.156      0.000      -0.026      -0.013\n",
      "d_102         -0.0249      0.002    -12.284      0.000      -0.029      -0.021\n",
      "d_107          0.0339      0.002     16.788      0.000       0.030       0.038\n",
      "b_36           0.0118      0.002      5.014      0.000       0.007       0.016\n",
      "r_26           0.0195      0.002      9.823      0.000       0.016       0.023\n",
      "r_27          -0.1027      0.002    -64.347      0.000      -0.106      -0.100\n",
      "b_38           0.0129      0.001      8.861      0.000       0.010       0.016\n",
      "d_108          0.0192      0.001     13.506      0.000       0.016       0.022\n",
      "d_109         -0.0127      0.002     -5.341      0.000      -0.017      -0.008\n",
      "d_111          0.0634      0.002     26.750      0.000       0.059       0.068\n",
      "d_112         -0.1294      0.002    -64.989      0.000      -0.133      -0.126\n",
      "s_27          -0.0233      0.001    -16.803      0.000      -0.026      -0.021\n",
      "d_113         -0.0150      0.002     -6.795      0.000      -0.019      -0.011\n",
      "d_114         -0.1292      0.004    -30.360      0.000      -0.138      -0.121\n",
      "d_115         -0.0527      0.003    -19.246      0.000      -0.058      -0.047\n",
      "d_117         -0.0216      0.001    -26.794      0.000      -0.023      -0.020\n",
      "d_120          0.0296      0.005      6.364      0.000       0.020       0.039\n",
      "d_121          0.1630      0.003     56.955      0.000       0.157       0.169\n",
      "d_122         -0.0157      0.002     -6.377      0.000      -0.020      -0.011\n",
      "d_123          0.0156      0.002      6.961      0.000       0.011       0.020\n",
      "d_124          0.0402      0.002     18.474      0.000       0.036       0.044\n",
      "d_125          0.0120      0.002      5.573      0.000       0.008       0.016\n",
      "d_126         -0.0133      0.003     -4.534      0.000      -0.019      -0.008\n",
      "d_127         -0.1103      0.004    -27.632      0.000      -0.118      -0.102\n",
      "d_128         -0.0190      0.003     -7.341      0.000      -0.024      -0.014\n",
      "d_129         -0.1353      0.003    -52.426      0.000      -0.140      -0.130\n",
      "b_41           0.0295      0.002     17.166      0.000       0.026       0.033\n",
      "d_130          0.0269      0.002     13.231      0.000       0.023       0.031\n",
      "d_131          0.1842      0.003     56.197      0.000       0.178       0.191\n",
      "d_133         -0.1105      0.002    -55.166      0.000      -0.114      -0.107\n",
      "r_28           0.0036      0.001      2.971      0.003       0.001       0.006\n",
      "d_138          0.0337      0.001     25.043      0.000       0.031       0.036\n",
      "d_140          0.0172      0.001     12.560      0.000       0.014       0.020\n",
      "d_144         -0.0260      0.002    -16.024      0.000      -0.029      -0.023\n",
      "d_145         -0.0072      0.001     -5.096      0.000      -0.010      -0.004\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "del(df_processed)\n",
    "# Remove the insignificant features and train the model again. I will keep the alpha level as 0.05\n",
    "logit_pvalues = round(sm_logit1.pvalues,3)\n",
    "high_pval_col = logit_pvalues.index[logit_pvalues > 0.05]\n",
    "\n",
    "# Drop these columns\n",
    "X = X.drop(columns = high_pval_col)\n",
    "print(f'The columns remaining after removing insignificant ones : {X.shape}')\n",
    "X_train, X_test,y_train, y_test= train_test_split(X, y, test_size=0.2,\n",
    "                                                     random_state=2303, stratify = y)\n",
    "\n",
    "# Model\n",
    "sm_logit2 = sm.Logit(y_train,X_train).fit()\n",
    "print(sm_logit2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[768007,  62710],\n",
       "       [ 81825, 193749]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "prediction_probab = sm_logit2.predict(X_test)\n",
    "prediction = list(map(round,prediction_probab))\n",
    "confusion_matrix(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.8693517347605648\n",
      "Logistic : ROC AUC = 0.931\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Validation accuracy is: {accuracy_score(y_test, prediction)}')\n",
    "\n",
    "# Calculate roc metric \n",
    "print('Logistic : ROC AUC = %.3f' % (roc_auc_score(y_test,prediction_probab)))\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_test,prediction_probab)\n",
    "plt.plot(fpr,tpr,marker = '.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the memory before loading the test data to predict\n",
    "del(fpr,tpr,X_test,X_train,y_test,y_train,X,y,prediction,prediction_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(df):\n",
    "    # Just add extra columns with 0 value so that pipeline does not fail --> these are the extra columns that we had in the training data\n",
    "    extra_cols = ['target']\n",
    "    # Concatenate the dataframe of extra columns with the dataframe of the test data\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.DataFrame(np.zeros((df.shape[0], len(extra_cols))), columns=extra_cols)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Use the pipeline to transform\n",
    "    X = pipeline.transform(df)\n",
    "\n",
    "    # Drop target & the insignificant variables found during the training using statsmodel p-value\n",
    "    X.drop(columns=['target','customer_id','s_2'] + df_vif[df_vif['VIF']> 10]['feature'].to_list()\n",
    "                    + high_pval_col.tolist(), inplace=True)\n",
    "\n",
    "    # return log_reg.predict(X), log_reg.predict_proba(X)\n",
    "    # In the statsmodel predict will give the probability\n",
    "    return list(map(round,sm_logit2.predict(X))), sm_logit2.predict(X).tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(sm_logit1)\n",
    "df_test = pd.read_parquet('D:/Sakshi/DSBA_6156_SERJ/data/test.parquet')\n",
    "df_test.columns= df_test.columns.str.lower()\n",
    "# Define the result mdf\n",
    "mdf = pd.DataFrame(columns=['customer_id', 's_2', 'pred', 'proba'])\n",
    "y, y_proba = execute_model(df_test)\n",
    "\n",
    "mdf = pd.concat([\n",
    "    mdf,\n",
    "    pd.DataFrame({\n",
    "        'customer_id': df_test['customer_id'].values,\n",
    "        's_2': df_test['s_2'].values,\n",
    "        'pred': y,\n",
    "        'proba': y_proba\n",
    "    })\n",
    "]) \n",
    "# mdf.to_csv('../ignore/final/logisticregression_baseline_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf['s_2'] = pd.to_datetime(mdf['s_2'])\n",
    "mdf['s_2'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just take the last statement probability of each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last statement probability of each of the customer\n",
    "df_result_last = mdf.sort_values(by = 's_2').groupby('customer_id')[['customer_id','proba']].tail(1)\n",
    "df_result_last.rename(columns= {'proba' : 'prediction'},inplace=True)\n",
    "df_result_last.head()\n",
    "df_result_last.to_csv('D:/Sakshi/DSBA_6156_SERJ/ignore/ppt_analysis/logistic_baseline_laststmt_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted probabilities using Joe's Code (test data) for the last 3 stmts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the outcome weighting.\n",
    "\n",
    "def conditions(x):\n",
    "    # Customer has 3 statements:\n",
    "    if   x == 3:   return 0.1\n",
    "    elif x == 6:   return 0.15\n",
    "    elif x == 9:   return 0.75\n",
    "    \n",
    "    # Customer has 2 statements:\n",
    "    elif x == 2:   return 0.2\n",
    "    elif x == 4:   return 0.8\n",
    "    \n",
    "    # Customer has 1 statement:\n",
    "    elif x == 1:   return 1.0 \n",
    "    else:          return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the last 3 statements of each customer\n",
    "mdf1 = mdf.sort_values('s_2').groupby('customer_id').tail(3)\n",
    "# if the customer has last 3 stmts the ranking will be as - 1st to the older stmt and 3rd rank to the latest stmt. \n",
    "mdf1[\"statement_num\"] = mdf1.groupby(\"customer_id\")[\"s_2\"].rank(method=\"first\", ascending=True)\n",
    "# The statement_count variable will give the count of the statements for each customer (i.e. to know if they have all the 3 or less than that)\n",
    "mdf1['statement_count'] = mdf1.groupby('customer_id')['statement_num'].transform('max')\n",
    "\n",
    "# Create a number so we can handle the case where a customer had only 1 or 2 statements. \n",
    "# Multiplied to give me a unique value for each case. See conditions() above.\n",
    "mdf1['statement_checksum'] = (mdf1['statement_count']) * mdf1['statement_num']\n",
    "\n",
    "# Assign the weights to the statements\n",
    "mdf1['statement_weight'] = mdf1['statement_checksum'].apply(conditions)\n",
    "\n",
    "# Calculating the weighted sum\n",
    "mdf1 ['prediction'] = mdf1['proba'] * mdf1['statement_weight']\n",
    "\n",
    "mdf1 = mdf1[['customer_id', 'prediction']]\n",
    "\n",
    "# Grouping those weighted sums by customer_id to give granularity of 1 proba per customer\n",
    "mdf1 = mdf1.groupby('customer_id').sum()\n",
    "# Bring the customer_id from index to column\n",
    "mdf1.reset_index(inplace=True)\n",
    "# Send the data to the file\n",
    "mdf1.to_csv('D:/Sakshi/DSBA_6156_SERJ/ignore/ppt_analysis/logistic_baseline_weighted_last3_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89a73c21ecc9236fdbb84984cd9e615404f96fb7d0e8948f841b3ff5dee670ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
