{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the customer has defaulted assigning last statement target value to 1 & others to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data \n",
    "df_train_x = pd.read_parquet('D:/Sakshi/DSBA_6156_SERJ/data/train.parquet')\n",
    "df_train_x.columns = df_train_x.columns.str.lower()\n",
    "# Read training data labels\n",
    "df_train_y = pd.read_csv('D:/Sakshi/DSBA_6156_SERJ/data/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "df_train = pd.merge(df_train_x, df_train_y, on='customer_id')\n",
    "del(df_train_x, df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before doing any transformation see the datatypes of the features\n",
    "df_train.dtypes.to_csv('../ignore/final/before_transformations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['last_statement_flag'] = (df_train.groupby('customer_id')['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(int)\n",
    "                   )\n",
    "df_train['last_statement_target'] = df_train['target']*df_train['last_statement_flag']\\\n",
    "   .apply(lambda x: 1 if x==1 else 0)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>s_2</th>\n",
       "      <th>target</th>\n",
       "      <th>last_statement_flag</th>\n",
       "      <th>last_statement_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-04-14</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-09-14</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-10-14</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2017-12-17</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-01-17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0000f99513770170a1aba690daeeb8a96da4a39f11fc27...</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           customer_id         s_2  target  \\\n",
       "104  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-03-15       1   \n",
       "105  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-04-14       1   \n",
       "106  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-05-15       1   \n",
       "107  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-06-14       1   \n",
       "108  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-07-15       1   \n",
       "109  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-08-15       1   \n",
       "110  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-09-14       1   \n",
       "111  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-10-14       1   \n",
       "112  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-11-14       1   \n",
       "113  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2017-12-17       1   \n",
       "114  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2018-01-17       1   \n",
       "115  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2018-02-05       1   \n",
       "116  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...  2018-03-01       1   \n",
       "\n",
       "     last_statement_flag  last_statement_target  \n",
       "104                   13                      0  \n",
       "105                   12                      0  \n",
       "106                   11                      0  \n",
       "107                   10                      0  \n",
       "108                    9                      0  \n",
       "109                    8                      0  \n",
       "110                    7                      0  \n",
       "111                    6                      0  \n",
       "112                    5                      0  \n",
       "113                    4                      0  \n",
       "114                    3                      0  \n",
       "115                    2                      0  \n",
       "116                    1                      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['customer_id']== '0000f99513770170a1aba690daeeb8a96da4a39f11fc27da5c30a79db61c1e85']\\\n",
    "    [['customer_id','s_2','target','last_statement_flag','last_statement_target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_63     0.0\n",
       "d_64     0.0\n",
       "d_66     0.0\n",
       "d_68     0.0\n",
       "b_30     0.0\n",
       "b_31     0.0\n",
       "b_38     0.0\n",
       "d_114    0.0\n",
       "d_116    0.0\n",
       "d_117    0.0\n",
       "d_120    0.0\n",
       "d_126    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[])\n",
    "# 89% of d_66 column values were missing, but it has been filled with -1 while parquet generation.\n",
    "# Also assign the column names in sequence in which it appars in the file  \n",
    "categorical_cols = ['d_63', 'd_64', 'd_66', 'd_68', 'b_30', 'b_31', 'b_38', 'd_114', 'd_116',\n",
    "                     'd_117', 'd_120', 'd_126']\n",
    "df_train[categorical_cols].isnull().sum() / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the percentage of missing values\n",
    "# null_series = df_train.isna().sum() / df_train.shape[0]\n",
    "# null_series.to_csv('../ignore/final/column_null_values_prop.csv')\n",
    "# del null_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values for column d_63 is [0 3 4 1 2 5]\n",
      "The unique values for column d_64 is [ 0  2 -1  3  1]\n",
      "The unique values for column d_66 is [-1  1  0]\n",
      "The unique values for column d_68 is [ 6  2  3 -1  5  4  0  1]\n",
      "The unique values for column b_30 is [ 0  2  1 -1]\n",
      "The unique values for column b_31 is [1 0]\n",
      "The unique values for column b_38 is [ 2  1  3  5  6  7  4 -1]\n",
      "The unique values for column d_114 is [ 1  0 -1]\n",
      "The unique values for column d_116 is [ 0 -1  1]\n",
      "The unique values for column d_117 is [ 5  0  7  3  2 -1  4  6]\n",
      "The unique values for column d_120 is [ 0  1 -1]\n",
      "The unique values for column d_126 is [ 2 -1  1  0]\n"
     ]
    }
   ],
   "source": [
    "# Check for the unique values for all the categorical features\n",
    "for i in categorical_cols:\n",
    "    print(f'The unique values for column {i} is {df_train[i].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the original file is:(5531451, 193)\n"
     ]
    }
   ],
   "source": [
    "print(f'The shape of the original file is:{df_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_cols):\n",
    "        self.categorical_cols = categorical_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Get the list of columns that have missing values greater than equal to 40%\n",
    "        missing_perc = round((X.isnull().sum() / len(X)) * 100, 2)\n",
    "        # Prepare final List of columns to drop\n",
    "        self.cols_to_drop = missing_perc[missing_perc.ge(40)].index.tolist() + ['last_statement_flag' , 'target']\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        numeric_cols = list(set(X.columns.tolist(\n",
    "        )) - set(self.categorical_cols + self.cols_to_drop + ['last_statement_target', 'customer_id', 's_2']))\n",
    "\n",
    "        # Impute the mean of the numeric columns\n",
    "        for col in numeric_cols:\n",
    "            # Check if the column has any null value, then only apply the imputation\n",
    "            if X[col].isnull().any():\n",
    "                X[col] = X[col].fillna(X[col].mean())\n",
    "                            \n",
    "            # Scale\n",
    "            mean = X[col].mean()\n",
    "            std = X[col].std()\n",
    "            if std > 0:\n",
    "                X[col] = ((X[col] - mean) / std).astype('float32')\n",
    "\n",
    "        X = X.drop(columns = self.cols_to_drop)\n",
    "\n",
    "        return X\n",
    "\n",
    "# use all the statements of a customer where all stmts are marked with the same target value\n",
    "preprocessing = PreProcessing(categorical_cols)\n",
    "df_processed = preprocessing.fit_transform(df_train)\n",
    "\n",
    "pipeline.steps.append(('preprocessing', preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing the shape is :(5531451, 173)\n"
     ]
    }
   ],
   "source": [
    "del (df_train)\n",
    "print(f'After processing the shape is :{df_processed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation for the numeric columns\n",
    "df_corr = df_processed.drop(columns = categorical_cols + ['target','customer_id','s_2']).corr()\n",
    "df_corr.to_csv(\"../ignore/final/num_corr_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def sklearn_vif(data):\n",
    "\n",
    "    # initialize dictionaries\n",
    "    result = {}\n",
    "\n",
    "    # form input data for each exogenous variable\n",
    "    exogs = data.columns.to_list()\n",
    "    for exog in exogs:\n",
    "        # print(exog)\n",
    "        not_exog = [i for i in exogs if i != exog]\n",
    "        # exog would be for which the VIF has to be calculated based on the combination of other columns\n",
    "        X, y = data[not_exog], data[exog]  \n",
    "        # extract r-squared from the fit\n",
    "        r_squared = LinearRegression(n_jobs=12).fit(X, y).score(X, y)\n",
    "\n",
    "        # calculate VIF\n",
    "        vif = 1/(1 - r_squared)\n",
    "        result[exog] = vif\n",
    "    return result\n",
    "\n",
    "vif_data1 = sklearn_vif(df_processed.drop(columns = categorical_cols + ['target','customer_id','s_2']))\n",
    "# Convert the results from the dictionary to dataframe\n",
    "df_vif = pd.DataFrame({\n",
    "    'feature': vif_data1.keys(),\n",
    "    'VIF': vif_data1.values()\n",
    "})\n",
    "del(vif_data1)\n",
    "df_vif.to_csv(\"../ignore/final/num_VIF_data_results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vif = pd.read_csv(\"D:/Sakshi/DSBA_6156_SERJ/ignore/final/num_VIF_data_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The remaining columns of the data after dropping columns with high VIF : 143\n"
     ]
    }
   ],
   "source": [
    "# Plainly drop all the columns with higher VIF values\n",
    "df_processed.drop(columns = df_vif[df_vif['VIF']> 10]['feature'].to_list(), inplace=True)\n",
    "print(f'The remaining columns of the data after dropping columns with high VIF : {df_processed.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_processed.drop(columns=['last_statement_target','customer_id','s_2']),\\\n",
    "                             df_processed['last_statement_target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, random_state=2303, stratify = y)                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078139\n",
      "         Iterations 12\n",
      "                             Logit Regression Results                            \n",
      "=================================================================================\n",
      "Dep. Variable:     last_statement_target   No. Observations:              4425160\n",
      "Model:                             Logit   Df Residuals:                  4425020\n",
      "Method:                              MLE   Df Model:                          139\n",
      "Date:                   Tue, 13 Dec 2022   Pseudo R-squ.:                  0.2469\n",
      "Time:                           22:25:57   Log-Likelihood:            -3.4578e+05\n",
      "converged:                          True   LL-Null:                   -4.5912e+05\n",
      "Covariance Type:               nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "p_2           -0.3997      0.008    -52.626      0.000      -0.415      -0.385\n",
      "d_39           0.1666      0.003     65.930      0.000       0.162       0.172\n",
      "r_1           -0.1191      0.006    -19.463      0.000      -0.131      -0.107\n",
      "s_3            0.1256      0.007     18.494      0.000       0.112       0.139\n",
      "d_41           0.0161      0.003      5.492      0.000       0.010       0.022\n",
      "b_3            0.1757      0.005     36.312      0.000       0.166       0.185\n",
      "d_43           0.0318      0.003      9.113      0.000       0.025       0.039\n",
      "d_44           0.0133      0.004      3.381      0.001       0.006       0.021\n",
      "b_4            0.1728      0.004     46.753      0.000       0.166       0.180\n",
      "d_45           0.0857      0.009      9.669      0.000       0.068       0.103\n",
      "b_5           -0.1064      0.016     -6.739      0.000      -0.137      -0.075\n",
      "r_2            0.0484      0.004     13.178      0.000       0.041       0.056\n",
      "d_46           0.0670      0.004     19.113      0.000       0.060       0.074\n",
      "d_47          -0.1620      0.006    -25.102      0.000      -0.175      -0.149\n",
      "d_48           0.0891      0.007     12.281      0.000       0.075       0.103\n",
      "d_49           0.0231      0.003      6.891      0.000       0.017       0.030\n",
      "b_6            0.0014      0.007      0.202      0.840      -0.012       0.015\n",
      "b_8            0.1550      0.007     23.673      0.000       0.142       0.168\n",
      "d_51           0.0438      0.010      4.606      0.000       0.025       0.062\n",
      "b_9            0.0552      0.003     16.971      0.000       0.049       0.062\n",
      "r_3           -0.0202      0.004     -5.445      0.000      -0.027      -0.013\n",
      "d_52           0.0065      0.005      1.424      0.154      -0.002       0.015\n",
      "p_3           -0.0134      0.003     -3.864      0.000      -0.020      -0.007\n",
      "b_10          -0.4324      0.079     -5.501      0.000      -0.587      -0.278\n",
      "s_5            0.0180      0.003      6.151      0.000       0.012       0.024\n",
      "s_6            0.0576      0.005     11.008      0.000       0.047       0.068\n",
      "d_54          -0.0542      0.002    -22.375      0.000      -0.059      -0.049\n",
      "r_4            0.0610      0.004     16.418      0.000       0.054       0.068\n",
      "s_7           -0.1123      0.007    -15.252      0.000      -0.127      -0.098\n",
      "b_12          -0.0450      0.010     -4.446      0.000      -0.065      -0.025\n",
      "s_8           -0.1366      0.008    -17.139      0.000      -0.152      -0.121\n",
      "d_55           0.0889      0.008     11.770      0.000       0.074       0.104\n",
      "b_13           0.0803      0.009      8.943      0.000       0.063       0.098\n",
      "d_59           0.3235      0.003     95.325      0.000       0.317       0.330\n",
      "d_60           0.0903      0.006     14.311      0.000       0.078       0.103\n",
      "d_61           0.0483      0.006      7.867      0.000       0.036       0.060\n",
      "s_11           0.0881      0.004     21.865      0.000       0.080       0.096\n",
      "d_62          -0.1176      0.009    -12.494      0.000      -0.136      -0.099\n",
      "d_63          -0.1605      0.004    -36.583      0.000      -0.169      -0.152\n",
      "d_64          -0.0267      0.003     -8.457      0.000      -0.033      -0.021\n",
      "d_65           0.0182      0.002      9.466      0.000       0.014       0.022\n",
      "b_16           0.2913      0.009     31.587      0.000       0.273       0.309\n",
      "b_19           0.0584      0.005     12.012      0.000       0.049       0.068\n",
      "d_66           0.0152      0.006      2.497      0.013       0.003       0.027\n",
      "b_20          -0.0739      0.008     -9.608      0.000      -0.089      -0.059\n",
      "d_68          -0.1277      0.003    -43.636      0.000      -0.133      -0.122\n",
      "s_12           0.0214      0.002      8.803      0.000       0.017       0.026\n",
      "r_6           -0.0029      0.003     -1.140      0.254      -0.008       0.002\n",
      "s_13           0.0535      0.007      8.182      0.000       0.041       0.066\n",
      "b_21           0.0027      0.002      1.183      0.237      -0.002       0.007\n",
      "d_69           0.0032      0.002      1.506      0.132      -0.001       0.007\n",
      "b_22           0.0289      0.005      6.304      0.000       0.020       0.038\n",
      "d_70           0.0074      0.003      2.694      0.007       0.002       0.013\n",
      "d_71          -0.1768      0.024     -7.386      0.000      -0.224      -0.130\n",
      "d_72          -0.0092      0.003     -3.044      0.002      -0.015      -0.003\n",
      "s_15          -0.0117      0.004     -2.629      0.009      -0.020      -0.003\n",
      "p_4            0.0626      0.004     14.804      0.000       0.054       0.071\n",
      "b_24          -0.0880      0.004    -23.534      0.000      -0.095      -0.081\n",
      "r_7            0.0042      0.003      1.603      0.109      -0.001       0.009\n",
      "b_26           0.0107      0.002      4.977      0.000       0.006       0.015\n",
      "d_78          -0.0207      0.002     -8.705      0.000      -0.025      -0.016\n",
      "d_79           0.0140      0.004      3.503      0.000       0.006       0.022\n",
      "r_9            0.0072      0.003      2.736      0.006       0.002       0.012\n",
      "s_16           0.0099      0.002      4.513      0.000       0.006       0.014\n",
      "d_80          -0.0028      0.004     -0.782      0.434      -0.010       0.004\n",
      "r_10           0.0550      0.003     20.238      0.000       0.050       0.060\n",
      "r_11           0.0305      0.003     10.342      0.000       0.025       0.036\n",
      "b_27           0.0185      0.003      5.645      0.000       0.012       0.025\n",
      "d_81          -0.0160      0.003     -5.338      0.000      -0.022      -0.010\n",
      "d_82          -0.1270      0.006    -21.668      0.000      -0.138      -0.115\n",
      "s_17           0.0247      0.003      9.417      0.000       0.020       0.030\n",
      "r_12           0.0597      0.003     18.136      0.000       0.053       0.066\n",
      "d_83           0.0085      0.003      3.265      0.001       0.003       0.014\n",
      "r_14           0.0111      0.002      6.265      0.000       0.008       0.015\n",
      "r_15           0.0337      0.003     12.542      0.000       0.028       0.039\n",
      "d_84          -0.0111      0.003     -3.591      0.000      -0.017      -0.005\n",
      "r_16           0.0301      0.003     10.301      0.000       0.024       0.036\n",
      "b_30          -0.0597      0.011     -5.347      0.000      -0.082      -0.038\n",
      "s_18          -0.3439      0.010    -35.397      0.000      -0.363      -0.325\n",
      "d_86          -0.0183      0.006     -3.090      0.002      -0.030      -0.007\n",
      "d_87          -0.0015      0.001     -1.001      0.317      -0.004       0.001\n",
      "r_18           0.0068      0.001      5.267      0.000       0.004       0.009\n",
      "b_31          -3.8040      0.022   -176.735      0.000      -3.846      -3.762\n",
      "s_19           0.0110      0.003      3.610      0.000       0.005       0.017\n",
      "r_19           0.0207      0.003      7.970      0.000       0.016       0.026\n",
      "b_32          -0.0028      0.003     -1.074      0.283      -0.008       0.002\n",
      "s_20          -0.0084      0.003     -2.534      0.011      -0.015      -0.002\n",
      "r_20           0.0122      0.002      4.978      0.000       0.007       0.017\n",
      "r_21           0.0503      0.004     13.317      0.000       0.043       0.058\n",
      "d_89          -0.0068      0.002     -2.895      0.004      -0.011      -0.002\n",
      "r_22           0.0100      0.002      5.061      0.000       0.006       0.014\n",
      "r_23          -0.0241      0.005     -5.307      0.000      -0.033      -0.015\n",
      "d_91          -0.3007      0.008    -38.221      0.000      -0.316      -0.285\n",
      "d_92          -0.0899      0.012     -7.688      0.000      -0.113      -0.067\n",
      "d_93          -0.0311      0.006     -5.234      0.000      -0.043      -0.019\n",
      "d_94           0.0438      0.014      3.190      0.001       0.017       0.071\n",
      "r_24           0.0166      0.003      5.451      0.000       0.011       0.023\n",
      "r_25          -0.0805      0.003    -30.212      0.000      -0.086      -0.075\n",
      "d_96           0.0179      0.006      3.127      0.002       0.007       0.029\n",
      "s_23           0.0020      0.001      1.475      0.140      -0.001       0.005\n",
      "s_25          -0.0036      0.002     -1.698      0.089      -0.008       0.001\n",
      "s_26          -0.0004      0.009     -0.048      0.962      -0.018       0.017\n",
      "d_102          0.0272      0.004      6.203      0.000       0.019       0.036\n",
      "d_106         -0.0034      0.003     -1.313      0.189      -0.008       0.002\n",
      "d_107         -0.0353      0.004     -8.284      0.000      -0.044      -0.027\n",
      "b_36           0.0056      0.004      1.262      0.207      -0.003       0.014\n",
      "r_26          -0.0340      0.003    -12.890      0.000      -0.039      -0.029\n",
      "r_27           0.0087      0.003      2.899      0.004       0.003       0.015\n",
      "b_38          -0.0531      0.003    -18.019      0.000      -0.059      -0.047\n",
      "d_108          0.0223      0.003      8.917      0.000       0.017       0.027\n",
      "d_109         -0.0059      0.008     -0.736      0.462      -0.022       0.010\n",
      "d_111          0.0314      0.004      7.577      0.000       0.023       0.040\n",
      "d_112         -0.1068      0.004    -29.503      0.000      -0.114      -0.100\n",
      "b_40          -0.0006      0.002     -0.242      0.809      -0.005       0.004\n",
      "s_27           0.0801      0.003     28.759      0.000       0.075       0.086\n",
      "d_113         -0.0888      0.005    -19.255      0.000      -0.098      -0.080\n",
      "d_114         -0.1461      0.009    -15.475      0.000      -0.165      -0.128\n",
      "d_115          0.0223      0.006      3.707      0.000       0.011       0.034\n",
      "d_116          1.1391      0.032     35.666      0.000       1.076       1.202\n",
      "d_117         -0.0225      0.002    -13.043      0.000      -0.026      -0.019\n",
      "d_120          0.8906      0.009    103.835      0.000       0.874       0.907\n",
      "d_121          0.2032      0.006     33.613      0.000       0.191       0.215\n",
      "d_122         -0.0674      0.006    -12.202      0.000      -0.078      -0.057\n",
      "d_123         -0.0446      0.005     -8.284      0.000      -0.055      -0.034\n",
      "d_124         -0.0084      0.005     -1.849      0.065      -0.017       0.001\n",
      "d_125          0.0036      0.005      0.759      0.448      -0.006       0.013\n",
      "d_126         -0.0053      0.007     -0.771      0.441      -0.019       0.008\n",
      "d_127         -0.1444      0.012    -11.819      0.000      -0.168      -0.120\n",
      "d_128         -0.0356      0.006     -5.827      0.000      -0.048      -0.024\n",
      "d_129          0.0202      0.006      3.211      0.001       0.008       0.033\n",
      "b_41           0.0497      0.002     21.469      0.000       0.045       0.054\n",
      "d_130          0.0233      0.004      5.275      0.000       0.015       0.032\n",
      "d_131          0.0877      0.006     15.444      0.000       0.077       0.099\n",
      "d_133         -0.0498      0.004    -13.693      0.000      -0.057      -0.043\n",
      "r_28          -0.0046      0.003     -1.817      0.069      -0.009       0.000\n",
      "d_136          0.0072      0.004      1.992      0.046       0.000       0.014\n",
      "d_138       5.267e-05      0.004      0.014      0.989      -0.007       0.007\n",
      "d_140          0.0217      0.003      8.178      0.000       0.016       0.027\n",
      "d_144         -0.0104      0.004     -2.842      0.004      -0.018      -0.003\n",
      "d_145         -0.0173      0.003     -6.132      0.000      -0.023      -0.012\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "sm_logit1 = sm.Logit(y_train,X_train).fit()\n",
    "print(sm_logit1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns remaining after removing insignificant ones : (5531451, 119)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078142\n",
      "         Iterations 13\n",
      "                             Logit Regression Results                            \n",
      "=================================================================================\n",
      "Dep. Variable:     last_statement_target   No. Observations:              4425160\n",
      "Model:                             Logit   Df Residuals:                  4425041\n",
      "Method:                              MLE   Df Model:                          118\n",
      "Date:                   Tue, 13 Dec 2022   Pseudo R-squ.:                  0.2468\n",
      "Time:                           22:29:22   Log-Likelihood:            -3.4579e+05\n",
      "converged:                          True   LL-Null:                   -4.5912e+05\n",
      "Covariance Type:               nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "p_2           -0.4004      0.008    -52.931      0.000      -0.415      -0.386\n",
      "d_39           0.1666      0.003     66.244      0.000       0.162       0.172\n",
      "r_1           -0.1200      0.006    -19.674      0.000      -0.132      -0.108\n",
      "s_3            0.1254      0.007     18.545      0.000       0.112       0.139\n",
      "d_41           0.0159      0.003      5.465      0.000       0.010       0.022\n",
      "b_3            0.1761      0.005     36.458      0.000       0.167       0.186\n",
      "d_43           0.0318      0.003      9.137      0.000       0.025       0.039\n",
      "d_44           0.0125      0.004      3.196      0.001       0.005       0.020\n",
      "b_4            0.1733      0.004     47.716      0.000       0.166       0.180\n",
      "d_45           0.0872      0.009      9.867      0.000       0.070       0.105\n",
      "b_5           -0.1073      0.016     -6.815      0.000      -0.138      -0.076\n",
      "r_2            0.0487      0.004     13.247      0.000       0.041       0.056\n",
      "d_46           0.0665      0.003     20.310      0.000       0.060       0.073\n",
      "d_47          -0.1610      0.006    -24.992      0.000      -0.174      -0.148\n",
      "d_48           0.0903      0.007     12.499      0.000       0.076       0.104\n",
      "d_49           0.0195      0.003      6.260      0.000       0.013       0.026\n",
      "b_8            0.1547      0.007     23.663      0.000       0.142       0.168\n",
      "d_51           0.0444      0.010      4.669      0.000       0.026       0.063\n",
      "b_9            0.0555      0.003     17.114      0.000       0.049       0.062\n",
      "r_3           -0.0204      0.004     -5.495      0.000      -0.028      -0.013\n",
      "p_3           -0.0134      0.003     -3.891      0.000      -0.020      -0.007\n",
      "b_10          -0.4299      0.078     -5.488      0.000      -0.583      -0.276\n",
      "s_5            0.0180      0.003      6.176      0.000       0.012       0.024\n",
      "s_6            0.0545      0.005     11.482      0.000       0.045       0.064\n",
      "d_54          -0.0541      0.002    -22.350      0.000      -0.059      -0.049\n",
      "r_4            0.0615      0.004     16.782      0.000       0.054       0.069\n",
      "s_7           -0.1124      0.007    -15.307      0.000      -0.127      -0.098\n",
      "b_12          -0.0445      0.010     -4.409      0.000      -0.064      -0.025\n",
      "s_8           -0.1367      0.008    -17.183      0.000      -0.152      -0.121\n",
      "d_55           0.0883      0.008     11.729      0.000       0.074       0.103\n",
      "b_13           0.0798      0.009      8.933      0.000       0.062       0.097\n",
      "d_59           0.3236      0.003     95.417      0.000       0.317       0.330\n",
      "d_60           0.0916      0.006     14.615      0.000       0.079       0.104\n",
      "d_61           0.0482      0.006      7.864      0.000       0.036       0.060\n",
      "s_11           0.0880      0.004     21.897      0.000       0.080       0.096\n",
      "d_62          -0.1143      0.009    -12.592      0.000      -0.132      -0.097\n",
      "d_63          -0.1605      0.004    -36.729      0.000      -0.169      -0.152\n",
      "d_64          -0.0271      0.003     -8.606      0.000      -0.033      -0.021\n",
      "d_65           0.0183      0.002      9.511      0.000       0.015       0.022\n",
      "b_16           0.2901      0.009     31.533      0.000       0.272       0.308\n",
      "b_19           0.0582      0.005     12.005      0.000       0.049       0.068\n",
      "d_66           0.0148      0.006      2.438      0.015       0.003       0.027\n",
      "b_20          -0.0734      0.008     -9.613      0.000      -0.088      -0.058\n",
      "d_68          -0.1282      0.003    -44.355      0.000      -0.134      -0.123\n",
      "s_12           0.0214      0.002      8.801      0.000       0.017       0.026\n",
      "s_13           0.0533      0.007      8.171      0.000       0.041       0.066\n",
      "b_22           0.0287      0.005      6.266      0.000       0.020       0.038\n",
      "d_70           0.0072      0.003      2.624      0.009       0.002       0.013\n",
      "d_71          -0.1769      0.024     -7.399      0.000      -0.224      -0.130\n",
      "d_72          -0.0093      0.003     -3.070      0.002      -0.015      -0.003\n",
      "s_15          -0.0115      0.004     -2.591      0.010      -0.020      -0.003\n",
      "p_4            0.0624      0.004     14.771      0.000       0.054       0.071\n",
      "b_24          -0.0880      0.004    -23.606      0.000      -0.095      -0.081\n",
      "b_26           0.0107      0.002      4.995      0.000       0.006       0.015\n",
      "d_78          -0.0209      0.002     -8.846      0.000      -0.026      -0.016\n",
      "d_79           0.0149      0.004      3.724      0.000       0.007       0.023\n",
      "r_9            0.0071      0.003      2.688      0.007       0.002       0.012\n",
      "s_16           0.0114      0.002      6.611      0.000       0.008       0.015\n",
      "r_10           0.0555      0.003     20.477      0.000       0.050       0.061\n",
      "r_11           0.0303      0.003     10.295      0.000       0.025       0.036\n",
      "b_27           0.0185      0.003      5.656      0.000       0.012       0.025\n",
      "d_81          -0.0160      0.003     -5.364      0.000      -0.022      -0.010\n",
      "d_82          -0.1272      0.006    -21.757      0.000      -0.139      -0.116\n",
      "s_17           0.0247      0.003      9.440      0.000       0.020       0.030\n",
      "r_12           0.0598      0.003     18.153      0.000       0.053       0.066\n",
      "d_83           0.0086      0.003      3.309      0.001       0.004       0.014\n",
      "r_14           0.0104      0.002      6.232      0.000       0.007       0.014\n",
      "r_15           0.0339      0.003     12.660      0.000       0.029       0.039\n",
      "d_84          -0.0111      0.003     -3.609      0.000      -0.017      -0.005\n",
      "r_16           0.0301      0.003     10.330      0.000       0.024       0.036\n",
      "b_30          -0.0599      0.011     -5.365      0.000      -0.082      -0.038\n",
      "s_18          -0.3439      0.010    -35.438      0.000      -0.363      -0.325\n",
      "d_86          -0.0184      0.006     -3.115      0.002      -0.030      -0.007\n",
      "r_18           0.0068      0.001      5.270      0.000       0.004       0.009\n",
      "b_31          -3.8076      0.020   -189.236      0.000      -3.847      -3.768\n",
      "s_19           0.0109      0.003      3.596      0.000       0.005       0.017\n",
      "r_19           0.0211      0.003      8.203      0.000       0.016       0.026\n",
      "s_20          -0.0080      0.003     -2.420      0.016      -0.014      -0.002\n",
      "r_20           0.0120      0.002      4.903      0.000       0.007       0.017\n",
      "r_21           0.0511      0.004     13.606      0.000       0.044       0.058\n",
      "d_89          -0.0069      0.002     -2.922      0.003      -0.012      -0.002\n",
      "r_22           0.0101      0.002      5.099      0.000       0.006       0.014\n",
      "r_23          -0.0241      0.005     -5.310      0.000      -0.033      -0.015\n",
      "d_91          -0.3014      0.008    -38.396      0.000      -0.317      -0.286\n",
      "d_92          -0.0902      0.012     -7.726      0.000      -0.113      -0.067\n",
      "d_93          -0.0311      0.006     -5.231      0.000      -0.043      -0.019\n",
      "d_94           0.0437      0.014      3.184      0.001       0.017       0.071\n",
      "r_24           0.0165      0.003      5.426      0.000       0.011       0.022\n",
      "r_25          -0.0805      0.003    -30.318      0.000      -0.086      -0.075\n",
      "d_96           0.0179      0.006      3.138      0.002       0.007       0.029\n",
      "d_102          0.0274      0.004      6.254      0.000       0.019       0.036\n",
      "d_107         -0.0358      0.004     -8.414      0.000      -0.044      -0.027\n",
      "r_26          -0.0341      0.003    -12.923      0.000      -0.039      -0.029\n",
      "r_27           0.0089      0.003      2.989      0.003       0.003       0.015\n",
      "b_38          -0.0537      0.003    -18.484      0.000      -0.059      -0.048\n",
      "d_108          0.0223      0.003      8.905      0.000       0.017       0.027\n",
      "d_111          0.0357      0.002     16.384      0.000       0.031       0.040\n",
      "d_112         -0.1073      0.004    -29.755      0.000      -0.114      -0.100\n",
      "s_27           0.0789      0.003     28.765      0.000       0.074       0.084\n",
      "d_113         -0.0906      0.004    -20.881      0.000      -0.099      -0.082\n",
      "d_114         -0.1474      0.009    -15.704      0.000      -0.166      -0.129\n",
      "d_115          0.0258      0.006      4.451      0.000       0.014       0.037\n",
      "d_116          1.1302      0.032     35.869      0.000       1.068       1.192\n",
      "d_117         -0.0225      0.002    -13.043      0.000      -0.026      -0.019\n",
      "d_120          0.8900      0.009    103.918      0.000       0.873       0.907\n",
      "d_121          0.1960      0.005     40.184      0.000       0.186       0.206\n",
      "d_122         -0.0687      0.005    -12.503      0.000      -0.079      -0.058\n",
      "d_123         -0.0425      0.004     -9.647      0.000      -0.051      -0.034\n",
      "d_127         -0.1445      0.012    -11.831      0.000      -0.168      -0.121\n",
      "d_128         -0.0358      0.006     -5.861      0.000      -0.048      -0.024\n",
      "d_129          0.0203      0.006      3.227      0.001       0.008       0.033\n",
      "b_41           0.0498      0.002     21.517      0.000       0.045       0.054\n",
      "d_130          0.0229      0.004      5.188      0.000       0.014       0.032\n",
      "d_131          0.0888      0.006     15.653      0.000       0.078       0.100\n",
      "d_133         -0.0506      0.004    -13.992      0.000      -0.058      -0.044\n",
      "d_136          0.0072      0.002      2.978      0.003       0.002       0.012\n",
      "d_140          0.0214      0.003      8.091      0.000       0.016       0.027\n",
      "d_144         -0.0105      0.004     -2.868      0.004      -0.018      -0.003\n",
      "d_145         -0.0176      0.003     -6.220      0.000      -0.023      -0.012\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "del(df_processed)\n",
    "# Remove the insignificant features and train the model again. I will keep the alpha level as 0.05\n",
    "logit_pvalues = round(sm_logit1.pvalues,3)\n",
    "high_pval_col = logit_pvalues.index[logit_pvalues > 0.05]\n",
    "\n",
    "# Drop these columns\n",
    "X = X.drop(columns = high_pval_col)\n",
    "print(f'The columns remaining after removing insignificant ones : {X.shape}')\n",
    "X_train, X_test,y_train, y_test= train_test_split(X, y, test_size=0.2,\n",
    "                                                     random_state=2303, stratify = y)\n",
    "\n",
    "# Model\n",
    "sm_logit2 = sm.Logit(y_train,X_train).fit()\n",
    "print(sm_logit2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1081056,    1469],\n",
       "       [  22611,    1155]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "prediction_probab = sm_logit2.predict(X_test)\n",
    "prediction = list(map(round,prediction_probab))\n",
    "confusion_matrix(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.9782335750720199\n",
      "Logistic : ROC AUC = 0.893\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Validation accuracy is: {accuracy_score(y_test, prediction)}')\n",
    "\n",
    "# Calculate roc metric \n",
    "print('Logistic : ROC AUC = %.3f' % (roc_auc_score(y_test,prediction_probab)))\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_test,prediction_probab)\n",
    "plt.plot(fpr,tpr,marker = '.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the memory before loading the test data to predict\n",
    "del(fpr,tpr,X_test,X_train,y_test,y_train,X,y,prediction,prediction_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(df):\n",
    "    # Just add extra columns with 0 value so that pipeline does not fail --> these are the extra columns that we had in the training data\n",
    "    extra_cols = ['target','last_statement_flag','last_statement_target']\n",
    "    # Concatenate the dataframe of extra columns with the dataframe of the test data\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.DataFrame(np.zeros((df.shape[0], len(extra_cols))), columns=extra_cols)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Use the pipeline to transform\n",
    "    X = pipeline.transform(df)\n",
    "\n",
    "    # Drop target & the insignificant variables found during the training using statsmodel p-value\n",
    "    X.drop(columns=['last_statement_target','customer_id','s_2'] + df_vif[df_vif['VIF']> 10]['feature'].to_list()\n",
    "                    + high_pval_col.tolist(), inplace=True)\n",
    "\n",
    "    # return log_reg.predict(X), log_reg.predict_proba(X)\n",
    "    # In the statsmodel predict will give the probability\n",
    "    return list(map(round,sm_logit2.predict(X))), sm_logit2.predict(X).tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(sm_logit1)\n",
    "df_test = pd.read_parquet('D:/Sakshi/DSBA_6156_SERJ/data/test.parquet')\n",
    "df_test.columns= df_test.columns.str.lower()\n",
    "# Define the result mdf\n",
    "mdf = pd.DataFrame(columns=['customer_id', 's_2', 'pred', 'proba'])\n",
    "y, y_proba = execute_model(df_test)\n",
    "\n",
    "mdf = pd.concat([\n",
    "    mdf,\n",
    "    pd.DataFrame({\n",
    "        'customer_id': df_test['customer_id'].values,\n",
    "        's_2': df_test['s_2'].values,\n",
    "        'pred': y,\n",
    "        'proba': y_proba\n",
    "    })\n",
    "]) \n",
    "# mdf.to_csv('../ignore/final/logisticregression_baseline_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf['s_2'] = pd.to_datetime(mdf['s_2'])\n",
    "mdf['s_2'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just take the last statement probability of each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last statement probability of each of the customer\n",
    "df_result_last = mdf.sort_values(by = 's_2').groupby('customer_id')[['customer_id','proba']].tail(1)\n",
    "df_result_last.rename(columns= {'proba' : 'prediction'},inplace=True)\n",
    "df_result_last.head()\n",
    "df_result_last.to_csv('D:/Sakshi/DSBA_6156_SERJ/ignore/ppt_analysis/logistic_baseline1_laststmt_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted probabilities using Joe's Code (test data) for the last 3 stmts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the outcome weighting.\n",
    "\n",
    "def conditions(x):\n",
    "    # Customer has 3 statements:\n",
    "    if   x == 3:   return 0.1\n",
    "    elif x == 6:   return 0.15\n",
    "    elif x == 9:   return 0.75\n",
    "    \n",
    "    # Customer has 2 statements:\n",
    "    elif x == 2:   return 0.2\n",
    "    elif x == 4:   return 0.8\n",
    "    \n",
    "    # Customer has 1 statement:\n",
    "    elif x == 1:   return 1.0 \n",
    "    else:          return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the last 3 statements of each customer\n",
    "mdf1 = mdf.sort_values('s_2').groupby('customer_id').tail(3)\n",
    "# if the customer has last 3 stmts the ranking will be as - 1st to the older stmt and 3rd rank to the latest stmt. \n",
    "mdf1[\"statement_num\"] = mdf1.groupby(\"customer_id\")[\"s_2\"].rank(method=\"first\", ascending=True)\n",
    "# The statement_count variable will give the count of the statements for each customer (i.e. to know if they have all the 3 or less than that)\n",
    "mdf1['statement_count'] = mdf1.groupby('customer_id')['statement_num'].transform('max')\n",
    "\n",
    "# Create a number so we can handle the case where a customer had only 1 or 2 statements. \n",
    "# Multiplied to give me a unique value for each case. See conditions() above.\n",
    "mdf1['statement_checksum'] = (mdf1['statement_count']) * mdf1['statement_num']\n",
    "\n",
    "# Assign the weights to the statements\n",
    "mdf1['statement_weight'] = mdf1['statement_checksum'].apply(conditions)\n",
    "\n",
    "# Calculating the weighted sum\n",
    "mdf1 ['prediction'] = mdf1['proba'] * mdf1['statement_weight']\n",
    "\n",
    "mdf1 = mdf1[['customer_id', 'prediction']]\n",
    "\n",
    "# Grouping those weighted sums by customer_id to give granularity of 1 proba per customer\n",
    "mdf1 = mdf1.groupby('customer_id').sum()\n",
    "# Bring the customer_id from index to column\n",
    "mdf1.reset_index(inplace=True)\n",
    "# Send the data to the file\n",
    "mdf1.to_csv('D:/Sakshi/DSBA_6156_SERJ/ignore/ppt_analysis/logistic_baseline1_weighted_last3_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89a73c21ecc9236fdbb84984cd9e615404f96fb7d0e8948f841b3ff5dee670ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
