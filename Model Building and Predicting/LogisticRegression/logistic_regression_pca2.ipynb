{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last stmt of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data \n",
    "df_train_x = pd.read_parquet('D:/Sakshi/DSBA_6156_SERJ/data/train.parquet')\n",
    "df_train_x.columns = df_train_x.columns.str.lower()\n",
    "# Read training data labels\n",
    "df_train_y = pd.read_csv('D:/Sakshi/DSBA_6156_SERJ/data/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "df_train = pd.merge(df_train_x, df_train_y, on='customer_id')\n",
    "del(df_train_x, df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['last_statement_flag'] = (df_train.groupby('customer_id')['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(int)\n",
    "                   )\n",
    "                 \n",
    "df_train = df_train[df_train['last_statement_flag']== 1]  \n",
    "df_train[df_train['customer_id']== '0000f99513770170a1aba690daeeb8a96da4a39f11fc27da5c30a79db61c1e85']\\\n",
    "   [['customer_id','s_2','target','last_statement_flag']]\n",
    "df_train.drop(columns = 'last_statement_flag', inplace=True)\n",
    "df_train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_63     0.0\n",
       "d_64     0.0\n",
       "d_66     0.0\n",
       "d_68     0.0\n",
       "b_30     0.0\n",
       "b_31     0.0\n",
       "b_38     0.0\n",
       "d_114    0.0\n",
       "d_116    0.0\n",
       "d_117    0.0\n",
       "d_120    0.0\n",
       "d_126    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[])\n",
    "# 89% of d_66 column values were missing, but it has been filled with -1 while parquet generation  \n",
    "categorical_cols = ['d_63', 'd_64', 'd_66', 'd_68', 'b_30', 'b_31', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126']\n",
    "df_train[categorical_cols].isnull().sum() / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_cols):\n",
    "    \n",
    "        self.categorical_cols = categorical_cols\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        # Get the list of columns that have missing values greater than equal to 40%\n",
    "        missing_perc = round((X.isnull().sum() / len(X)) * 100,2)\n",
    "        # Prepare final List of columns to drop\n",
    "        self.cols_to_drop = missing_perc[missing_perc.ge(40)].index.tolist()\n",
    " \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Get the clean dataframe with columns to work\n",
    "        X = X.drop(columns=self.cols_to_drop)\n",
    "\n",
    "        numeric_cols = list(set(X.columns.tolist(\n",
    "        )) - set(self.categorical_cols + self.cols_to_drop + ['target', 'customer_id', 's_2']))\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            # Impute col with mean\n",
    "            if X[col].isnull().any().any():\n",
    "                X[col] = X[col].fillna(X[col].mean())\n",
    "\n",
    "            # Scale\n",
    "            mean = X[col].mean()\n",
    "            std = X[col].std()\n",
    "            if std > 0:\n",
    "                X[col] = ((X[col] - mean) / std).astype('float32')\n",
    "        \n",
    "        # # Apply the transformation defined in pipeline\n",
    "        # full_transformer = ColumnTransformer(\n",
    "        #     transformers=[\n",
    "        #         (\"numeric\", self.numeric_pipeline, numeric_cols),\n",
    "        #         (\"categorical\", self.categorical_pipeline, self.categorical_cols),\n",
    "        #     ],\n",
    "        #     # to keep the target column and just passthrough it, instead of dropping\n",
    "        #     remainder='passthrough'\n",
    "        # )\n",
    "\n",
    "        # transformed = full_transformer.fit_transform(df)\n",
    "        # Converted the array to dataframe\n",
    "        # df_transformed = pd.DataFrame(data=transformed, columns= numeric_cols +\\\n",
    "        #  self.categorical_cols + ['target']).astype(df.dtypes.to_dict())\n",
    "\n",
    "        return X\n",
    "\n",
    "# use all the statements of a customer where all stmts are marked with the same target value\n",
    "preprocessing = PreProcessing(categorical_cols)\n",
    "df_processed = preprocessing.fit_transform(df_train)\n",
    "\n",
    "pipeline.steps.append(('preprocessing', preprocessing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw dataframe shape : (458913, 191)\n",
      "The raw dataframe shape : (458913, 173)\n"
     ]
    }
   ],
   "source": [
    "print(f'The raw dataframe shape : {df_train.shape}')\n",
    "print(f'The raw dataframe shape : {df_processed.shape}')\n",
    "del (df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.read_csv('D:/Sakshi/DSBA_6156_SERJ/ignore/ppt_analysis/corr_laststmt.csv',index_col='Unnamed: 0')\n",
    "df_vif = pd.read_csv('D:/Sakshi/DSBA_6156_SERJ/ignore/ppt_analysis/vif_laststmt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 2303\n",
    "class PCATransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vif_data):\n",
    "        self.vif_data = vif_data\n",
    "        \n",
    "        # Get the list of the variables with the higher VIF value\n",
    "        self.vif_variable_lst = vif_data[vif_data['VIF'] >= 10]['feature'].tolist() \n",
    "\n",
    "        # List of columns that were considered in pca\n",
    "        self.pca_cols_to_drop = set()\n",
    "\n",
    "        self.pca_models = []\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Looping on VIF list\n",
    "        for i in self.vif_variable_lst:   \n",
    "            # Check if the VIF variable is already in the set from the earlier pass\n",
    "            bool1 = i in self.pca_cols_to_drop\n",
    "            # print(bool1)\n",
    "            if bool1 == True:   \n",
    "                continue\n",
    "            else: \n",
    "                # Get the list of correlated columns for the current vif column processed\n",
    "                pca_list = df_corr[(df_corr[i] != 1 ) & ((df_corr[i] >= 0.7) | (df_corr[i] <= -0.7 ))].index.tolist()\n",
    "                # Perform the below logic, only when any correlated column found\n",
    "                if len(pca_list) != 0:\n",
    "\n",
    "                    # Add the processed VIF column also\n",
    "                    pca_list = [i] + pca_list\n",
    "                    # print(pca_list)\n",
    "\n",
    "                    # Append this list to set, as these columns at the later stage needs to be dropped off from the main dataframe, because then the PCA values will be used instead of original column\n",
    "                    self.pca_cols_to_drop.update(pca_list)\n",
    "                    \n",
    "                    # Create the dataframe of only those columns that are correlated with the column processed in the loop\n",
    "                    df_pca = X.loc[:,pca_list]\n",
    "                    # Create instance of PCA model\n",
    "                    pca = PCA(random_state=rand_state)\n",
    "                    pca.fit_transform(df_pca)\n",
    "\n",
    "                    # Create eigen value array\n",
    "                    eigen_arr = pca.explained_variance_\n",
    "                    # Create a filter array where the eigen value should be >= 1\n",
    "                    filter_arr = eigen_arr >=1\n",
    "                    # No. of components with eigen value >= 1\n",
    "                    no_of_components = len(eigen_arr[filter_arr])\n",
    "                    \n",
    "                    # Run the PCA again with the no_of_components found\n",
    "                    pca = PCA(n_components = no_of_components, random_state=rand_state)\n",
    "                    pca.fit(df_pca)\n",
    "                    # append the tuple of the columns went for PCA , no. of components, and the instance of the fitted PCA\n",
    "                    self.pca_models.append((pca_list, no_of_components, pca))\n",
    "\n",
    "                    \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for pca_list, no_of_components, pca in self.pca_models:\n",
    "            df_pca = X.loc[:,pca_list]\n",
    "            PCA_values = pca.transform(df_pca)\n",
    "\n",
    "            # The number of columns to create for the final PCA dataframe\n",
    "            pca_columns = []\n",
    "            for val in range(1, no_of_components + 1):\n",
    "                a = pca_list[0] + '_pca_' + str(val)\n",
    "                pca_columns += [a]\n",
    "                    \n",
    "            # Create the final PCA dataframe that will be concatenated to original dataframe\n",
    "            finalpca_df = pd.DataFrame(data = PCA_values, columns=pca_columns)\n",
    "\n",
    "            # Append this dataframe to main one\n",
    "            X = pd.concat([X, finalpca_df], axis=1)   \n",
    "\n",
    "            # Clean-up RAM & memory\n",
    "            del [[df_pca, finalpca_df]]\n",
    "\n",
    "        \n",
    "        # Now remove the columns for which pca was done\n",
    "        X.drop(columns=self.pca_cols_to_drop, inplace=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "pca_transform = PCATransform(df_vif)\n",
    "df_pca = pca_transform.fit_transform(df_processed)\n",
    "\n",
    "pipeline.steps.append(('pca', pca_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of dataframe after pca (458913, 143)\n"
     ]
    }
   ],
   "source": [
    "print(f'the shape of dataframe after pca {df_pca.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.237377\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:               367130\n",
      "Model:                          Logit   Df Residuals:                   366990\n",
      "Method:                           MLE   Df Model:                          139\n",
      "Date:                Tue, 13 Dec 2022   Pseudo R-squ.:                  0.5850\n",
      "Time:                        23:55:40   Log-Likelihood:                -87148.\n",
      "converged:                       True   LL-Null:                   -2.0998e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "p_2            -0.8991      0.015    -60.051      0.000      -0.928      -0.870\n",
      "d_39            0.3342      0.010     35.113      0.000       0.316       0.353\n",
      "s_3             0.1144      0.013      8.631      0.000       0.088       0.140\n",
      "d_41            0.1827      0.013     13.765      0.000       0.157       0.209\n",
      "d_43            0.1089      0.009     12.349      0.000       0.092       0.126\n",
      "d_45           -0.0907      0.014     -6.494      0.000      -0.118      -0.063\n",
      "b_5            -0.1712      0.018     -9.630      0.000      -0.206      -0.136\n",
      "d_46            0.1509      0.007     20.713      0.000       0.137       0.165\n",
      "d_47           -0.1638      0.011    -14.873      0.000      -0.185      -0.142\n",
      "d_48            0.1359      0.012     11.149      0.000       0.112       0.160\n",
      "d_49            0.0603      0.008      7.191      0.000       0.044       0.077\n",
      "b_6            -0.0548      0.017     -3.243      0.001      -0.088      -0.022\n",
      "b_8             0.1888      0.011     17.313      0.000       0.167       0.210\n",
      "d_51           -0.2470      0.014    -17.144      0.000      -0.275      -0.219\n",
      "b_9             0.1246      0.009     14.173      0.000       0.107       0.142\n",
      "r_3             0.1793      0.008     21.539      0.000       0.163       0.196\n",
      "d_52           -0.0435      0.008     -5.563      0.000      -0.059      -0.028\n",
      "p_3            -0.0010      0.008     -0.121      0.904      -0.016       0.014\n",
      "b_10           -0.0223      0.017     -1.342      0.179      -0.055       0.010\n",
      "s_5             0.0243      0.007      3.645      0.000       0.011       0.037\n",
      "s_6             0.0382      0.009      4.322      0.000       0.021       0.056\n",
      "d_54           -0.0875      0.009    -10.268      0.000      -0.104      -0.071\n",
      "s_7             0.0703      0.014      5.207      0.000       0.044       0.097\n",
      "b_12           -0.1935      0.031     -6.181      0.000      -0.255      -0.132\n",
      "s_8            -0.1133      0.014     -7.925      0.000      -0.141      -0.085\n",
      "b_13            0.0722      0.016      4.462      0.000       0.041       0.104\n",
      "d_59            0.0204      0.007      2.766      0.006       0.006       0.035\n",
      "d_60            0.0988      0.011      9.176      0.000       0.078       0.120\n",
      "d_61            0.0414      0.012      3.374      0.001       0.017       0.065\n",
      "s_11           -0.1083      0.007    -14.699      0.000      -0.123      -0.094\n",
      "d_62           -0.1894      0.014    -13.303      0.000      -0.217      -0.162\n",
      "d_63           -0.0346      0.007     -4.651      0.000      -0.049      -0.020\n",
      "d_64            0.0101      0.006      1.794      0.073      -0.001       0.021\n",
      "d_65            0.1055      0.014      7.292      0.000       0.077       0.134\n",
      "d_66           -0.1695      0.011    -15.703      0.000      -0.191      -0.148\n",
      "d_68           -0.0448      0.006     -7.408      0.000      -0.057      -0.033\n",
      "s_12            0.0454      0.006      7.939      0.000       0.034       0.057\n",
      "r_6            -0.0001      0.010     -0.013      0.990      -0.019       0.019\n",
      "s_13            0.0163      0.011      1.422      0.155      -0.006       0.039\n",
      "b_21            0.0276      0.011      2.620      0.009       0.007       0.048\n",
      "d_69            0.0158      0.006      2.437      0.015       0.003       0.028\n",
      "b_22            0.0426      0.009      4.614      0.000       0.024       0.061\n",
      "d_70            0.0674      0.007      9.760      0.000       0.054       0.081\n",
      "d_71           -0.1163      0.031     -3.785      0.000      -0.177      -0.056\n",
      "d_72            0.0098      0.009      1.073      0.283      -0.008       0.028\n",
      "s_15            0.0105      0.010      1.087      0.277      -0.008       0.029\n",
      "p_4             0.0589      0.007      8.047      0.000       0.045       0.073\n",
      "b_24            0.0001      0.012      0.012      0.990      -0.022       0.023\n",
      "r_7             0.0004      0.008      0.042      0.966      -0.016       0.017\n",
      "b_25           -0.0274      0.008     -3.509      0.000      -0.043      -0.012\n",
      "b_26            0.0132      0.006      2.079      0.038       0.001       0.026\n",
      "d_78           -0.0014      0.008     -0.168      0.866      -0.018       0.015\n",
      "d_79            0.0168      0.011      1.528      0.127      -0.005       0.038\n",
      "r_9             0.0065      0.006      1.137      0.256      -0.005       0.018\n",
      "s_16            0.0049      0.008      0.629      0.529      -0.010       0.020\n",
      "d_80            0.0264      0.007      3.956      0.000       0.013       0.039\n",
      "r_10            0.0726      0.008      8.926      0.000       0.057       0.089\n",
      "r_11            0.0774      0.006     14.032      0.000       0.067       0.088\n",
      "b_27        -7.367e-05      0.007     -0.010      0.992      -0.014       0.014\n",
      "d_81            0.0205      0.010      1.984      0.047       0.000       0.041\n",
      "d_82            0.0810      0.010      8.450      0.000       0.062       0.100\n",
      "s_17            0.0120      0.005      2.243      0.025       0.002       0.023\n",
      "r_12           -0.0184      0.008     -2.303      0.021      -0.034      -0.003\n",
      "d_83            0.0126      0.006      2.269      0.023       0.002       0.023\n",
      "r_14            0.0326      0.009      3.537      0.000       0.015       0.051\n",
      "r_15           -0.0056      0.007     -0.753      0.451      -0.020       0.009\n",
      "d_84           -0.0264      0.017     -1.511      0.131      -0.061       0.008\n",
      "r_16           -0.0328      0.007     -4.667      0.000      -0.047      -0.019\n",
      "b_30            0.0821      0.020      4.010      0.000       0.042       0.122\n",
      "s_18           -0.0058      0.007     -0.886      0.375      -0.019       0.007\n",
      "d_86           -0.0506      0.009     -5.732      0.000      -0.068      -0.033\n",
      "d_87           -0.0087      0.009     -1.019      0.308      -0.025       0.008\n",
      "r_17            0.0073      0.014      0.524      0.600      -0.020       0.035\n",
      "r_18           -0.0020      0.008     -0.272      0.785      -0.017       0.013\n",
      "b_31           -1.9656      0.049    -40.360      0.000      -2.061      -1.870\n",
      "s_19            0.0098      0.006      1.635      0.102      -0.002       0.022\n",
      "r_19           -0.0661      0.006    -12.010      0.000      -0.077      -0.055\n",
      "b_32           -0.0005      0.005     -0.104      0.917      -0.011       0.010\n",
      "s_20            0.0102      0.008      1.269      0.204      -0.006       0.026\n",
      "r_20           -0.0097      0.013     -0.724      0.469      -0.036       0.017\n",
      "r_21            0.0586      0.009      6.782      0.000       0.042       0.076\n",
      "d_89           -0.0170      0.014     -1.200      0.230      -0.045       0.011\n",
      "r_22           -0.0073      0.005     -1.558      0.119      -0.016       0.002\n",
      "r_23            0.0036      0.005      0.712      0.476      -0.006       0.014\n",
      "d_91           -0.0716      0.011     -6.750      0.000      -0.092      -0.051\n",
      "d_92           -0.0164      0.017     -0.994      0.320      -0.049       0.016\n",
      "d_93            0.0163      0.009      1.840      0.066      -0.001       0.034\n",
      "d_94            0.0140      0.019      0.720      0.472      -0.024       0.052\n",
      "r_24            0.0012      0.010      0.114      0.909      -0.019       0.022\n",
      "r_25           -0.0346      0.008     -4.113      0.000      -0.051      -0.018\n",
      "d_96           -0.0376      0.009     -4.272      0.000      -0.055      -0.020\n",
      "s_23            0.0694      0.013      5.458      0.000       0.044       0.094\n",
      "s_25           -0.0244      0.005     -4.531      0.000      -0.035      -0.014\n",
      "s_26           -0.0088      0.015     -0.571      0.568      -0.039       0.021\n",
      "d_106          -0.0259      0.006     -4.138      0.000      -0.038      -0.014\n",
      "b_36            0.0166      0.009      1.817      0.069      -0.001       0.034\n",
      "r_26            0.0061      0.008      0.802      0.422      -0.009       0.021\n",
      "r_27           -0.0744      0.006    -12.041      0.000      -0.086      -0.062\n",
      "b_38           -0.0202      0.005     -3.878      0.000      -0.030      -0.010\n",
      "d_108           0.0161      0.005      3.137      0.002       0.006       0.026\n",
      "d_109          -0.0058      0.012     -0.497      0.619      -0.029       0.017\n",
      "d_111           0.0568      0.009      6.252      0.000       0.039       0.075\n",
      "d_112          -0.1606      0.008    -20.041      0.000      -0.176      -0.145\n",
      "b_40            0.0123      0.009      1.306      0.191      -0.006       0.031\n",
      "s_27           -0.0220      0.005     -4.219      0.000      -0.032      -0.012\n",
      "d_113          -0.0029      0.008     -0.351      0.726      -0.019       0.013\n",
      "d_114          -0.1271      0.017     -7.591      0.000      -0.160      -0.094\n",
      "d_116          -0.1122      0.064     -1.757      0.079      -0.237       0.013\n",
      "d_117          -0.0251      0.003     -8.166      0.000      -0.031      -0.019\n",
      "d_120           0.2090      0.017     12.086      0.000       0.175       0.243\n",
      "d_121           0.1742      0.012     14.839      0.000       0.151       0.197\n",
      "d_122          -0.0157      0.009     -1.716      0.086      -0.034       0.002\n",
      "d_123           0.0170      0.008      2.147      0.032       0.001       0.032\n",
      "d_124           0.0081      0.008      0.986      0.324      -0.008       0.024\n",
      "d_125          -0.0054      0.008     -0.650      0.516      -0.022       0.011\n",
      "d_126          -0.0061      0.017     -0.361      0.718      -0.039       0.027\n",
      "d_127          -0.1027      0.018     -5.779      0.000      -0.137      -0.068\n",
      "d_128          -0.0433      0.011     -4.044      0.000      -0.064      -0.022\n",
      "d_129          -0.1171      0.011    -11.082      0.000      -0.138      -0.096\n",
      "b_41            0.0456      0.008      6.084      0.000       0.031       0.060\n",
      "d_130           0.0108      0.008      1.355      0.176      -0.005       0.026\n",
      "d_131           0.2068      0.013     16.503      0.000       0.182       0.231\n",
      "d_133          -0.1350      0.007    -18.069      0.000      -0.150      -0.120\n",
      "r_28           -0.0005      0.004     -0.116      0.908      -0.009       0.008\n",
      "d_140           0.0539      0.005      9.813      0.000       0.043       0.065\n",
      "d_144          -0.0041      0.007     -0.543      0.587      -0.019       0.011\n",
      "d_145           0.0064      0.007      0.952      0.341      -0.007       0.020\n",
      "b_1_pca_1      -0.1593      0.005    -30.108      0.000      -0.170      -0.149\n",
      "b_2_pca_1       0.0917      0.006     14.331      0.000       0.079       0.104\n",
      "r_1_pca_1       0.1291      0.010     12.367      0.000       0.109       0.150\n",
      "b_7_pca_1      -0.0740      0.007    -10.660      0.000      -0.088      -0.060\n",
      "r_5_pca_1       0.1490      0.020      7.278      0.000       0.109       0.189\n",
      "d_58_pca_1      0.0166      0.008      2.194      0.028       0.002       0.031\n",
      "b_14_pca_1      0.0349      0.009      3.678      0.000       0.016       0.054\n",
      "b_28_pca_1      0.2311      0.011     20.324      0.000       0.209       0.253\n",
      "s_22_pca_1     -0.0136      0.007     -2.034      0.042      -0.027      -0.000\n",
      "d_103_pca_1    -0.0055      0.004     -1.539      0.124      -0.013       0.002\n",
      "d_118_pca_1    -0.0563      0.007     -8.640      0.000      -0.069      -0.044\n",
      "d_135_pca_1     0.0161      0.003      6.212      0.000       0.011       0.021\n",
      "d_139_pca_1    -0.0342      0.005     -6.397      0.000      -0.045      -0.024\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression model\n",
    "X = df_pca.drop(columns=['customer_id','s_2','target'])\n",
    "y = df_pca['target']\n",
    "# Split the data using stratify method, to avoid only one class data seep in train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=rand_state,stratify = y)\n",
    "\n",
    "# Model\n",
    "sm_logit1 = sm.Logit(y_train,X_train).fit()\n",
    "print(sm_logit1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns remaining after removing insignificant ones : (458913, 93)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.237461\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:               367130\n",
      "Model:                          Logit   Df Residuals:                   367037\n",
      "Method:                           MLE   Df Model:                           92\n",
      "Date:                Tue, 13 Dec 2022   Pseudo R-squ.:                  0.5848\n",
      "Time:                        23:55:49   Log-Likelihood:                -87179.\n",
      "converged:                       True   LL-Null:                   -2.0998e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "p_2            -0.9034      0.013    -67.393      0.000      -0.930      -0.877\n",
      "d_39            0.3351      0.009     35.314      0.000       0.316       0.354\n",
      "s_3             0.1169      0.013      9.016      0.000       0.091       0.142\n",
      "d_41            0.1801      0.013     13.653      0.000       0.154       0.206\n",
      "d_43            0.1097      0.009     12.483      0.000       0.093       0.127\n",
      "d_45           -0.0923      0.014     -6.639      0.000      -0.120      -0.065\n",
      "b_5            -0.1717      0.018     -9.785      0.000      -0.206      -0.137\n",
      "d_46            0.1512      0.007     21.635      0.000       0.137       0.165\n",
      "d_47           -0.1638      0.011    -14.974      0.000      -0.185      -0.142\n",
      "d_48            0.1355      0.012     11.197      0.000       0.112       0.159\n",
      "d_49            0.0646      0.008      8.212      0.000       0.049       0.080\n",
      "b_6            -0.0567      0.017     -3.340      0.001      -0.090      -0.023\n",
      "b_8             0.1883      0.011     17.310      0.000       0.167       0.210\n",
      "d_51           -0.2475      0.013    -19.030      0.000      -0.273      -0.222\n",
      "b_9             0.1253      0.009     14.301      0.000       0.108       0.142\n",
      "r_3             0.1793      0.008     21.691      0.000       0.163       0.195\n",
      "d_52           -0.0430      0.008     -5.613      0.000      -0.058      -0.028\n",
      "s_5             0.0248      0.007      3.764      0.000       0.012       0.038\n",
      "s_6             0.0377      0.008      4.574      0.000       0.022       0.054\n",
      "d_54           -0.0870      0.009    -10.232      0.000      -0.104      -0.070\n",
      "s_7             0.0714      0.013      5.343      0.000       0.045       0.098\n",
      "b_12           -0.1991      0.031     -6.433      0.000      -0.260      -0.138\n",
      "s_8            -0.1164      0.009    -12.805      0.000      -0.134      -0.099\n",
      "b_13            0.0705      0.015      4.554      0.000       0.040       0.101\n",
      "d_59            0.0188      0.007      2.571      0.010       0.004       0.033\n",
      "d_60            0.1012      0.011      9.562      0.000       0.080       0.122\n",
      "d_61            0.0428      0.012      3.497      0.000       0.019       0.067\n",
      "s_11           -0.1055      0.007    -14.956      0.000      -0.119      -0.092\n",
      "d_62           -0.1911      0.014    -13.488      0.000      -0.219      -0.163\n",
      "d_63           -0.0342      0.007     -4.610      0.000      -0.049      -0.020\n",
      "d_65            0.1053      0.014      7.274      0.000       0.077       0.134\n",
      "d_66           -0.1698      0.011    -15.847      0.000      -0.191      -0.149\n",
      "d_68           -0.0483      0.005     -9.823      0.000      -0.058      -0.039\n",
      "s_12            0.0467      0.006      8.226      0.000       0.036       0.058\n",
      "b_21            0.0330      0.010      3.451      0.001       0.014       0.052\n",
      "d_69            0.0166      0.006      2.601      0.009       0.004       0.029\n",
      "b_22            0.0425      0.009      4.615      0.000       0.024       0.061\n",
      "d_70            0.0666      0.007      9.694      0.000       0.053       0.080\n",
      "d_71           -0.1044      0.024     -4.307      0.000      -0.152      -0.057\n",
      "p_4             0.0607      0.007      8.473      0.000       0.047       0.075\n",
      "b_25           -0.0240      0.008     -3.144      0.002      -0.039      -0.009\n",
      "b_26            0.0136      0.006      2.153      0.031       0.001       0.026\n",
      "d_80            0.0263      0.007      3.963      0.000       0.013       0.039\n",
      "r_10            0.0740      0.008      9.598      0.000       0.059       0.089\n",
      "r_11            0.0780      0.005     14.178      0.000       0.067       0.089\n",
      "d_81            0.0129      0.008      1.534      0.125      -0.004       0.029\n",
      "d_82            0.0840      0.009      8.903      0.000       0.066       0.103\n",
      "s_17            0.0123      0.005      2.301      0.021       0.002       0.023\n",
      "r_12           -0.0193      0.007     -2.778      0.005      -0.033      -0.006\n",
      "d_83            0.0119      0.005      2.206      0.027       0.001       0.023\n",
      "r_14            0.0313      0.008      3.885      0.000       0.015       0.047\n",
      "r_16           -0.0331      0.007     -4.712      0.000      -0.047      -0.019\n",
      "b_30            0.0785      0.020      3.857      0.000       0.039       0.118\n",
      "d_86           -0.0499      0.009     -5.674      0.000      -0.067      -0.033\n",
      "b_31           -1.9153      0.036    -53.162      0.000      -1.986      -1.845\n",
      "r_19           -0.0657      0.005    -11.996      0.000      -0.076      -0.055\n",
      "r_21            0.0676      0.007     10.190      0.000       0.055       0.081\n",
      "d_91           -0.0748      0.010     -7.277      0.000      -0.095      -0.055\n",
      "r_25           -0.0333      0.007     -4.738      0.000      -0.047      -0.020\n",
      "d_96           -0.0375      0.009     -4.297      0.000      -0.055      -0.020\n",
      "s_23            0.0682      0.013      5.376      0.000       0.043       0.093\n",
      "s_25           -0.0239      0.005     -4.436      0.000      -0.034      -0.013\n",
      "d_106          -0.0274      0.006     -4.536      0.000      -0.039      -0.016\n",
      "r_27           -0.0777      0.006    -13.921      0.000      -0.089      -0.067\n",
      "b_38           -0.0209      0.005     -4.113      0.000      -0.031      -0.011\n",
      "d_108           0.0162      0.005      3.157      0.002       0.006       0.026\n",
      "d_111           0.0708      0.005     14.439      0.000       0.061       0.080\n",
      "d_112          -0.1607      0.008    -20.089      0.000      -0.176      -0.145\n",
      "s_27           -0.0203      0.005     -4.054      0.000      -0.030      -0.010\n",
      "d_114          -0.1464      0.016     -9.335      0.000      -0.177      -0.116\n",
      "d_117          -0.0286      0.003    -10.038      0.000      -0.034      -0.023\n",
      "d_120           0.1998      0.016     12.226      0.000       0.168       0.232\n",
      "d_121           0.1851      0.009     20.283      0.000       0.167       0.203\n",
      "d_123           0.0119      0.006      2.004      0.045       0.000       0.023\n",
      "d_127          -0.1031      0.018     -5.819      0.000      -0.138      -0.068\n",
      "d_128          -0.0426      0.010     -4.070      0.000      -0.063      -0.022\n",
      "d_129          -0.1203      0.010    -11.627      0.000      -0.141      -0.100\n",
      "b_41            0.0452      0.007      6.193      0.000       0.031       0.059\n",
      "d_131           0.2241      0.009     24.430      0.000       0.206       0.242\n",
      "d_133          -0.1340      0.007    -18.080      0.000      -0.149      -0.120\n",
      "d_140           0.0549      0.005     10.462      0.000       0.045       0.065\n",
      "b_1_pca_1      -0.1580      0.005    -30.101      0.000      -0.168      -0.148\n",
      "b_2_pca_1       0.0915      0.006     14.585      0.000       0.079       0.104\n",
      "r_1_pca_1       0.1227      0.008     14.478      0.000       0.106       0.139\n",
      "b_7_pca_1      -0.0752      0.007    -10.912      0.000      -0.089      -0.062\n",
      "r_5_pca_1       0.1463      0.018      7.973      0.000       0.110       0.182\n",
      "d_58_pca_1      0.0166      0.007      2.234      0.025       0.002       0.031\n",
      "b_14_pca_1      0.0366      0.009      3.935      0.000       0.018       0.055\n",
      "b_28_pca_1      0.2306      0.011     20.399      0.000       0.208       0.253\n",
      "s_22_pca_1     -0.0144      0.007     -2.129      0.033      -0.028      -0.001\n",
      "d_118_pca_1    -0.0609      0.006    -10.574      0.000      -0.072      -0.050\n",
      "d_135_pca_1     0.0163      0.003      6.308      0.000       0.011       0.021\n",
      "d_139_pca_1    -0.0333      0.004     -9.296      0.000      -0.040      -0.026\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Remove the insignificant features and train the model again. I will keep the alpha level as 0.05\n",
    "logit_pvalues = round(sm_logit1.pvalues,3)\n",
    "high_pval_col = logit_pvalues.index[logit_pvalues > 0.05]\n",
    "\n",
    "# Drop these columns\n",
    "X = X.drop(columns = high_pval_col)\n",
    "print(f'The columns remaining after removing insignificant ones : {X.shape}')\n",
    "X_train, X_test,y_train, y_test= train_test_split(X, y, test_size=0.2,\n",
    "                                                     random_state=2303, stratify = y)\n",
    "\n",
    "# Model\n",
    "sm_logit2 = sm.Logit(y_train,X_train).fit()\n",
    "print(sm_logit2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63727,  4290],\n",
       "       [ 5315, 18451]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "prediction_probab = sm_logit2.predict(X_test)\n",
    "prediction = list(map(round,prediction_probab))\n",
    "confusion_matrix(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.8953509909242452\n",
      "Logistic : ROC AUC = 0.955\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Validation accuracy is: {accuracy_score(y_test, prediction)}')\n",
    "\n",
    "# Calculate roc metric \n",
    "print('Logistic : ROC AUC = %.3f' % (roc_auc_score(y_test,prediction_probab)))\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_test,prediction_probab)\n",
    "plt.plot(fpr,tpr,marker = '.')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     68017\n",
      "           1       0.81      0.78      0.79     23766\n",
      "\n",
      "    accuracy                           0.90     91783\n",
      "   macro avg       0.87      0.86      0.86     91783\n",
      "weighted avg       0.89      0.90      0.89     91783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Recall and Precision\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the memory before loading the test data to predict\n",
    "del(fpr,tpr,X_test,X_train,y_test,y_train,X,y,prediction,prediction_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(df):\n",
    "    # Just add extra columns with 0 value so that pipeline does not fail --> these are the extra columns that we had in the training data\n",
    "    extra_cols = ['target']\n",
    "    # Concatenate the dataframe of extra columns with the dataframe of the test data\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.DataFrame(np.zeros((df.shape[0], len(extra_cols))), columns=extra_cols)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Use the pipeline to transform\n",
    "    X = pipeline.transform(df)\n",
    "\n",
    "    # Drop target & the insignificant variables found during the training using statsmodel p-value\n",
    "    X.drop(columns=['target','customer_id','s_2'] \\\n",
    "                    + high_pval_col.tolist() , inplace=True)\n",
    "\n",
    "    # return log_reg.predict_proba(X)\n",
    "    # In the statsmodel predict will give the probability\n",
    "    return sm_logit2.predict(X).tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(sm_logit1)\n",
    "df_test = pd.read_parquet('D:/Sakshi/DSBA_6156_SERJ/data/test.parquet')\n",
    "df_test.columns= df_test.columns.str.lower()\n",
    "df_test['last_statement_flag'] = (df_test.groupby('customer_id')['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(np.int8)\n",
    "                   )\n",
    "df_test = df_test[df_test['last_statement_flag']== 1].copy()\n",
    "df_test = df_test.drop(columns='last_statement_flag').reset_index(drop=True)\n",
    "# Define the result mdf\n",
    "mdf = pd.DataFrame(columns=['customer_id', 's_2', 'proba'])\n",
    "y_proba = execute_model(df_test)\n",
    "\n",
    "mdf = pd.concat([\n",
    "    mdf,\n",
    "    pd.DataFrame({\n",
    "        'customer_id': df_test['customer_id'].values,\n",
    "        's_2': df_test['s_2'].values,\n",
    "        'proba': y_proba\n",
    "    })\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf['s_2'] = pd.to_datetime(mdf['s_2'])\n",
    "mdf['s_2'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last statement probability of each of the customer\n",
    "mdf.rename(columns= {'proba' : 'prediction'},inplace=True)\n",
    "mdf.to_csv('D:/Sakshi/DSBA_6156_SERJ/ignore/ppt_analysis/logistic_pca_laststmt_upd.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89a73c21ecc9236fdbb84984cd9e615404f96fb7d0e8948f841b3ff5dee670ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
