{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "def set_col_types(df):\n",
    "    if \"target\" in df.columns:\n",
    "        categorical_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68', 'target']\n",
    "    else:\n",
    "        categorical_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "    df['customer_ID'] = df['customer_ID'].astype(\"string\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype(\"string\")\n",
    "    df[\"S_2\"] = pd.to_datetime(df['S_2'], format=r'%Y-%m-%d').astype('datetime64[ns]')\n",
    "    df[\"B_31\"] = df[\"B_31\"].astype(np.int8)\n",
    "    return df\n",
    "\n",
    "def sync_cols(train_df, pred_df):\n",
    "    for col in train_df.columns:\n",
    "      if col not in pred_df.columns:\n",
    "        print(col, \"not in pred_df so adding - should always be categorical!\")\n",
    "        pred_df[col] = 0\n",
    "    for col in pred_df.columns:\n",
    "      if col not in train_df.columns:\n",
    "        print(col, \"not in train_df so dropping\")\n",
    "        pred_df = pred_df.drop(col, axis=1)\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.90591   0.92077   0.91328      1830\n",
      "           1    0.76948   0.73445   0.75155       659\n",
      "\n",
      "    accuracy                        0.87143      2489\n",
      "   macro avg    0.83769   0.82761   0.83242      2489\n",
      "weighted avg    0.86979   0.87143   0.87046      2489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from parquet file\n",
    "df = pd.read_parquet(r\"../../../amex-default-prediction/train_data.parquet\")\n",
    "#reduce df for development !!!!! comment out line below for final model\n",
    "df = df[:100000]\n",
    "\n",
    "# Set the data types for the columns\n",
    "df = set_col_types(df)\n",
    "\n",
    "#engineer statement num\n",
    "df['statement_num'] = df.groupby(\"customer_ID\")['S_2'].rank(method='first', ascending=False).astype(np.int8) \n",
    "df = df[df[\"statement_num\"] == 1]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "#engineer date cols\n",
    "df[\"Month\"] = df[\"S_2\"].dt.month\n",
    "df[\"Day\"] = df[\"S_2\"].dt.day\n",
    "df[\"Year\"] = df[\"S_2\"].dt.year\n",
    "df = df.drop([\"S_2\"], axis=1)\n",
    "\n",
    "# Separate target variable and feature columns\n",
    "target = df[\"target\"].astype(int)\n",
    "labels = df['customer_ID']\n",
    "features = df.drop([\"customer_ID\", \"target\"], axis=1)\n",
    "\n",
    "# Impute missing values using mode for categorical columns and median for numerical columns\n",
    "cat_columns = features.select_dtypes(include=[\"string\"]).columns\n",
    "num_columns = features.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# Replace missing values in the categorical columns with the most frequent value\n",
    "for col in cat_columns:\n",
    "    features[col].fillna(\"NA\", inplace=True)\n",
    "\n",
    "# Replace missing values in the numerical columns with the median value\n",
    "for col in num_columns:\n",
    "    features[col].fillna(features[col].mean(), inplace=True)\n",
    "\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "features = features.sort_index(axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7159046532615896"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amex_test = y_test.reset_index(drop=True).to_frame()\n",
    "amex_test['target'] = amex_test['target'].astype('int')\n",
    "\n",
    "amex_pred = model.predict_proba(X_test)\n",
    "amex_pred = pd.DataFrame(amex_pred,columns=[\"proba-inv\",\"proba\"])\n",
    "amex_pred = amex_pred.drop(columns=['proba-inv'])\n",
    "amex_pred = amex_pred.rename(columns={\"proba\":\"prediction\"})\n",
    "\n",
    "\n",
    "amex_metric(amex_test, amex_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B_30_NA not in pred_df so adding - should always be categorical!\n",
      "B_38_NA not in pred_df so adding - should always be categorical!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3', 'B_30_0.0', 'B_30_1.0', 'B_30_2.0', 'B_30_NA', 'B_31', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38_1.0', 'B_38_2.0', 'B_38_3.0', 'B_38_4.0', 'B_38_5.0', 'B_38_6.0', 'B_38_7.0', 'B_38_NA', 'B_39', 'B_4', 'B_40', 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'D_108', 'D_109', 'D_110', 'D_111', 'D_112', 'D_113', 'D_114_0.0', 'D_114_1.0', 'D_114_NA', 'D_115', 'D_116_0.0', 'D_116_1.0', 'D_116_NA', 'D_117_-1.0', 'D_117_1.0', 'D_117_2.0', 'D_117_3.0', 'D_117_4.0', 'D_117_5.0', 'D_117_6.0', 'D_117_NA', 'D_118', 'D_119', 'D_120_0.0', 'D_120_1.0', 'D_120_NA', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126_0.0', 'D_126_1.0', 'D_127', 'D_128', 'D_129', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63_CL', 'D_63_CO', 'D_63_CR', 'D_63_XL', 'D_63_XM', 'D_63_XZ', 'D_64_NA', 'D_64_O', 'D_64_R', 'D_64_U', 'D_65', 'D_66_1.0', 'D_66_NA', 'D_68_1.0', 'D_68_2.0', 'D_68_3.0', 'D_68_4.0', 'D_68_5.0', 'D_68_6.0', 'D_68_NA', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_84', 'D_86', 'D_87', 'D_88', 'D_89', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96', 'Day', 'Month', 'P_2', 'P_3', 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_16', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', 'R_21', 'R_22', 'R_23', 'R_24', 'R_25', 'R_26', 'R_27', 'R_28', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'Year', 'statement_num'] ['B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3', 'B_30_0.0', 'B_30_1.0', 'B_30_2.0', 'B_31', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38_1.0', 'B_38_2.0', 'B_38_3.0', 'B_38_4.0', 'B_38_5.0', 'B_38_6.0', 'B_38_7.0', 'B_39', 'B_4', 'B_40', 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'D_108', 'D_109', 'D_110', 'D_111', 'D_112', 'D_113', 'D_114_0.0', 'D_114_1.0', 'D_114_NA', 'D_115', 'D_116_0.0', 'D_116_1.0', 'D_116_NA', 'D_117_-1.0', 'D_117_1.0', 'D_117_2.0', 'D_117_3.0', 'D_117_4.0', 'D_117_5.0', 'D_117_6.0', 'D_117_NA', 'D_118', 'D_119', 'D_120_0.0', 'D_120_1.0', 'D_120_NA', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126_0.0', 'D_126_1.0', 'D_127', 'D_128', 'D_129', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63_CL', 'D_63_CO', 'D_63_CR', 'D_63_XL', 'D_63_XM', 'D_63_XZ', 'D_64_NA', 'D_64_O', 'D_64_R', 'D_64_U', 'D_65', 'D_66_1.0', 'D_66_NA', 'D_68_1.0', 'D_68_2.0', 'D_68_3.0', 'D_68_4.0', 'D_68_5.0', 'D_68_6.0', 'D_68_NA', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_84', 'D_86', 'D_87', 'D_88', 'D_89', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96', 'Day', 'Month', 'P_2', 'P_3', 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_16', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', 'R_21', 'R_22', 'R_23', 'R_24', 'R_25', 'R_26', 'R_27', 'R_28', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'Year', 'statement_num', 'B_30_NA', 'B_38_NA']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m features \u001b[39m=\u001b[39m sync_cols(X_train, features)\n\u001b[0;32m     39\u001b[0m \u001b[39m# Make predictions on the test data\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(features)\n\u001b[0;32m     41\u001b[0m y_prob \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(features)\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\sklearn.py:1551\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m   1542\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1543\u001b[0m     X: ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     iteration_range: Optional[Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1549\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1550\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(verbosity\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbosity):\n\u001b[1;32m-> 1551\u001b[0m         class_probs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m   1552\u001b[0m             X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1553\u001b[0m             output_margin\u001b[39m=\u001b[39;49moutput_margin,\n\u001b[0;32m   1554\u001b[0m             ntree_limit\u001b[39m=\u001b[39;49mntree_limit,\n\u001b[0;32m   1555\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1556\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1557\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1558\u001b[0m         )\n\u001b[0;32m   1559\u001b[0m         \u001b[39mif\u001b[39;00m output_margin:\n\u001b[0;32m   1560\u001b[0m             \u001b[39m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m             \u001b[39mreturn\u001b[39;00m class_probs\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\sklearn.py:1140\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1139\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m         predts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster()\u001b[39m.\u001b[39;49minplace_predict(\n\u001b[0;32m   1141\u001b[0m             data\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1142\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1143\u001b[0m             predict_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmargin\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m output_margin \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1144\u001b[0m             missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m   1145\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1146\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1147\u001b[0m         )\n\u001b[0;32m   1148\u001b[0m         \u001b[39mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[0;32m   1149\u001b[0m             \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m  \u001b[39m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:2285\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2283\u001b[0m     data, fns, _ \u001b[39m=\u001b[39m _transform_pandas_df(data, enable_categorical)\n\u001b[0;32m   2284\u001b[0m     \u001b[39mif\u001b[39;00m validate_features:\n\u001b[1;32m-> 2285\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_features(fns)\n\u001b[0;32m   2287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m   2288\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m _ensure_np_dtype\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xgboost\\core.py:2779\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[1;34m(self, feature_names)\u001b[0m\n\u001b[0;32m   2773\u001b[0m \u001b[39mif\u001b[39;00m my_missing:\n\u001b[0;32m   2774\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m   2775\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mtraining data did not have the following fields: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2776\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m my_missing)\n\u001b[0;32m   2777\u001b[0m     )\n\u001b[1;32m-> 2779\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3', 'B_30_0.0', 'B_30_1.0', 'B_30_2.0', 'B_30_NA', 'B_31', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38_1.0', 'B_38_2.0', 'B_38_3.0', 'B_38_4.0', 'B_38_5.0', 'B_38_6.0', 'B_38_7.0', 'B_38_NA', 'B_39', 'B_4', 'B_40', 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'D_108', 'D_109', 'D_110', 'D_111', 'D_112', 'D_113', 'D_114_0.0', 'D_114_1.0', 'D_114_NA', 'D_115', 'D_116_0.0', 'D_116_1.0', 'D_116_NA', 'D_117_-1.0', 'D_117_1.0', 'D_117_2.0', 'D_117_3.0', 'D_117_4.0', 'D_117_5.0', 'D_117_6.0', 'D_117_NA', 'D_118', 'D_119', 'D_120_0.0', 'D_120_1.0', 'D_120_NA', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126_0.0', 'D_126_1.0', 'D_127', 'D_128', 'D_129', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63_CL', 'D_63_CO', 'D_63_CR', 'D_63_XL', 'D_63_XM', 'D_63_XZ', 'D_64_NA', 'D_64_O', 'D_64_R', 'D_64_U', 'D_65', 'D_66_1.0', 'D_66_NA', 'D_68_1.0', 'D_68_2.0', 'D_68_3.0', 'D_68_4.0', 'D_68_5.0', 'D_68_6.0', 'D_68_NA', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_84', 'D_86', 'D_87', 'D_88', 'D_89', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96', 'Day', 'Month', 'P_2', 'P_3', 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_16', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', 'R_21', 'R_22', 'R_23', 'R_24', 'R_25', 'R_26', 'R_27', 'R_28', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'Year', 'statement_num'] ['B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3', 'B_30_0.0', 'B_30_1.0', 'B_30_2.0', 'B_31', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38_1.0', 'B_38_2.0', 'B_38_3.0', 'B_38_4.0', 'B_38_5.0', 'B_38_6.0', 'B_38_7.0', 'B_39', 'B_4', 'B_40', 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'D_102', 'D_103', 'D_104', 'D_105', 'D_106', 'D_107', 'D_108', 'D_109', 'D_110', 'D_111', 'D_112', 'D_113', 'D_114_0.0', 'D_114_1.0', 'D_114_NA', 'D_115', 'D_116_0.0', 'D_116_1.0', 'D_116_NA', 'D_117_-1.0', 'D_117_1.0', 'D_117_2.0', 'D_117_3.0', 'D_117_4.0', 'D_117_5.0', 'D_117_6.0', 'D_117_NA', 'D_118', 'D_119', 'D_120_0.0', 'D_120_1.0', 'D_120_NA', 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126_0.0', 'D_126_1.0', 'D_127', 'D_128', 'D_129', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_63_CL', 'D_63_CO', 'D_63_CR', 'D_63_XL', 'D_63_XM', 'D_63_XZ', 'D_64_NA', 'D_64_O', 'D_64_R', 'D_64_U', 'D_65', 'D_66_1.0', 'D_66_NA', 'D_68_1.0', 'D_68_2.0', 'D_68_3.0', 'D_68_4.0', 'D_68_5.0', 'D_68_6.0', 'D_68_NA', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', 'D_80', 'D_81', 'D_82', 'D_83', 'D_84', 'D_86', 'D_87', 'D_88', 'D_89', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96', 'Day', 'Month', 'P_2', 'P_3', 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_16', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', 'R_21', 'R_22', 'R_23', 'R_24', 'R_25', 'R_26', 'R_27', 'R_28', 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'Year', 'statement_num', 'B_30_NA', 'B_38_NA']"
     ]
    }
   ],
   "source": [
    "# Read test data from parquet file\n",
    "df = pd.read_parquet(r\"../../../amex-default-prediction/test_data.parquet\")\n",
    "df = df[:100000]\n",
    "\n",
    "# Set the data types for the columns\n",
    "df = set_col_types(df)\n",
    "\n",
    "# Engineer statement num\n",
    "df['statement_num'] = df.groupby(\"customer_ID\")['S_2'].rank(method='first', ascending=False).astype(np.int8)\n",
    "df = df[df[\"statement_num\"] == 1]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "# Engineer date cols\n",
    "df[\"Month\"] = df[\"S_2\"].dt.month\n",
    "df[\"Day\"] = df[\"S_2\"].dt.day\n",
    "df[\"Year\"] = df[\"S_2\"].dt.year\n",
    "df = df.drop([\"S_2\"], axis=1)\n",
    "\n",
    "# Separate labels and feature columns\n",
    "labels = df['customer_ID']\n",
    "features = df.drop([\"customer_ID\"], axis=1)\n",
    "\n",
    "# Impute missing values using mode for categorical columns and median for numerical columns\n",
    "cat_columns = features.select_dtypes(include=[\"string\"]).columns\n",
    "num_columns = features.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# Replace missing values in the categorical columns with the most frequent value\n",
    "for col in cat_columns:\n",
    "    features[col].fillna(\"NA\", inplace=True)\n",
    "\n",
    "# Replace missing values in the numerical columns with the median value\n",
    "for col in num_columns:\n",
    "    features[col].fillna(features[col].mean(), inplace=True)\n",
    "\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "features = sync_cols(X_train, features)\n",
    "\n",
    "features = features.sort_index(axis=1)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(features)\n",
    "y_prob = model.predict_proba(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the labels to the predictions\n",
    "prediction_output = pd.concat([labels,pd.DataFrame(y_pred,columns=[\"pred\"]),pd.DataFrame(y_prob,columns=[\"proba-inv\",\"proba\"])], axis=1)\n",
    "\n",
    "prediction_output = prediction_output.drop(columns=['proba-inv','pred'])\n",
    "prediction_output = prediction_output.rename(columns={\"proba\":\"prediction\"})\n",
    "\n",
    "prediction_output.to_csv(\"train_last_pred_last_mean_tuned_but_int.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79cebfffb2e3a4b7d2d2fd53b48f0eab2f20a6a535e26e1d02c2764acd76f0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
