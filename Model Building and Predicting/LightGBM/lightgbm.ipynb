{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "def set_col_types(df):\n",
    "    if \"target\" in df.columns:\n",
    "        categorical_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68', 'target']\n",
    "    else:\n",
    "        categorical_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "    df['customer_ID'] = df['customer_ID'].astype(\"string\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype(\"string\")\n",
    "    df[\"S_2\"] = pd.to_datetime(df['S_2'], format=r'%Y-%m-%d').astype('datetime64[ns]')\n",
    "    df[\"B_31\"] = df[\"B_31\"].astype(np.int8)\n",
    "    return df\n",
    "\n",
    "def sync_cols(train_df, pred_df):\n",
    "    for col in train_df.columns:\n",
    "      if col not in pred_df.columns:\n",
    "        print(col, \"not in pred_df so adding - should always be categorical!\")\n",
    "        pred_df[col] = 0\n",
    "    for col in pred_df.columns:\n",
    "      if col not in train_df.columns:\n",
    "        print(col, \"not in train_df so dropping\")\n",
    "        pred_df = pred_df.drop(col, axis=1)\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [111], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../../amex-default-prediction/train_data.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[39m#reduce df for development !!!!! comment out line below for final model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m#df = df[:100000]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[39m# Set the data types for the columns\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df \u001b[39m=\u001b[39m set_col_types(df)\n\u001b[0;32m     16\u001b[0m \u001b[39m#engineer statement num\u001b[39;00m\n\u001b[0;32m     17\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mstatement_num\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby(\u001b[39m\"\u001b[39m\u001b[39mcustomer_ID\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mS_2\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mrank(method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint8) \n",
      "Cell \u001b[1;32mIn [96], line 40\u001b[0m, in \u001b[0;36mset_col_types\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     37\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mcustomer_ID\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcustomer_ID\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m categorical_cols:\n\u001b[1;32m---> 40\u001b[0m     df[col] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39;49mastype(\u001b[39m\"\u001b[39;49m\u001b[39mstring\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     41\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mS_2\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m'\u001b[39m\u001b[39mS_2\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mdatetime64[ns]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mB_31\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mB_31\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint8)\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6233\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[0;32m   6234\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m   6235\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[0;32m   6236\u001b[0m     ]\n\u001b[0;32m   6238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6239\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6240\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   6241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6243\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py:445\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mastype\u001b[39m(\u001b[39mself\u001b[39m: T, dtype, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py:347\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    346\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    348\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    349\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    524\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[1;32m--> 526\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m    528\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    529\u001b[0m newb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m values\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    298\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    300\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m    301\u001b[0m     \u001b[39m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    227\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    229\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     values \u001b[39m=\u001b[39m astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    232\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:95\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m# dispatch on extension dtype if needed\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[1;32m---> 95\u001b[0m     \u001b[39mreturn\u001b[39;00m dtype\u001b[39m.\u001b[39;49mconstruct_array_type()\u001b[39m.\u001b[39;49m_from_sequence(arr, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m     97\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdtype must be np.dtype or ExtensionDtype\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\arrays\\string_.py:350\u001b[0m, in \u001b[0;36mStringArray._from_sequence\u001b[1;34m(cls, scalars, dtype, copy)\u001b[0m\n\u001b[0;32m    346\u001b[0m     result \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mensure_string_array(scalars, na_value\u001b[39m=\u001b[39mlibmissing\u001b[39m.\u001b[39mNA, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    348\u001b[0m \u001b[39m# Manually creating new array avoids the validation step in the __init__, so is\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[39m# faster. Refactor need for validation?\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m new_string_array \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    351\u001b[0m NDArrayBacked\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(new_string_array, result, StringDtype(storage\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    353\u001b[0m \u001b[39mreturn\u001b[39;00m new_string_array\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Read data from parquet file\n",
    "df = pd.read_parquet(r\"../../amex-default-prediction/train_data.parquet\")\n",
    "#reduce df for development !!!!! comment out line below for final model\n",
    "#df = df[:100000]\n",
    "\n",
    "# Set the data types for the columns\n",
    "df = set_col_types(df)\n",
    "\n",
    "#engineer statement num\n",
    "df['statement_num'] = df.groupby(\"customer_ID\")['S_2'].rank(method='first', ascending=False).astype(np.int8) \n",
    "df = df[df[\"statement_num\"] == 1]\n",
    "df.reset_index(inplace=True)\n",
    "#engineer date cols\n",
    "df[\"Month\"] = df[\"S_2\"].dt.month\n",
    "df[\"Day\"] = df[\"S_2\"].dt.day\n",
    "df[\"Year\"] = df[\"S_2\"].dt.year\n",
    "df = df.drop([\"S_2\"], axis=1)\n",
    "\n",
    "# Separate target variable and feature columns\n",
    "target = df[\"target\"]\n",
    "labels = df['customer_ID']\n",
    "features = df.drop([\"customer_ID\", \"target\"], axis=1)\n",
    "\n",
    "# Impute missing values using mode for categorical columns and median for numerical columns\n",
    "cat_columns = features.select_dtypes(include=[\"string\"]).columns\n",
    "num_columns = features.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# Replace missing values in the categorical columns with the most frequent value\n",
    "# for col in cat_columns:\n",
    "#     features[col].fillna(\"NA\", inplace=True)\n",
    "\n",
    "# Replace missing values in the numerical columns with the median value\n",
    "for col in num_columns:\n",
    "    features[col].fillna(features[col].mean(), inplace=True)\n",
    "\n",
    "features = pd.get_dummies(features, dummy_na=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a LightGBM model on the training data\n",
    "model = LGBMClassifier()\n",
    "model.set_params(**{'bagging_fraction': 0.9212862922324698, 'bagging_freq': 6, 'feature_fraction': 0.5071870813127417, 'learning_rate': 0.09393773349524241, 'num_leaves': 82, 'reg_alpha': 0.8316783754122955, 'reg_lambda': 0.1695940899747675})\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print(classification_report(y_test, y_pred, digits=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7798888718374688"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amex_test = y_test.reset_index(drop=True).to_frame()\n",
    "amex_test['target'] = amex_test['target'].astype('int')\n",
    "\n",
    "amex_pred = model.predict_proba(X_test)\n",
    "amex_pred = pd.DataFrame(amex_pred,columns=[\"proba-inv\",\"proba\"])\n",
    "amex_pred = amex_pred.drop(columns=['proba-inv'])\n",
    "amex_pred = amex_pred.rename(columns={\"proba\":\"prediction\"})\n",
    "\n",
    "\n",
    "amex_metric(amex_test, amex_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430   \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124472 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117896 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115578 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106519 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                 \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                               \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119221 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102667 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099470 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 45010                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 321239, number of used features: 226\n",
      "[LightGBM] [Info] Start training from score 0.259430                              \n",
      "100%|| 100/100 [07:54<00:00,  4.74s/trial, best loss: -0.778971510608933]\n",
      "{'bagging_fraction': 0.9212862922324698, 'bagging_freq': 6.0, 'feature_fraction': 0.5071870813127417, 'learning_rate': 0.09393773349524241, 'num_leaves': 82.0, 'reg_alpha': 0.8316783754122955, 'reg_lambda': 0.1695940899747675}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train.astype(int))\n",
    "\n",
    "# Define the search space for the hyperparameters\n",
    "space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.5, 1.0),\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1.0),\n",
    "    'bagging_freq': hp.quniform('bagging_freq', 5, 25, 1),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "}\n",
    "\n",
    "# Define the objective function that will be minimized\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'num_leaves': int(params['num_leaves']),\n",
    "        'feature_fraction': params['feature_fraction'],\n",
    "        'bagging_fraction': params['bagging_fraction'],\n",
    "        'bagging_freq': int(params['bagging_freq']),\n",
    "        'reg_alpha': params['reg_alpha'],\n",
    "        'reg_lambda': params['reg_lambda'],\n",
    "    }\n",
    "\n",
    "    # Use the LightGBM model to train on the data with the given hyperparameters\n",
    "    model = lgb.train(params, train_data)\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate the accuracy of the model on the validation data\n",
    "    amex_test = y_test.reset_index(drop=True).to_frame()\n",
    "    amex_test['target'] = amex_test['target'].astype(int)\n",
    "\n",
    "    amex_pred = model.predict(X_test)\n",
    "    amex_pred = pd.DataFrame(amex_pred,columns=[\"prediction\"])\n",
    "    #amex_pred = amex_pred.drop(columns=['proba-inv'])\n",
    "    #amex_pred = amex_pred.rename(columns={\"proba\":\"prediction\"})\n",
    "\n",
    "\n",
    "    accuracy = float(amex_metric(amex_test, amex_pred))\n",
    "\n",
    "\n",
    "    # Return the negative accuracy since we want to minimize the objective\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
    "\n",
    "# Use the Tree-structured Parzen Estimator (TPE) to find the best set of hyperparameters\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=Trials())\n",
    "\n",
    "# Print the best set of hyperparameters\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data from parquet file\n",
    "df = pd.read_parquet(r\"../../amex-default-prediction/test_data.parquet\")\n",
    "#df = df[:100000]\n",
    "\n",
    "# Set the data types for the columns\n",
    "df = set_col_types(df)\n",
    "\n",
    "# Engineer statement num\n",
    "df['statement_num'] = df.groupby(\"customer_ID\")['S_2'].rank(method='first', ascending=False).astype(np.int8)\n",
    "df = df[df[\"statement_num\"] == 1]\n",
    "df.reset_index(inplace=True)\n",
    "# Engineer date cols\n",
    "df[\"Month\"] = df[\"S_2\"].dt.month\n",
    "df[\"Day\"] = df[\"S_2\"].dt.day\n",
    "df[\"Year\"] = df[\"S_2\"].dt.year\n",
    "df = df.drop([\"S_2\"], axis=1)\n",
    "\n",
    "# Separate labels and feature columns\n",
    "labels = df['customer_ID']\n",
    "features = df.drop([\"customer_ID\"], axis=1)\n",
    "\n",
    "# Impute missing values using mode for categorical columns and median for numerical columns\n",
    "cat_columns = features.select_dtypes(include=[\"string\"]).columns\n",
    "num_columns = features.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# Replace missing values in the categorical columns with the most frequent value\n",
    "# for col in cat_columns:\n",
    "#     features[col].fillna(\"NA\", inplace=True)\n",
    "\n",
    "# Replace missing values in the numerical columns with the median value\n",
    "for col in num_columns:\n",
    "    features[col].fillna(features[col].mean(), inplace=True)\n",
    "\n",
    "features = pd.get_dummies(features, dummy_na=True)\n",
    "\n",
    "features = sync_cols(X_train, features)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(features)\n",
    "y_prob = model.predict_proba(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the labels to the predictions\n",
    "prediction_output = pd.concat([labels,pd.DataFrame(y_pred,columns=[\"pred\"]),pd.DataFrame(y_prob,columns=[\"proba-inv\",\"proba\"])], axis=1)\n",
    "\n",
    "prediction_output = prediction_output.drop(columns=['proba-inv','pred'])\n",
    "prediction_output = prediction_output.rename(columns={\"proba\":\"prediction\"})\n",
    "\n",
    "prediction_output.to_csv(\"train_last_pred_last_mean_tuned_but_int.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79cebfffb2e3a4b7d2d2fd53b48f0eab2f20a6a535e26e1d02c2764acd76f0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
