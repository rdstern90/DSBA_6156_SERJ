{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import xgboost as xgb\n",
    "\n",
    "rand_state = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipelines and functions \n",
    "1. Preprocessing 2. Sampling 3. Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines: Defining the categorical imputation and one-hot encoder for categorical variables.\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "        # (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)), #Commented out because the categorical variables won't play nice with dummies between test/train. Retry when we do a full train model. Can impute values on test_data.csv if necessary.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# defining the numerical imputation and standard scaler for numerical variables.\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    "           #(\"scale\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# def_prep_df: Preparing the TRAINING data for creating and testing the model.\n",
    "def prep_df(df, target, target_to_drop):\n",
    "\n",
    "    # save indices\n",
    "    df_index = df.index\n",
    "    # save statement_age column\n",
    "    statement_age_s = df['statement_age']\n",
    "\n",
    "    # Drop columns that shouldn't be scaled or imputed\n",
    "    df = df.drop(columns=[\"s_2\", 'statement_age', target_to_drop])\n",
    "\n",
    "    # Missing values handling\n",
    "    missing_props = df.isna().mean(axis=0)\n",
    "    \n",
    "\n",
    "    over_threshold = missing_props[missing_props >= 0.4]\n",
    "    over_threshold\n",
    "    \n",
    "\n",
    "    df.drop(over_threshold.index, \n",
    "            axis=1, \n",
    "            inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Split into predictors and target\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Split categorical and numerical columns\n",
    "    cat_cols_all = ['b_30', 'b_38', 'd_114', 'd_116', 'd_117', 'd_120', 'd_126', 'd_63', 'd_64', 'd_66', 'd_68', 'b_31', 'd_87']\n",
    "    cat_cols = [col for col in X.columns.str.lower() if col in cat_cols_all]\n",
    "    num_cols = [col for col in X.columns.str.lower() if col not in cat_cols]\n",
    "    \n",
    "    # get dummies for categorical variables\n",
    "    Xcat = pd.get_dummies(X[cat_cols], columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    X = pd.concat([X[num_cols],Xcat], axis=1)\n",
    "    X.columns = X.columns.str.lower()\n",
    "    cols_list = X.columns.tolist()\n",
    "\n",
    "    cat_cols = [col for col in cols_list if col not in num_cols]\n",
    "   \n",
    "\n",
    "\n",
    "    full_processor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_processed = full_processor.fit_transform(X)\n",
    "    X_processed = pd.concat([pd.DataFrame(X_processed, index=df_index), statement_age_s], axis=1)\n",
    "    print(X_processed.shape)\n",
    "\n",
    "    y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "            y.values.reshape(-1, 1)\n",
    "            )\n",
    "    y_processed = pd.DataFrame(y_processed, index=df_index)\n",
    "\n",
    "    \n",
    "    return X_processed, y_processed, cols_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction):\n",
    "    n = 100\n",
    "    ids = np.array(df_train_y.index)\n",
    "    target = np.array(df_train_y['target'])\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n, shuffle=True, random_state=rand_state)\n",
    "    skf.get_n_splits(ids, target)\n",
    "\n",
    "    i = 0\n",
    "    id_subsets = [None]*n\n",
    "    for _, subset in skf.split(ids, target):\n",
    "        id_subsets[i] =list(ids[subset])\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    list1 = list(np.arange(0, int(usefraction[0]*100), 1))\n",
    "    list2 = list(np.arange(int(usefraction[0]*100), int(usefraction[0]*100)+int(usefraction[1]*100), 1))\n",
    "\n",
    "\n",
    "    train_ids = []\n",
    "    for i in list1:\n",
    "        train_ids.extend(id_subsets[i])\n",
    "    test_ids = []\n",
    "    for i in list2:\n",
    "        test_ids.extend(id_subsets[i])\n",
    "\n",
    "\n",
    "    X_train = X_processed[df_train.index.isin(train_ids)]\n",
    "    y_train = y_processed[df_train.index.isin(train_ids)]\n",
    "    X_test = X_processed[df_train.index.isin(test_ids)]\n",
    "    y_test = y_processed[df_train.index.isin(test_ids)]\n",
    "\n",
    "\n",
    "    print(f'Train data obs.: {len(X_train)}')\n",
    "    print(f'Test data obs: {len(X_test)}')\n",
    "\n",
    "    # also extract the statement dates for combining the predictions later on\n",
    "    # train_statement_age = X_train['statement_age']\n",
    "    # test_statement_age = X_test['statement_age']\n",
    "    # X_train = X_train.drop(columns='statement_age')\n",
    "    # X_test = X_test.drop(columns='statement_age')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create initial df to be further processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = pd.read_parquet('./../../ignore/train.parquet')\n",
    "df_train_x.columns = df_train_x.columns.str.lower()\n",
    "df_train_x = df_train_x.sort_values(['customer_id', 's_2'])\n",
    "df_train_x = df_train_x.set_index('customer_id')\n",
    "\n",
    "df_train_y = pd.read_csv('./../../ignore/train_labels.csv')\n",
    "df_train_y.columns = df_train_y.columns.str.lower()\n",
    "df_train_y = df_train_y.set_index('customer_id')\n",
    "\n",
    "\n",
    "df_train = pd.merge(df_train_x, df_train_y, left_index=True, right_on='customer_id', how='left')\n",
    "\n",
    "df_train['last_statement_flag'] = (df_train.groupby(df_train.index)['s_2']\n",
    "                      .rank(method='dense', ascending=False)\n",
    "                      .astype(int)\n",
    "                   )\n",
    "\n",
    "df_train['last_statement_target'] = df_train['target']*df_train['last_statement_flag'].apply(lambda x: 1 if x==1 else 0)\n",
    "df_train = df_train.rename(columns={'last_statement_flag':'statement_age'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which statements to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_statements = [1,2,3]\n",
    "df_train = df_train[df_train['statement_age'].isin(use_statements)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the data after selecting statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1360401, 204)\n"
     ]
    }
   ],
   "source": [
    "# Prep the dataframe\n",
    "# Note that the last column 'statement_age' is left in the dataframes for scoring, not for predicting!\n",
    "X_processed, y_processed, cols_list = prep_df(df_train, target='target', target_to_drop='last_statement_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get samples for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data obs.: 136056\n",
      "Test data obs: 136031\n"
     ]
    }
   ],
   "source": [
    "# First vale of \"usefraction\" specifies the train size and the second, the test size (fraction of total train data available)\n",
    "X_train, X_test, y_train, y_test = get_train_test(df_train, df_train_y, X_processed, y_processed, usefraction = [0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_proba_score_xgb: 0.7654475216038875\n"
     ]
    }
   ],
   "source": [
    "# Init classifier\n",
    "xgb_a = xgb.XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "# Fit\n",
    "xgb_a.fit(X_train.iloc[:,:-1], y_train, verbose=0, eval_metric='logloss')\n",
    "\n",
    "# Predict\n",
    "y_pred_a_xgb = pd.DataFrame({'customer_id':X_test.index.values,\n",
    "                            'statement_age':X_test.iloc[:,-1].values,\n",
    "                             'prediction':[val[1] for val in xgb_a.predict_proba(X_test.iloc[:,:-1])]})\n",
    "\n",
    "# Score\n",
    "last_proba_xgb = y_pred_a_xgb[y_pred_a_xgb['statement_age']==1].set_index('customer_id')\n",
    "y_test_amexeval = y_test.groupby(y_test.index).max().rename(columns={0:'target'})\n",
    "print('last_proba_score_xgb:', amex_metric(y_test_amexeval, last_proba_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_proba_score_keras: 0.7502083314181278\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "keras_a = Sequential()\n",
    "keras_a.add(Dense(240, input_shape=(X_train.shape[1]-1,), activation='relu'))\n",
    "keras_a.add(Dense(120, activation='relu'))\n",
    "keras_a.add(Dense(40, activation='relu'))\n",
    "keras_a.add(Dense(8, activation='relu'))\n",
    "keras_a.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and fit\n",
    "keras_a.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "keras_a.fit(X_train.iloc[:,:-1], y_train, epochs=4, batch_size=40, verbose=0)\n",
    "# model.save('xxxxx')\n",
    "\n",
    "# Predict\n",
    "y_pred_a_keras = pd.DataFrame({'customer_id':X_test.index.values,\n",
    "                        'statement_age':X_test.iloc[:,-1].values,\n",
    "                        'prediction':[val[0] for val in list(keras_a.predict(X_test.iloc[:,:-1], verbose=0))]})\n",
    "\n",
    "# Score\n",
    "last_proba_keras = y_pred_a_keras[y_pred_a_keras['statement_age']==1].set_index('customer_id')\n",
    "y_test_amexeval = y_test.groupby(y_test.index).max().rename(columns={0:'target'})\n",
    "print('last_proba_score_keras:', amex_metric(y_test_amexeval, last_proba_keras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary model to consolidate per statement predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make prediction on the train dataset\n",
    "model_b_train_input = pd.DataFrame({'customer_id':X_train.index.values,\n",
    "                            'statement_age':list(X_train.iloc[:,-1]), \n",
    "                            'prediction':[val[0] for val in list(keras_a.predict(X_train.iloc[:,:-1], verbose=0))],\n",
    "                            'target':y_train[0].values})\n",
    "\n",
    "# pivot, impute predictions & split into x and y\n",
    "df_train_b = pd.pivot_table(model_b_train_input, values=['prediction', 'target'], index='customer_id', columns='statement_age')\n",
    "df_train_b = df_train_b.iloc[:,0:4] # remove unnecessary columns\n",
    "# df_train_b.head()\n",
    "\n",
    "df_train_b.columns = ['pred1', 'pred2', 'pred3', 'target']\n",
    "\n",
    "imparray = SimpleImputer(strategy='mean').fit_transform(df_train_b)\n",
    "df_train_b = pd.DataFrame(imparray, index=df_train_b.index, columns = df_train_b.columns)\n",
    "\n",
    "X_train_b = df_train_b.drop(columns=['target'])\n",
    "y_train_b = df_train_b['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1148/1148 [==============================] - 3s 2ms/step - loss: 0.2561 - accuracy: 0.9155\n",
      "Epoch 2/10\n",
      "1148/1148 [==============================] - 2s 1ms/step - loss: 0.1804 - accuracy: 0.9233\n",
      "Epoch 3/10\n",
      "1148/1148 [==============================] - 2s 1ms/step - loss: 0.1792 - accuracy: 0.9249\n",
      "Epoch 4/10\n",
      "1148/1148 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9247\n",
      "Epoch 5/10\n",
      "1148/1148 [==============================] - 2s 1ms/step - loss: 0.1786 - accuracy: 0.9245\n",
      "Epoch 6/10\n",
      "1148/1148 [==============================] - 2s 1ms/step - loss: 0.1790 - accuracy: 0.9246\n",
      "Epoch 7/10\n",
      "1148/1148 [==============================] - 2s 1ms/step - loss: 0.1787 - accuracy: 0.9250\n",
      "Epoch 8/10\n",
      "1148/1148 [==============================] - 2s 1ms/step - loss: 0.1787 - accuracy: 0.9249\n",
      "Epoch 9/10\n",
      "1148/1148 [==============================] - 2s 2ms/step - loss: 0.1764 - accuracy: 0.9249\n",
      "Epoch 10/10\n",
      "1148/1148 [==============================] - 2s 1ms/step - loss: 0.1743 - accuracy: 0.9246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17e8dc91b08>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define and fit a keras model\n",
    "keras_b = Sequential()\n",
    "keras_b.add(Dense(8, input_shape=(X_train_b.shape[1],), activation='relu'))\n",
    "keras_b.add(Dense(4, activation='relu'))\n",
    "keras_b.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and fit the keras model\n",
    "keras_b.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "keras_b.fit(X_train_b, y_train_b, epochs=10, batch_size=40, verbose=1)\n",
    "\n",
    "# score model on training data\n",
    "# y_pred_train_b = pd.DataFrame({'customer_id':X_train_b.index.values, 'prediction':[val[0] for val in list(model_b.predict(X_train_b, verbose=0))]}).set_index('customer_id')\n",
    "# print(amex_metric(y_train_amexeval, y_pred_train_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrain keras_b using both keras and xgb predictions\n",
    "This creates something like an ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2295/2295 [==============================] - 6s 2ms/step - loss: 0.2111 - accuracy: 0.9307\n",
      "Epoch 2/10\n",
      "2295/2295 [==============================] - 4s 2ms/step - loss: 0.1587 - accuracy: 0.9358\n",
      "Epoch 3/10\n",
      "2295/2295 [==============================] - 3s 1ms/step - loss: 0.1585 - accuracy: 0.9359\n",
      "Epoch 4/10\n",
      "2295/2295 [==============================] - 3s 1ms/step - loss: 0.1585 - accuracy: 0.9359\n",
      "Epoch 5/10\n",
      "2295/2295 [==============================] - 3s 1ms/step - loss: 0.1585 - accuracy: 0.9359\n",
      "Epoch 6/10\n",
      "2295/2295 [==============================] - 3s 1ms/step - loss: 0.1585 - accuracy: 0.9359\n",
      "Epoch 7/10\n",
      "2295/2295 [==============================] - 3s 1ms/step - loss: 0.1585 - accuracy: 0.9358\n",
      "Epoch 8/10\n",
      "2295/2295 [==============================] - 3s 1ms/step - loss: 0.1585 - accuracy: 0.9359\n",
      "Epoch 9/10\n",
      "2295/2295 [==============================] - 3s 1ms/step - loss: 0.1585 - accuracy: 0.9360\n",
      "Epoch 10/10\n",
      "2295/2295 [==============================] - 3s 1ms/step - loss: 0.1585 - accuracy: 0.9357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17e8e2daac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First make prediction on the train dataset\n",
    "model_b_train_input2 = pd.DataFrame({'customer_id':X_train.index.values,\n",
    "                            'statement_age':list(X_train.iloc[:,-1]), \n",
    "                            'prediction':[val[1] for val in xgb_a.predict_proba(X_train.iloc[:,:-1])],\n",
    "                            'target':y_train[0].values})\n",
    "\n",
    "# pivot, impute predictions & split into x and y\n",
    "df2_train_b = pd.pivot_table(model_b_train_input2, values=['prediction', 'target'], index='customer_id', columns='statement_age')\n",
    "df2_train_b = df2_train_b.iloc[:,0:4] # remove unnecessary columns\n",
    "df2_train_b.columns = ['pred1', 'pred2', 'pred3', 'target']\n",
    "\n",
    "imparray = SimpleImputer(strategy='mean').fit_transform(df2_train_b)\n",
    "df2_train_b = pd.DataFrame(imparray, index=df2_train_b.index, columns = df2_train_b.columns)\n",
    "\n",
    "X_train_b = pd.concat([X_train_b, df2_train_b.drop(columns=['target'])], axis=0)\n",
    "y_train_b = pd.concat([y_train_b, df2_train_b['target']],axis=0)\n",
    "\n",
    "\n",
    "# define and fit a keras model\n",
    "keras_b = Sequential()\n",
    "keras_b.add(Dense(8, input_shape=(X_train_b.shape[1],), activation='relu'))\n",
    "keras_b.add(Dense(4, activation='relu'))\n",
    "keras_b.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and fit the keras model\n",
    "keras_b.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "keras_b.fit(X_train_b, y_train_b, epochs=10, batch_size=40, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consolidated score: 0.7429679512849126\n"
     ]
    }
   ],
   "source": [
    "model_b_test_keras = pd.DataFrame({'customer_id':X_test.index.values,\n",
    "                            'statement_age':list(X_test.iloc[:,-1]), \n",
    "                            'prediction':[val[0] for val in list(keras_a.predict(X_test.iloc[:,:-1], verbose=0))],\n",
    "                            'target':y_test[0].values})\n",
    "\n",
    "df2_test_keras = pd.pivot_table(model_b_test_keras, values=['prediction', 'target'], index='customer_id', columns='statement_age')\n",
    "df2_test_keras = df2_test_keras.iloc[:,0:4] # remove unnecessary columns\n",
    "df2_test_keras.columns = ['pred1', 'pred2', 'pred3', 'target']\n",
    "\n",
    "imparray = SimpleImputer(strategy='mean').fit_transform(df2_test_keras)\n",
    "df2_test_keras = pd.DataFrame(imparray, index=df2_test_keras.index, columns = df2_test_keras.columns)\n",
    "\n",
    "model_b_test_xgb = pd.DataFrame({'customer_id':X_test.index.values,\n",
    "                            'statement_age':list(X_test.iloc[:,-1]), \n",
    "                            'prediction':[val[1] for val in xgb_a.predict_proba(X_test.iloc[:,:-1])],\n",
    "                            'target':y_test[0].values})\n",
    "\n",
    "df2_test_xgb = pd.pivot_table(model_b_test_xgb, values=['prediction', 'target'], index='customer_id', columns='statement_age')\n",
    "df2_test_xgb = df2_test_keras.iloc[:,0:4] # remove unnecessary columns\n",
    "df2_test_xgb.columns = ['pred1', 'pred2', 'pred3', 'target']\n",
    "\n",
    "imparray = SimpleImputer(strategy='mean').fit_transform(df2_test_xgb)\n",
    "df2_test_xgb = pd.DataFrame(imparray, index=df2_test_xgb.index, columns = df2_test_xgb.columns)\n",
    "\n",
    "\n",
    "X_test_b = pd.concat([df2_test_keras.drop(columns=['target']), df2_test_xgb.drop(columns=['target'])], axis=0)\n",
    "y_test_b = pd.concat([df2_test_keras['target'], df2_test_xgb['target']],axis=0)\n",
    "\n",
    "\n",
    "# Predict\n",
    "y_pred_model_b = pd.DataFrame({'customer_id':X_test_b.index.values,\n",
    "                        'prediction':[val[0] for val in list(keras_b.predict(X_test_b, verbose=0))]})\n",
    "\n",
    "# Score\n",
    "average_proba_keras_b = y_pred_model_b.groupby(by='customer_id').mean()\n",
    "y_test_b_amexeval = pd.DataFrame(y_test_b.groupby(y_test_b.index).max())\n",
    "print('consolidated score:', amex_metric(y_test_b_amexeval, average_proba_keras_b))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca7b95862a80fe182a4f5fb98c5ac1654efde8b126ad722bfd53b4f5adb0e8de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
