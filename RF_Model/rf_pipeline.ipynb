{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "rand_state = 1337\n",
    "df_chunk_num_lines = 3500000 #number of lines you want in each df chunk - make this lower if you get a memory error\n",
    "\n",
    "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "for i in range(0,len(cat_cols)):\n",
    "    cat_cols[i] = cat_cols[i].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://user:DeEJNEAhy@34.75.124.150/postgres')\n",
    "\n",
    "query = '''select train_data_random.*, train_labels_random.target\n",
    "            from train_data_random\n",
    "            inner join train_labels_random\n",
    "            on train_data_random.customer_id = train_labels_random.customer_id;'''\n",
    "\n",
    "df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, label_cols=[], forced_cat_cols=[], drop_cols=[]):\n",
    "\n",
    "  df = df.fillna(np.nan) #because SimpleImputer requires specification of the type of nan value, we use this generic to change all nan types to np.nan types\n",
    "  df.columns= df.columns.str.lower()\n",
    "\n",
    "  df[forced_cat_cols] = df[forced_cat_cols].astype(object) #change dtype of force category cols to object so get_dummies encodes it\n",
    "\n",
    "  df = df.drop(columns=drop_cols)\n",
    "\n",
    "  df_labels = df[label_cols] #splits any specified columns off to a label df\n",
    "  df = df.drop(label_cols,axis=1)\n",
    "\n",
    "  cat_cols = df.select_dtypes(exclude=\"number\").columns #should include forced_cat_cols set above\n",
    "  num_cols = df.select_dtypes(include=\"number\").columns\n",
    "\n",
    "  #impute num + cat\n",
    "  for col in cat_cols:\n",
    "    df[col] = SimpleImputer(strategy=\"most_frequent\").fit_transform(df[[col]]) #used this syntax with the loop because it keeps the data as a dataframe. zero chance of column's getting mixed handling nparrays\n",
    "  for col in num_cols:\n",
    "    df[col] = SimpleImputer(strategy=\"mean\").fit_transform(df[[col]]) #used this syntax with the loop because it keeps the data as a dataframe. zero chance of column's getting mixed handling nparrays\n",
    "  if df.isna().sum().sum() > 0:\n",
    "    print(f\"WARNING: {df.isna().sum().sum()} nulls still exist after imputing.\")\n",
    "\n",
    "  #scale num\n",
    "  for col in num_cols:\n",
    "    df[col] = StandardScaler().fit_transform(df[[col]]) #used this syntax with the loop because it keeps the data as a dataframe. zero chance of column's getting mixed handling nparrays\n",
    "\n",
    "  df = pd.get_dummies(df)\n",
    "\n",
    "  if len(label_cols)>0:\n",
    "    return df, df_labels\n",
    "  else:\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_cols(expected_cols,df):\n",
    "    for col in expected_cols:\n",
    "        if col not in df.columns:\n",
    "            print(col, \"not in df so adding - should always be categorical!\")\n",
    "            df[col] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build list of columns with 50 percent missing values\n",
    "percent_null = df.isnull().sum() / len(df) \n",
    "half_missing_cols = percent_null[percent_null > 0.5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "c:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9141837496887708"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build model\n",
    "y = df['target']\n",
    "x = df.drop(columns=[\"target\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=rand_state)\n",
    "\n",
    "x_train, x_train_labels = preprocess_data(x_train, forced_cat_cols=cat_cols, label_cols=[\"customer_id\",\"s_2\"], drop_cols=half_missing_cols)\n",
    "\n",
    "\n",
    "rf_all = RandomForestClassifier(random_state=rand_state)\n",
    "rf_all.fit(x_train, y_train)\n",
    "\n",
    "x_test, x_test_labels = preprocess_data(x_test, forced_cat_cols=cat_cols, label_cols=[\"customer_id\",\"s_2\"], drop_cols=half_missing_cols)\n",
    "\n",
    "rf_all.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_64_-1 not in df so adding - should always be categorical!\n",
      "d_68_0.0 not in df so adding - should always be categorical!\n",
      "d_126_-1.0 not in df so adding - should always be categorical!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "rf_all_output = pd.DataFrame()\n",
    "\n",
    "current_position = 0 #defines starting position and keeps track of where in file to read\n",
    "df_columns = None #object to hold the col names collected from the first df chunk\n",
    "while True:\n",
    "    try:\n",
    "        df_chunk = pd.read_csv(r'..\\amex-default-prediction\\test_data.csv', skiprows=current_position, nrows=df_chunk_num_lines)\n",
    "        df_chunk.columns = df_chunk.columns.str.lower() #convert all the column names to lowercase so that they match the column names from the database\n",
    "\n",
    "        if current_position == 0:\n",
    "            df_columns = df_chunk.columns\n",
    "        else:\n",
    "            df_chunk.columns = df_columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(current_position)\n",
    "        #you get DF chunks that you can do whatever with here\n",
    "\n",
    "        df_chunk, df_chunk_labels = preprocess_data(df_chunk, forced_cat_cols=cat_cols, label_cols=[\"customer_id\",\"s_2\"], drop_cols=half_missing_cols)\n",
    "\n",
    "        model_cols = x_test.columns #define index of columns used at time of model fit\n",
    "        df_chunk = add_missing_cols(model_cols, df_chunk)\n",
    "\n",
    "\n",
    "        rf_all_prediction_chunk_preds = rf_all.predict(df_chunk)\n",
    "        rf_all_prediction_chunk_proba = rf_all.predict_proba(df_chunk)\n",
    "        rf_all_prediction_chunk_output = pd.concat([df_chunk_labels,pd.DataFrame(rf_all_prediction_chunk_preds,columns=[\"pred\"]),pd.DataFrame(rf_all_prediction_chunk_proba,columns=[\"proba-inv\",\"proba\"])], axis=1)\n",
    "        rf_all_output = pd.concat([rf_all_output, rf_all_prediction_chunk_output])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        current_position += df_chunk_num_lines #increments position by chunk size for the next loop\n",
    "    except pd.errors.EmptyDataError:\n",
    "        break #stop when an empty dataframe is returned\n",
    "\n",
    "#rf_all_output = rf_all_output.drop(columns=[\"pred-inverse\"])\n",
    "rf_all_output.to_csv(r\"..\\amex-default-prediction\\rf_all_output.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b38687464f26cc788bd6aab5d6ae24f3673c8f039be5f165e71e31106dd7d5ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
