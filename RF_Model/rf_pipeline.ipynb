{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImputeAsDataFrame(BaseEstimator, TransformerMixin):\n",
    "#   def fit(self, X, y=None):\n",
    "#     return self\n",
    "#   def transform(self, df):\n",
    "#     cat_cols = df.select_dtypes(exclude=\"number\").columns\n",
    "#     num_cols = df.select_dtypes(include=\"number\").columns\n",
    "\n",
    "#     for col in cat_cols:\n",
    "#       df[col] = SimpleImputer(missing_values=None, strategy=\"most_frequent\").fit_transform(df[[col]])\n",
    "#     for col in num_cols:\n",
    "#       df[col] = SimpleImputer(strategy=\"mean\").fit_transform(df[[col]])\n",
    "\n",
    "#     if df.isna().sum().sum() > 0:\n",
    "#       print(f\"WARNING: {df.isna().sum().sum()} nulls still exist after imputing.\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "rand_state = 1337\n",
    "df_chunk_num_lines = 3500000 #number of lines you want in each df chunk - make this lower if you get a memory error\n",
    "\n",
    "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "for i in range(0,len(cat_cols)):\n",
    "    cat_cols[i] = cat_cols[i].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  2.  1. nan]\n",
      "[ 3.  1.  2.  5.  4.  7.  6. nan]\n",
      "[ 1. nan  0.]\n",
      "[ 0. nan  1.]\n",
      "[-1.  6.  4. nan  3.  5.  1.  2.]\n",
      "[ 0. nan  1.]\n",
      "[ 1.  0. nan -1.]\n",
      "['CR' 'CO' 'CL' 'XZ' 'XM' 'XL']\n",
      "['O' 'R' None 'U' '-1']\n",
      "[nan  1.  0.]\n",
      "[ 3.  4.  5. nan  6.  1.  2.  0.]\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine('postgresql://user:DeEJNEAhy@34.75.124.150/postgres')\n",
    "\n",
    "query = '''select train_data_random.*, train_labels_random.target\n",
    "            from train_data_random\n",
    "            inner join train_labels_random\n",
    "            on train_data_random.customer_id = train_labels_random.customer_id;'''\n",
    "\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "for col in df[cat_cols]:\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BackToDataFrame(BaseEstimator, TransformerMixin):\n",
    "#   def __init__(self, cols=[]):\n",
    "#     self.cols = cols\n",
    "#   def fit(self, X, y=None):\n",
    "#     return self\n",
    "#   def fit_transform(self, X):\n",
    "#     df = pd.DataFrame(X)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build list of columns with 50 percent missing values\n",
    "percent_null = df.isnull().sum() / len(df) \n",
    "half_missing_cols = percent_null[percent_null > 0.5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, label_cols=[], forced_cat_cols=[], drop_cols=[]):\n",
    "\n",
    "  df = df.fillna(np.nan) #because SimpleImputer requires specification of the type of nan value, we use this generic to change all nan types to np.nan types\n",
    "  df.columns= df.columns.str.lower()\n",
    "\n",
    "  df[forced_cat_cols] = df[forced_cat_cols].astype(object)\n",
    "\n",
    "  df = df.drop(columns=drop_cols)\n",
    "\n",
    "  df_labels = df[label_cols]\n",
    "  df = df.drop(label_cols,axis=1)\n",
    "\n",
    "  cat_cols = df.select_dtypes(exclude=\"number\").columns\n",
    "  num_cols = df.select_dtypes(include=\"number\").columns\n",
    "\n",
    "  #impute num + cat\n",
    "  for col in cat_cols:\n",
    "    df[col] = SimpleImputer(strategy=\"most_frequent\").fit_transform(df[[col]])\n",
    "  for col in num_cols:\n",
    "    df[col] = SimpleImputer(strategy=\"mean\").fit_transform(df[[col]])\n",
    "  if df.isna().sum().sum() > 0:\n",
    "    print(f\"WARNING: {df.isna().sum().sum()} nulls still exist after imputing.\")\n",
    "\n",
    "  #scale num\n",
    "  for col in num_cols:\n",
    "    df[col] = StandardScaler().fit_transform(df[[col]])\n",
    "\n",
    "  df = pd.get_dummies(df)\n",
    "\n",
    "  if len(label_cols)>0:\n",
    "    return df, df_labels\n",
    "  else:\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n",
      "c:\\Users\\Neo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9141837496887708"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build model\n",
    "\n",
    "y = df['target']\n",
    "x = df.drop(columns=[\"target\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=rand_state)\n",
    "\n",
    "x_train, x_train_labels = preprocess_data(x_train, forced_cat_cols=cat_cols, label_cols=[\"customer_id\",\"s_2\"], drop_cols=half_missing_cols)\n",
    "\n",
    "\n",
    "rf_all = RandomForestClassifier(random_state=rand_state)\n",
    "rf_all.fit(x_train, y_train)\n",
    "\n",
    "x_test, x_test_labels = preprocess_data(x_test, forced_cat_cols=cat_cols, label_cols=[\"customer_id\",\"s_2\"], drop_cols=half_missing_cols)\n",
    "\n",
    "rf_all.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "preprocess_data() got an unexpected keyword argument 'dropcols'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19468\\2667587267.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#you get DF chunks that you can do whatever with here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mdf_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_chunk_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_chunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"customer_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"s_2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhalf_missing_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mrf_all_prediction_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_chunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocess_data() got an unexpected keyword argument 'dropcols'"
     ]
    }
   ],
   "source": [
    "rf_all_output = pd.DataFrame()\n",
    "\n",
    "current_position = 0 #defines starting position and keeps track of where in file to read\n",
    "df_columns = None #object to hold the col names collected from the first df chunk\n",
    "while True:\n",
    "    try:\n",
    "        df_chunk = pd.read_csv(r'..\\amex-default-prediction\\test_data_last_statement.csv', skiprows=current_position, nrows=df_chunk_num_lines)\n",
    "        df_chunk.columns = df_chunk.columns.str.lower() #convert all the column names to lowercase so that they match the column names from the database\n",
    "        if current_position == 0:\n",
    "            df_columns = df_chunk.columns\n",
    "        else:\n",
    "            df_chunk.columns = df_columns\n",
    "\n",
    "\n",
    "\n",
    "        print(current_position)\n",
    "        #you get DF chunks that you can do whatever with here\n",
    "\n",
    "        df_chunk, df_chunk_labels = preprocess_data(df_chunk, forced_cat_cols=cat_cols, label_cols=[\"customer_id\",\"s_2\"], drop_cols=half_missing_cols)\n",
    "\n",
    "        rf_all_prediction_chunk = rf_all.predict_proba(df_chunk)\n",
    "        rf_all_prediction_chunk_output = pd.concat([df_chunk_labels,pd.DataFrame(rf_all_prediction_chunk,columns=[\"pred-inverse\",\"prediction\"])], axis=1)\n",
    "        rf_all_prediction_chunk_output = rf_all_prediction_chunk_output.drop(columns=[\"pred-inverse\"])\n",
    "        \n",
    "        rf_all_output = pd.concat([rf_all_output, rf_all_prediction_chunk_output],axis=1)\n",
    "\n",
    "        current_position += df_chunk_num_lines #increments position by chunk size for the next loop\n",
    "    except pd.errors.EmptyDataError:\n",
    "        break\n",
    "\n",
    "rf_all_output.to_csv(r\"..\\amex-default-prediction\\rf_all_output.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b38687464f26cc788bd6aab5d6ae24f3673c8f039be5f165e71e31106dd7d5ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
